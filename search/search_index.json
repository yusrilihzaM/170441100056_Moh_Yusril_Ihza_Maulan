{"config":{"lang":["en"],"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Identitas Penulis \u00b6 Nama : Moh Yusril Ihza Maulana NRP : 170441100056 Mata kuliah : Data Mining Jurusan : Teknik Informatika Prodi : Sistem Informasi Perguruan Tinggi : Universitas Trunojoyo Madura","title":"Identitas Penulis"},{"location":"#identitas-penulis","text":"Nama : Moh Yusril Ihza Maulana NRP : 170441100056 Mata kuliah : Data Mining Jurusan : Teknik Informatika Prodi : Sistem Informasi Perguruan Tinggi : Universitas Trunojoyo Madura","title":"Identitas Penulis"},{"location":"dtc/","text":"Pengertian Decision Tree Classification \u00b6 Decision tree merupakan suatu metode klasifikasi yang menggunakan struktur pohon, dimana setiap node merepresentasikan atribut dan cabangnya merepresentasikan nilai dari atribut, sedangkan daunnya digunakan untuk merepresentasikan kelas. Node teratas dari decision tree ini disebut dengan root . Ini memecah dataset menjadi himpunan bagian yang lebih kecil dengan peningkatan kedalaman pohon. Hasil akhirnya adalah pohon dengan simpul keputusan dan simpul daun. Node keputusan (mis., Outlook) memiliki dua atau lebih cabang (mis., Sunny, Overcast, dan Rainy). Node daun (mis., Play) mewakili klasifikasi atau keputusan. Node keputusan teratas dalam pohon yang sesuai dengan prediktor terbaik disebut simpul akar. Pohon keputusan dapat menangani data kategorikal dan numerik. Jenis Decision Tree \u00b6 Categorical Variable Decision Tree (Pohon Keputusan Variabel Kategorikal) merupakan pohon keputusan yang memiliki variabel target kategorikal Continuous Variable Decision Tree() merupakan Decision Tree yang memiliki variabel target kontinu Terminologi Penting terkait dengan Decision Tree Classification \u00b6 Root Node : Ini mewakili seluruh populasi atau sampel dan ini selanjutnya dibagi menjadi dua atau lebih set homogen. Splitting : Ini adalah proses membagi sebuah node menjadi dua atau lebih sub-node. Decision Node : Ketika sebuah sub-node terbagi menjadi beberapa sub-node, maka itu disebut simpul keputusan (decision node). Leaf/ Terminal Node : Node tanpa anak (tanpa pemisahan lebih lanjut) disebut Leaf atau Terminal node. Pruning : Ketika kita mengurangi ukuran pohon keputusan dengan menghapus node (kebalikan dari Splitting), proses ini disebut pemangkasan (pruning). Branch / Sub-Tree : Subbagian pohon keputusan disebut cabang (Branch) atau sub-pohon.( Sub-Tree) Parent and Child Node : Sebuah node, yang dibagi menjadi beberapa sub-node disebut parent node dari sub-node dimana sebagai sub-node adalah anak dari node induk Kelebihan dan Kekurangan Decision Tree Classification \u00b6 Kelebihan: \u00b6 Daerah pengambilan keputusan yang sebelumnya kompleks dan sangat global, dapat diubah menjadi lebih simpel dan spesifik. Eliminasi perhitungan-perhitungan yang tidak diperlukan, karena ketika menggunakan metode pohon keputusan maka sample diuji hanya berdasarkan kriteria atau kelas tertentu. Fleksibel untuk memilih fitur dari internal node yang berbeda, fitur yang terpilih akan membedakan suatu kriteria dibandingkan kriteria yang lain dalam node yang sama. Kefleksibelan metode pohon keputusan ini meningkatkan kualitas keputusan yang dihasilkan jika dibandingkan ketika menggunakan metode penghitungan satu tahap yang lebih konvensional Dalam analisis multivariat, dengan kriteria dan kelas yang jumlahnya sangat banyak, seorang penguji biasanya perlu untuk mengestimasikan baik itu distribusi dimensi tinggi ataupun parameter tertentu dari distribusi kelas tersebut. Metode pohon keputusan dapat menghindari munculnya permasalahan ini dengan menggunakan criteria yang jumlahnya lebih sedikit pada setiap node internal tanpa banyak mengurangi kualitas keputusan yang dihasilkan. Kekurangan: \u00b6 Terjadi overlap terutama ketika kelas-kelas dan criteria yang digunakan jumlahnya sangat banyak. Hal tersebut juga dapat menyebabkan meningkatnya waktu pengambilan keputusan dan jumlah memori yang diperlukan. Pengakumulasian jumlah eror dari setiap tingkat dalam sebuah pohon keputusan yang besar. Kesulitan dalam mendesain pohon keputusan yang optimal. Hasil kualitas keputusan yang didapatkan dari metode pohon keputusan sangat tergantung pada bagaimana pohon tersebut didesain. Algoritma Decision Tree Classification \u00b6 ID3 Gini Index Chi-Square Reduction in Variance Namun disini kita hanya membahan ID3 dan Gini index saja. ID3 \u00b6 Algoritma inti untuk membangun pohon keputusan disebut ID3. Dikembangkan oleh J. R. Quinlan, algoritma ini menggunakan pencarian top-down, melalui ruang cabang yang mungkin tanpa backtracking. ID3 menggunakan Entropy dan Information Gain untuk membuat keputusan Entropy \u00b6 Pohon keputusan dibangun dari atas ke bawah dari simpul akar (root node) dan melibatkan mempartisi data menjadi subset yang berisi instance dengan nilai yang sama (homogen). Algoritma ID3 menggunakan entropi untuk menghitung homogenitas sampel. Jika sampel benar-benar homogen, entropinya nol dan jika sampel dibagi rata maka entropinya satu. Untuk membangun pohon keputusan, kita perlu menghitung dua jenis entropi menggunakan tabel frekuensi sebagai berikut: a. Entropy menggunakan tabel frekuensi satu atribut: b. Entropi menggunakan tabel frekuensi dua atribut: Information Gain \u00b6 Information Gain didasarkan pada penurunan entropi setelah kumpulan data dibagi pada atribut. Membangun pohon keputusan adalah tentang menemukan atribut yang mengembalikan perolehan informasi tertinggi (mis., Cabang yang paling homogen). Langkah 1 : Hitung entropi target. Langkah 2 : Kumpulan data kemudian dibagi pada atribut yang berbeda. Entropi untuk setiap cabang dihitung. Kemudian ditambahkan secara proporsional, untuk mendapatkan total entropi untuk pemisahan. Entropi yang dihasilkan dikurangi dari entropi sebelum pemisahan. Hasilnya adalah Information Gain, atau penurunan entropi. Langkah 3 : Pilih atribut dengan perolehan Information Gain terbesar sebagai simpul keputusan (decision node), bagi dataset dengan cabang-cabangnya dan ulangi proses yang sama pada setiap cabang. Langkah 4a : Cabang dengan entropi 0 adalah simpul daun. Langkah 4b : Cabang dengan entropi lebih dari 0 membutuhkan pemisahan lebih lanjut Langkah 5 : Algoritma ID3 dijalankan secara rekursif pada cabang-cabang non-daun, sampai semua data diklasifikasikan. Gini Index \u00b6 Indeks Gini mengatakan, jika kita memilih dua item dari populasi secara acak maka mereka harus dari kelas yang sama dan probabilitas untuk ini adalah 1 jika populasi murni. Ia bekerja dengan variabel target kategori \"Sukses\" atau \"Kegagalan\". Ini hanya melakukan split Biner Semakin tinggi nilai Gini semakin tinggi homogenitasnya. CART (Klasifikasi dan Pohon Regresi) menggunakan metode Gini untuk membuat pemisahan biner. Rumus Gini Index \u00b6 \\operatorname{Gini}(\\mathrm{D})=1-\\sum_{\\mathrm{i}=1}^{m} \\mathrm{Pi}^{2} \\operatorname{Gini}(\\mathrm{D})=1-\\sum_{\\mathrm{i}=1}^{m} \\mathrm{Pi}^{2} pi adalah probabilitas bahwa sebuah tuple dalam D milik kelas Ci. Weighted Gini untuk Pemisahan: $$ \\operatorname{Gini}_{\\mathrm{A}}(\\mathrm{D})=\\frac{|D 1|}{|D|} \\operatorname{Gini}\\left(\\mathrm{D}_{1}\\right)+\\frac{|D 2|}{|D|} \\operatorname{Gini}\\left(\\mathrm{D}_{2}\\right) \\operatorname{Gini}_{\\mathrm{A}}(\\mathrm{D})=\\frac{|D 1|}{|D|} \\operatorname{Gini}\\left(\\mathrm{D}_{1}\\right)+\\frac{|D 2|}{|D|} \\operatorname{Gini}\\left(\\mathrm{D}_{2}\\right) $$ Langkah-langkah untuk Menghitung Gini untuk pemisahan \u00b6 Hitung Gini untuk sub-node, menggunakan rumus jumlah kuadrat probabilitas untuk keberhasilan dan kegagalan (p\u00b2 + q\u00b2). Hitung Gini untuk split menggunakan skor Gini tertimbang dari setiap node dari split itu Contoh: - Mengacu pada contoh di mana kami ingin memisahkan siswa berdasarkan variabel target (playing criket atau tidak). Dalam snapshot di bawah ini, kami membagi populasi menggunakan dua variabel input Gender dan Class. Sekarang, saya ingin mengidentifikasi split mana yang menghasilkan lebih banyak sub-node homogen menggunakan indeks Gini. Pemisahan di gender Gini untuk sub node Female = (0.43)x(0.43)+(0.57)x(0.57)=0.51 Gini untuk sub node Male = (0.56)x(0.56)+(0.44)x(0.44)=0.51 Weighted Gini untuk Pemisahan Gender = (10/30)x0.68+(20/30)x0.55 = 0.59 Pemisahan di class Gini untuk sub node Class IX= (0.2)x(0.2)+(0.8)x(0.8)=0.68 Gini untuk sub node Class X= (0.65)x(0.65)+(0.35)x(0.35)=0.55 Weighted Gini untuk Pemisahan Class = (14/30)x0.51+(16/30)x0.51 = 0.51 Implementasi Decision Tree Classification Breast Cancer Wisconsin (Diagnostic) Dataset \u00b6 Kebutuhan Software \u00b6 Python 3.0 atau versi yang lebih baru, disini saya menggunakan python 3.7 IDE Pycharm Jupyter notebook Library Python yang digunakan: \u00b6 Pandas pandas adalah sebuah librari berlisensi BSD dan open source yang menyediakan struktur data dan analisis data yang mudah digunakan dan berkinerja tinggi untuk bahasa pemrograman Python. instal pandas: pip install pandas Scikit Learn Machine learning ada yang berbasis statistika ada juga yang tidak. Salah satunya adalah support vector machine dan regresi linier. Mungkin bagi sebagian orang sudah biasa menulis sendiri library untuk implementasi kedua algoritma tadi. Tapi untuk membuatnya dalam waktu singkat tentu butuh waktu yang tidak sedikit pula. Scikit-Learn memberikan sejumlah fitur untuk keperluan data science seperti: Algoritma Regresi Algoritma Naive Bayes Algoritma Clustering Algoritma Decision Tree Parameter Tuning Data Preprocessing Tool Export / Import Model Machine learning pipeline dan lainnya instal Scikit Learn : pip install scikit-learn graphviz kitadapat menggunakan fungsi Scikit-learn export_graphviz untuk menampilkan pohon di dalam notebook Jupyter. Untuk merencanakan pohon, Anda juga perlu menginstal graphviz dan pydotplus instal menggunakan pip: pip install graphviz Instal menggunakan conda: conda install -c conda-forge graphviz pydotplus PyDotPlus adalah versi perbaikan dari proyek pydot lama yang menyediakan Antarmuka Python ke bahasa Dot Graphviz pip install pydotplus Instal menggunakan conda: conda install -c conda-forge pydotplus Import Library yang dibutuhkan \u00b6 import pandas as pd from sklearn.model_selection import train_test_split from sklearn import metrics #importing modul metrik from sklearn.tree import DecisionTreeClassifier from sklearn.tree import export_graphviz from sklearn.externals.six import StringIO from IPython.display import Image import pydotplus Memuat Dataset \u00b6 Mengimport dataset yang digunakan untuk pengimplemtasian K-Nearest Neighbor dataset bisa didownload disini atau langsung dari kaggle disini # Memuat dataset data = pd.read_csv(\"E:\\Semester 4\\data mining/bc.csv\",encoding = \"ISO-8859-1\") Data Preprocessing \u00b6 Menampilkan 5 Data teratas \u00b6 Menampilkan jumlah masing-masing diagnosis dan kolom diagnosis digunakan sebagai class nantinya. data.head(5) Output: Menampilkan ringkasan dataset serta menghilangkan kolom yang tidak berguna \u00b6 # ringkasan dataset data.info() Output: <class 'pandas.core.frame.DataFrame'> RangeIndex: 569 entries, 0 to 568 Data columns (total 33 columns): id 569 non-null int64 diagnosis 569 non-null object radius_mean 569 non-null float64 texture_mean 569 non-null float64 perimeter_mean 569 non-null float64 area_mean 569 non-null float64 smoothness_mean 569 non-null float64 compactness_mean 569 non-null float64 concavity_mean 569 non-null float64 concave points_mean 569 non-null float64 symmetry_mean 569 non-null float64 fractal_dimension_mean 569 non-null float64 radius_se 569 non-null float64 texture_se 569 non-null float64 perimeter_se 569 non-null float64 area_se 569 non-null float64 smoothness_se 569 non-null float64 compactness_se 569 non-null float64 concavity_se 569 non-null float64 concave points_se 569 non-null float64 symmetry_se 569 non-null float64 fractal_dimension_se 569 non-null float64 radius_worst 569 non-null float64 texture_worst 569 non-null float64 perimeter_worst 569 non-null float64 area_worst 569 non-null float64 smoothness_worst 569 non-null float64 compactness_worst 569 non-null float64 concavity_worst 569 non-null float64 concave points_worst 569 non-null float64 symmetry_worst 569 non-null float64 fractal_dimension_worst 569 non-null float64 Unnamed: 32 0 non-null float64 dtypes: float64(31), int64(1), object(1) memory usage: 146.8+ KB bisa dilihat dari hasil output diatas ada kolom yang tidak bernama (\"Unnamed\") akan kita hilangkan, serta menghilangkan kolom yang tidak berguna yaitu kolom ()\"id\") : #menghapus kolom yang tidak berguna #menghapus kolom \"id\" data.drop(\"id\",axis=1,inplace=True) #menghapus the \"Unnamed: 32\" column data.drop(\"Unnamed: 32\",axis=1,inplace=True) #hasil data.info() Output: bisa dilihat kolom \"id\" dan kolom yang tidak mempunyai nama \"unnamed\" telah hilang <class 'pandas.core.frame.DataFrame'> RangeIndex: 569 entries, 0 to 568 Data columns (total 31 columns): diagnosis 569 non-null object radius_mean 569 non-null float64 texture_mean 569 non-null float64 perimeter_mean 569 non-null float64 area_mean 569 non-null float64 smoothness_mean 569 non-null float64 compactness_mean 569 non-null float64 concavity_mean 569 non-null float64 concave points_mean 569 non-null float64 symmetry_mean 569 non-null float64 fractal_dimension_mean 569 non-null float64 radius_se 569 non-null float64 texture_se 569 non-null float64 perimeter_se 569 non-null float64 area_se 569 non-null float64 smoothness_se 569 non-null float64 compactness_se 569 non-null float64 concavity_se 569 non-null float64 concave points_se 569 non-null float64 symmetry_se 569 non-null float64 fractal_dimension_se 569 non-null float64 radius_worst 569 non-null float64 texture_worst 569 non-null float64 perimeter_worst 569 non-null float64 area_worst 569 non-null float64 smoothness_worst 569 non-null float64 compactness_worst 569 non-null float64 concavity_worst 569 non-null float64 concave points_worst 569 non-null float64 symmetry_worst 569 non-null float64 fractal_dimension_worst 569 non-null float64 dtypes: float64(30), object(1) memory usage: 137.9+ KB Kemudian kita lihat lagi kolom beserta 5 data pertama: # 5 baris pertama data.head(5) Output: Mengganti class M dan B menjadi 0 dan 1 \u00b6 diagnosis adalah variabel yang bertanggung jawab untuk klasifikasi disini kita mengganti M dan B masing-masing dengan 1 dan 0 #diagnosis adalah variabel yang bertanggung jawab untuk klasifikasi #mengganti M dan B masing-masing dengan 1 dan 0 data.diagnosis=data.diagnosis.map({'M':1,'B':0}) Kemudian kita hitung berapa banyak jumlah masing- masing feature #menghitung variabel diagnosis data.diagnosis.value_counts() output: 0 357 1 212 Name: diagnosis, dtype: int64 Membagi data 30% sebagai data testing dan 70% sebagai data training \u00b6 # preprocessing dataset selesai #splitting dataset ke training dan testing train, test = train_test_split(data, test_size = 0.3,random_state=1234) #mencari hasil print(train.shape) print(test.shape) (398, 31) (171, 31) Membuat variabel independen dan responsible \u00b6 variabel independen dan responsible nantinya akan digunakan dalam proses prediksi variable independen mengambil dari semua kolom dan variable responsible dari diagnosis #membuat variabel independen untuk training train_X = train.iloc[:, 1:31] #membuat variabel responsible untuk training train_y=train.diagnosis #membuat variabel independen untuk testing test_X= test.iloc[:, 1:31] #membuat variabel responsible untuk ttesting test_y =test.diagnosis kita cek dulu berapa jumlahnya #mencari hasil print(train_X.shape) print(train_y.shape) print(test_X.shape) print(test_y.shape) output: (398, 30) (398,) (171, 30) (171,) Seleksi Fitur untuk visualisasi Decision tree \u00b6 dilangkah atas sudah ada seleksi fitur untuk perhitungan decision tree , dilangkah ini seleksi fitur digunakan untuk melengkapi komponen visualisasi decision tree yang dilangkah selanjutnya feature_cols = [\"radius_mean\",\"texture_mean\",\"perimeter_mean\",\"area_mean\",\"smoothness_mean\",\"compactness_mean\",\"concavity_mean\",\"concave points_mean\",\"symmetry_mean\",\"fractal_dimension_mean\",\"radius_se\",\"texture_se\",\"perimeter_se\",\"area_se\",\"smoothness_se\",\"compactness_se\",\"concavity_se\",\"concave points_se\",\"symmetry_se\",\"fractal_dimension_se\",\"radius_worst\",\"texture_worst\",\"perimeter_worst\",\"area_worst\",\"smoothness_worst\",\"compactness_worst\",\"concavity_worst\",\"concave points_worst\",\"symmetry_worst\",\"fractal_dimension_worst\",] X = data[feature_cols] # mengambil Features y = data.diagnosis # Target variable Decision Tree Classifier dengan criterion information gain \u00b6 model_entropy= DecisionTreeClassifier(criterion=\"entropy\",random_state=1234) #learning model_entropy.fit(train_X,train_y) #Prediksi prediction_entropy=model_entropy.predict(test_X) #mengevaluasi(Accuracy) print(\"Accuracy:\",metrics.accuracy_score(prediction,test_y)) #evaluation(Confusion Metrix) print(\"Confusion Metrix:\\n\",metrics.confusion_matrix(prediction,test_y)) Output: Accuracy: 0.9298245614035088 Confusion Metrix: [[99 6] [ 6 60]] Visualisasi Decision Tree dengan criterion information gain \u00b6 dot_data = StringIO() export_graphviz(model_entropy, out_file=dot_data, filled=True, rounded=True, special_characters=True,feature_names = feature_cols,class_names=['M','L']) graph = pydotplus.graph_from_dot_data(dot_data.getvalue()) graph.write_png('entropy.png') Image(graph.create_png()) Output: Decision Tree Classifier dengan criterion gini index \u00b6 model_gini= DecisionTreeClassifier(criterion=\"gini\",random_state=1234) #learning model_gini.fit(train_X,train_y) #Prediksi prediction_gini=model_gini.predict(test_X) #mengevaluasi(Accuracy) print(\"Accuracy:\",metrics.accuracy_score(prediction,test_y)) #evaluation(Confusion Metrix) print(\"Confusion Metrix:\\n\",metrics.confusion_matrix(prediction,test_y)) Output: Accuracy: 0.9298245614035088 Confusion Metrix: [[99 6] [ 6 60]] Visualisasi Decision Tree dengan criterion gini index \u00b6 dot_data = StringIO() export_graphviz(model_gini, out_file=dot_data, filled=True, rounded=True, special_characters=True,feature_names = feature_cols,class_names=['M','L']) graph = pydotplus.graph_from_dot_data(dot_data.getvalue()) graph.write_png('gini.png') Image(graph.create_png()) Output: Menampilkan Hasil Prediksi Data testing \u00b6 Pada langkah ini kita menampilkan hasil prediksi dari data testing dan hasil prediksi menggunakan information gain dan gini index. menampilan semua feature datates dan hasil prediksi menggunakan information gain dan gini index. datatest=pd.DataFrame(test_X) datatest['diagnosis']=test_y datatest['hasil prediksi entropy']=prediction_entropy datatest['hasil prediksi gini']=prediction_gini print(datatest) Output: selajutnya kita akan mensederhanakan agar lebih enak dilihat dengan hanya menampilkan diagnosis, dan hasil prediksi menggunakan information gain dan gini index. final=pd.DataFrame({\"diagnosis\":test_y,\"hasil prediksi entropy\":prediction_entropy,\"hasil prediksi gini\":prediction_gini}) print(final) Output: diagnosis hasil prediksi entropy hasil prediksi gini 531 0 0 0 166 0 0 0 485 0 0 0 66 0 0 0 220 0 0 0 356 0 0 0 414 1 1 1 525 0 0 0 77 1 1 1 239 1 1 1 254 1 1 1 447 0 0 0 301 0 0 0 133 0 0 0 187 0 0 0 78 1 1 1 319 0 0 0 412 0 0 0 349 0 0 0 11 1 1 1 240 0 0 0 29 1 1 1 302 1 1 1 521 1 1 1 373 1 1 1 481 0 0 0 100 1 1 1 304 0 0 0 159 0 0 0 360 0 0 1 .. ... ... ... 202 1 1 1 435 1 1 1 375 0 0 0 47 1 1 0 497 0 0 0 13 1 1 1 221 0 0 0 22 1 1 1 255 1 0 1 109 0 0 0 348 0 0 0 129 1 1 1 152 0 1 0 67 0 0 0 213 1 1 1 495 0 0 1 517 1 1 1 219 1 1 1 290 0 0 0 488 0 0 0 309 0 0 0 6 1 1 1 405 0 0 0 452 0 0 0 54 1 1 1 305 0 0 0 560 0 1 1 285 0 0 0 355 0 0 0 329 1 1 1 [171 rows x 3 columns] Referensi \u00b6 Mayu Shinohara. 2017., Hyper Parameters Tuning of DTree,RF,SVM,kNN di https://www.kaggle.com/mayu0116/hyper-parameters-tuning-of-dtree-rf-svm-knn sklearn.tree.DecisionTreeClassifier. di https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html Avinash Navlani. 2018. ,Decision Tree Classification in Python di https://www.datacamp.com/community/tutorials/decision-tree-classification-python Dafni Sidiropoulou Velidou., 2018. Interactive Visualization of Decision Trees with Jupyter Widgets di https://towardsdatascience.com/interactive-visualization-of-decision-trees-with-jupyter-widgets-ca15dd312084 Rishabh Jain., 2017. Decision Tree. It begins here. di https://medium.com/@rishabhjain_22692/decision-trees-it-begins-here-93ff54ef134 Decision Tree - Classification di https://www.saedsayad.com/decision_tree.htm","title":"Decision Tree Classifier"},{"location":"dtc/#pengertian-decision-tree-classification","text":"Decision tree merupakan suatu metode klasifikasi yang menggunakan struktur pohon, dimana setiap node merepresentasikan atribut dan cabangnya merepresentasikan nilai dari atribut, sedangkan daunnya digunakan untuk merepresentasikan kelas. Node teratas dari decision tree ini disebut dengan root . Ini memecah dataset menjadi himpunan bagian yang lebih kecil dengan peningkatan kedalaman pohon. Hasil akhirnya adalah pohon dengan simpul keputusan dan simpul daun. Node keputusan (mis., Outlook) memiliki dua atau lebih cabang (mis., Sunny, Overcast, dan Rainy). Node daun (mis., Play) mewakili klasifikasi atau keputusan. Node keputusan teratas dalam pohon yang sesuai dengan prediktor terbaik disebut simpul akar. Pohon keputusan dapat menangani data kategorikal dan numerik.","title":"Pengertian Decision Tree Classification"},{"location":"dtc/#jenis-decision-tree","text":"Categorical Variable Decision Tree (Pohon Keputusan Variabel Kategorikal) merupakan pohon keputusan yang memiliki variabel target kategorikal Continuous Variable Decision Tree() merupakan Decision Tree yang memiliki variabel target kontinu","title":"Jenis Decision Tree"},{"location":"dtc/#terminologi-penting-terkait-dengan-decision-tree-classification","text":"Root Node : Ini mewakili seluruh populasi atau sampel dan ini selanjutnya dibagi menjadi dua atau lebih set homogen. Splitting : Ini adalah proses membagi sebuah node menjadi dua atau lebih sub-node. Decision Node : Ketika sebuah sub-node terbagi menjadi beberapa sub-node, maka itu disebut simpul keputusan (decision node). Leaf/ Terminal Node : Node tanpa anak (tanpa pemisahan lebih lanjut) disebut Leaf atau Terminal node. Pruning : Ketika kita mengurangi ukuran pohon keputusan dengan menghapus node (kebalikan dari Splitting), proses ini disebut pemangkasan (pruning). Branch / Sub-Tree : Subbagian pohon keputusan disebut cabang (Branch) atau sub-pohon.( Sub-Tree) Parent and Child Node : Sebuah node, yang dibagi menjadi beberapa sub-node disebut parent node dari sub-node dimana sebagai sub-node adalah anak dari node induk","title":"Terminologi Penting terkait dengan Decision Tree Classification"},{"location":"dtc/#kelebihan-dan-kekurangan-decision-tree-classification","text":"","title":"Kelebihan dan Kekurangan Decision Tree Classification"},{"location":"dtc/#kelebihan","text":"Daerah pengambilan keputusan yang sebelumnya kompleks dan sangat global, dapat diubah menjadi lebih simpel dan spesifik. Eliminasi perhitungan-perhitungan yang tidak diperlukan, karena ketika menggunakan metode pohon keputusan maka sample diuji hanya berdasarkan kriteria atau kelas tertentu. Fleksibel untuk memilih fitur dari internal node yang berbeda, fitur yang terpilih akan membedakan suatu kriteria dibandingkan kriteria yang lain dalam node yang sama. Kefleksibelan metode pohon keputusan ini meningkatkan kualitas keputusan yang dihasilkan jika dibandingkan ketika menggunakan metode penghitungan satu tahap yang lebih konvensional Dalam analisis multivariat, dengan kriteria dan kelas yang jumlahnya sangat banyak, seorang penguji biasanya perlu untuk mengestimasikan baik itu distribusi dimensi tinggi ataupun parameter tertentu dari distribusi kelas tersebut. Metode pohon keputusan dapat menghindari munculnya permasalahan ini dengan menggunakan criteria yang jumlahnya lebih sedikit pada setiap node internal tanpa banyak mengurangi kualitas keputusan yang dihasilkan.","title":"Kelebihan:"},{"location":"dtc/#kekurangan","text":"Terjadi overlap terutama ketika kelas-kelas dan criteria yang digunakan jumlahnya sangat banyak. Hal tersebut juga dapat menyebabkan meningkatnya waktu pengambilan keputusan dan jumlah memori yang diperlukan. Pengakumulasian jumlah eror dari setiap tingkat dalam sebuah pohon keputusan yang besar. Kesulitan dalam mendesain pohon keputusan yang optimal. Hasil kualitas keputusan yang didapatkan dari metode pohon keputusan sangat tergantung pada bagaimana pohon tersebut didesain.","title":"Kekurangan:"},{"location":"dtc/#algoritma-decision-tree-classification","text":"ID3 Gini Index Chi-Square Reduction in Variance Namun disini kita hanya membahan ID3 dan Gini index saja.","title":"Algoritma Decision Tree Classification"},{"location":"dtc/#id3","text":"Algoritma inti untuk membangun pohon keputusan disebut ID3. Dikembangkan oleh J. R. Quinlan, algoritma ini menggunakan pencarian top-down, melalui ruang cabang yang mungkin tanpa backtracking. ID3 menggunakan Entropy dan Information Gain untuk membuat keputusan","title":"ID3"},{"location":"dtc/#entropy","text":"Pohon keputusan dibangun dari atas ke bawah dari simpul akar (root node) dan melibatkan mempartisi data menjadi subset yang berisi instance dengan nilai yang sama (homogen). Algoritma ID3 menggunakan entropi untuk menghitung homogenitas sampel. Jika sampel benar-benar homogen, entropinya nol dan jika sampel dibagi rata maka entropinya satu. Untuk membangun pohon keputusan, kita perlu menghitung dua jenis entropi menggunakan tabel frekuensi sebagai berikut: a. Entropy menggunakan tabel frekuensi satu atribut: b. Entropi menggunakan tabel frekuensi dua atribut:","title":"Entropy"},{"location":"dtc/#information-gain","text":"Information Gain didasarkan pada penurunan entropi setelah kumpulan data dibagi pada atribut. Membangun pohon keputusan adalah tentang menemukan atribut yang mengembalikan perolehan informasi tertinggi (mis., Cabang yang paling homogen). Langkah 1 : Hitung entropi target. Langkah 2 : Kumpulan data kemudian dibagi pada atribut yang berbeda. Entropi untuk setiap cabang dihitung. Kemudian ditambahkan secara proporsional, untuk mendapatkan total entropi untuk pemisahan. Entropi yang dihasilkan dikurangi dari entropi sebelum pemisahan. Hasilnya adalah Information Gain, atau penurunan entropi. Langkah 3 : Pilih atribut dengan perolehan Information Gain terbesar sebagai simpul keputusan (decision node), bagi dataset dengan cabang-cabangnya dan ulangi proses yang sama pada setiap cabang. Langkah 4a : Cabang dengan entropi 0 adalah simpul daun. Langkah 4b : Cabang dengan entropi lebih dari 0 membutuhkan pemisahan lebih lanjut Langkah 5 : Algoritma ID3 dijalankan secara rekursif pada cabang-cabang non-daun, sampai semua data diklasifikasikan.","title":"Information Gain"},{"location":"dtc/#gini-index","text":"Indeks Gini mengatakan, jika kita memilih dua item dari populasi secara acak maka mereka harus dari kelas yang sama dan probabilitas untuk ini adalah 1 jika populasi murni. Ia bekerja dengan variabel target kategori \"Sukses\" atau \"Kegagalan\". Ini hanya melakukan split Biner Semakin tinggi nilai Gini semakin tinggi homogenitasnya. CART (Klasifikasi dan Pohon Regresi) menggunakan metode Gini untuk membuat pemisahan biner.","title":"Gini Index"},{"location":"dtc/#rumus-gini-index","text":"\\operatorname{Gini}(\\mathrm{D})=1-\\sum_{\\mathrm{i}=1}^{m} \\mathrm{Pi}^{2} \\operatorname{Gini}(\\mathrm{D})=1-\\sum_{\\mathrm{i}=1}^{m} \\mathrm{Pi}^{2} pi adalah probabilitas bahwa sebuah tuple dalam D milik kelas Ci. Weighted Gini untuk Pemisahan: $$ \\operatorname{Gini}_{\\mathrm{A}}(\\mathrm{D})=\\frac{|D 1|}{|D|} \\operatorname{Gini}\\left(\\mathrm{D}_{1}\\right)+\\frac{|D 2|}{|D|} \\operatorname{Gini}\\left(\\mathrm{D}_{2}\\right) \\operatorname{Gini}_{\\mathrm{A}}(\\mathrm{D})=\\frac{|D 1|}{|D|} \\operatorname{Gini}\\left(\\mathrm{D}_{1}\\right)+\\frac{|D 2|}{|D|} \\operatorname{Gini}\\left(\\mathrm{D}_{2}\\right) $$","title":"Rumus Gini Index"},{"location":"dtc/#langkah-langkah-untuk-menghitung-gini-untuk-pemisahan","text":"Hitung Gini untuk sub-node, menggunakan rumus jumlah kuadrat probabilitas untuk keberhasilan dan kegagalan (p\u00b2 + q\u00b2). Hitung Gini untuk split menggunakan skor Gini tertimbang dari setiap node dari split itu Contoh: - Mengacu pada contoh di mana kami ingin memisahkan siswa berdasarkan variabel target (playing criket atau tidak). Dalam snapshot di bawah ini, kami membagi populasi menggunakan dua variabel input Gender dan Class. Sekarang, saya ingin mengidentifikasi split mana yang menghasilkan lebih banyak sub-node homogen menggunakan indeks Gini. Pemisahan di gender Gini untuk sub node Female = (0.43)x(0.43)+(0.57)x(0.57)=0.51 Gini untuk sub node Male = (0.56)x(0.56)+(0.44)x(0.44)=0.51 Weighted Gini untuk Pemisahan Gender = (10/30)x0.68+(20/30)x0.55 = 0.59 Pemisahan di class Gini untuk sub node Class IX= (0.2)x(0.2)+(0.8)x(0.8)=0.68 Gini untuk sub node Class X= (0.65)x(0.65)+(0.35)x(0.35)=0.55 Weighted Gini untuk Pemisahan Class = (14/30)x0.51+(16/30)x0.51 = 0.51","title":"Langkah-langkah untuk Menghitung Gini untuk pemisahan"},{"location":"dtc/#implementasi-decision-tree-classification-breast-cancer-wisconsin-diagnostic-dataset","text":"","title":"Implementasi Decision Tree Classification Breast Cancer Wisconsin (Diagnostic) Dataset"},{"location":"dtc/#kebutuhan-software","text":"Python 3.0 atau versi yang lebih baru, disini saya menggunakan python 3.7 IDE Pycharm Jupyter notebook","title":"Kebutuhan Software"},{"location":"dtc/#library-python-yang-digunakan","text":"Pandas pandas adalah sebuah librari berlisensi BSD dan open source yang menyediakan struktur data dan analisis data yang mudah digunakan dan berkinerja tinggi untuk bahasa pemrograman Python. instal pandas: pip install pandas Scikit Learn Machine learning ada yang berbasis statistika ada juga yang tidak. Salah satunya adalah support vector machine dan regresi linier. Mungkin bagi sebagian orang sudah biasa menulis sendiri library untuk implementasi kedua algoritma tadi. Tapi untuk membuatnya dalam waktu singkat tentu butuh waktu yang tidak sedikit pula. Scikit-Learn memberikan sejumlah fitur untuk keperluan data science seperti: Algoritma Regresi Algoritma Naive Bayes Algoritma Clustering Algoritma Decision Tree Parameter Tuning Data Preprocessing Tool Export / Import Model Machine learning pipeline dan lainnya instal Scikit Learn : pip install scikit-learn graphviz kitadapat menggunakan fungsi Scikit-learn export_graphviz untuk menampilkan pohon di dalam notebook Jupyter. Untuk merencanakan pohon, Anda juga perlu menginstal graphviz dan pydotplus instal menggunakan pip: pip install graphviz Instal menggunakan conda: conda install -c conda-forge graphviz pydotplus PyDotPlus adalah versi perbaikan dari proyek pydot lama yang menyediakan Antarmuka Python ke bahasa Dot Graphviz pip install pydotplus Instal menggunakan conda: conda install -c conda-forge pydotplus","title":"Library Python yang digunakan:"},{"location":"dtc/#import-library-yang-dibutuhkan","text":"import pandas as pd from sklearn.model_selection import train_test_split from sklearn import metrics #importing modul metrik from sklearn.tree import DecisionTreeClassifier from sklearn.tree import export_graphviz from sklearn.externals.six import StringIO from IPython.display import Image import pydotplus","title":"Import Library yang dibutuhkan"},{"location":"dtc/#memuat-dataset","text":"Mengimport dataset yang digunakan untuk pengimplemtasian K-Nearest Neighbor dataset bisa didownload disini atau langsung dari kaggle disini # Memuat dataset data = pd.read_csv(\"E:\\Semester 4\\data mining/bc.csv\",encoding = \"ISO-8859-1\")","title":"Memuat Dataset"},{"location":"dtc/#data-preprocessing","text":"","title":"Data Preprocessing"},{"location":"dtc/#menampilkan-5-data-teratas","text":"Menampilkan jumlah masing-masing diagnosis dan kolom diagnosis digunakan sebagai class nantinya. data.head(5) Output:","title":"Menampilkan 5 Data teratas"},{"location":"dtc/#menampilkan-ringkasan-dataset-serta-menghilangkan-kolom-yang-tidak-berguna","text":"# ringkasan dataset data.info() Output: <class 'pandas.core.frame.DataFrame'> RangeIndex: 569 entries, 0 to 568 Data columns (total 33 columns): id 569 non-null int64 diagnosis 569 non-null object radius_mean 569 non-null float64 texture_mean 569 non-null float64 perimeter_mean 569 non-null float64 area_mean 569 non-null float64 smoothness_mean 569 non-null float64 compactness_mean 569 non-null float64 concavity_mean 569 non-null float64 concave points_mean 569 non-null float64 symmetry_mean 569 non-null float64 fractal_dimension_mean 569 non-null float64 radius_se 569 non-null float64 texture_se 569 non-null float64 perimeter_se 569 non-null float64 area_se 569 non-null float64 smoothness_se 569 non-null float64 compactness_se 569 non-null float64 concavity_se 569 non-null float64 concave points_se 569 non-null float64 symmetry_se 569 non-null float64 fractal_dimension_se 569 non-null float64 radius_worst 569 non-null float64 texture_worst 569 non-null float64 perimeter_worst 569 non-null float64 area_worst 569 non-null float64 smoothness_worst 569 non-null float64 compactness_worst 569 non-null float64 concavity_worst 569 non-null float64 concave points_worst 569 non-null float64 symmetry_worst 569 non-null float64 fractal_dimension_worst 569 non-null float64 Unnamed: 32 0 non-null float64 dtypes: float64(31), int64(1), object(1) memory usage: 146.8+ KB bisa dilihat dari hasil output diatas ada kolom yang tidak bernama (\"Unnamed\") akan kita hilangkan, serta menghilangkan kolom yang tidak berguna yaitu kolom ()\"id\") : #menghapus kolom yang tidak berguna #menghapus kolom \"id\" data.drop(\"id\",axis=1,inplace=True) #menghapus the \"Unnamed: 32\" column data.drop(\"Unnamed: 32\",axis=1,inplace=True) #hasil data.info() Output: bisa dilihat kolom \"id\" dan kolom yang tidak mempunyai nama \"unnamed\" telah hilang <class 'pandas.core.frame.DataFrame'> RangeIndex: 569 entries, 0 to 568 Data columns (total 31 columns): diagnosis 569 non-null object radius_mean 569 non-null float64 texture_mean 569 non-null float64 perimeter_mean 569 non-null float64 area_mean 569 non-null float64 smoothness_mean 569 non-null float64 compactness_mean 569 non-null float64 concavity_mean 569 non-null float64 concave points_mean 569 non-null float64 symmetry_mean 569 non-null float64 fractal_dimension_mean 569 non-null float64 radius_se 569 non-null float64 texture_se 569 non-null float64 perimeter_se 569 non-null float64 area_se 569 non-null float64 smoothness_se 569 non-null float64 compactness_se 569 non-null float64 concavity_se 569 non-null float64 concave points_se 569 non-null float64 symmetry_se 569 non-null float64 fractal_dimension_se 569 non-null float64 radius_worst 569 non-null float64 texture_worst 569 non-null float64 perimeter_worst 569 non-null float64 area_worst 569 non-null float64 smoothness_worst 569 non-null float64 compactness_worst 569 non-null float64 concavity_worst 569 non-null float64 concave points_worst 569 non-null float64 symmetry_worst 569 non-null float64 fractal_dimension_worst 569 non-null float64 dtypes: float64(30), object(1) memory usage: 137.9+ KB Kemudian kita lihat lagi kolom beserta 5 data pertama: # 5 baris pertama data.head(5) Output:","title":"Menampilkan ringkasan dataset serta menghilangkan kolom yang tidak berguna"},{"location":"dtc/#mengganti-class-m-dan-b-menjadi-0-dan-1","text":"diagnosis adalah variabel yang bertanggung jawab untuk klasifikasi disini kita mengganti M dan B masing-masing dengan 1 dan 0 #diagnosis adalah variabel yang bertanggung jawab untuk klasifikasi #mengganti M dan B masing-masing dengan 1 dan 0 data.diagnosis=data.diagnosis.map({'M':1,'B':0}) Kemudian kita hitung berapa banyak jumlah masing- masing feature #menghitung variabel diagnosis data.diagnosis.value_counts() output: 0 357 1 212 Name: diagnosis, dtype: int64","title":"Mengganti class M dan B menjadi 0 dan 1"},{"location":"dtc/#membagi-data-30-sebagai-data-testing-dan-70-sebagai-data-training","text":"# preprocessing dataset selesai #splitting dataset ke training dan testing train, test = train_test_split(data, test_size = 0.3,random_state=1234) #mencari hasil print(train.shape) print(test.shape) (398, 31) (171, 31)","title":"Membagi data 30% sebagai data testing dan 70% sebagai data training"},{"location":"dtc/#membuat-variabel-independen-dan-responsible","text":"variabel independen dan responsible nantinya akan digunakan dalam proses prediksi variable independen mengambil dari semua kolom dan variable responsible dari diagnosis #membuat variabel independen untuk training train_X = train.iloc[:, 1:31] #membuat variabel responsible untuk training train_y=train.diagnosis #membuat variabel independen untuk testing test_X= test.iloc[:, 1:31] #membuat variabel responsible untuk ttesting test_y =test.diagnosis kita cek dulu berapa jumlahnya #mencari hasil print(train_X.shape) print(train_y.shape) print(test_X.shape) print(test_y.shape) output: (398, 30) (398,) (171, 30) (171,)","title":"Membuat variabel independen dan responsible"},{"location":"dtc/#seleksi-fitur-untuk-visualisasi-decision-tree","text":"dilangkah atas sudah ada seleksi fitur untuk perhitungan decision tree , dilangkah ini seleksi fitur digunakan untuk melengkapi komponen visualisasi decision tree yang dilangkah selanjutnya feature_cols = [\"radius_mean\",\"texture_mean\",\"perimeter_mean\",\"area_mean\",\"smoothness_mean\",\"compactness_mean\",\"concavity_mean\",\"concave points_mean\",\"symmetry_mean\",\"fractal_dimension_mean\",\"radius_se\",\"texture_se\",\"perimeter_se\",\"area_se\",\"smoothness_se\",\"compactness_se\",\"concavity_se\",\"concave points_se\",\"symmetry_se\",\"fractal_dimension_se\",\"radius_worst\",\"texture_worst\",\"perimeter_worst\",\"area_worst\",\"smoothness_worst\",\"compactness_worst\",\"concavity_worst\",\"concave points_worst\",\"symmetry_worst\",\"fractal_dimension_worst\",] X = data[feature_cols] # mengambil Features y = data.diagnosis # Target variable","title":"Seleksi Fitur untuk visualisasi Decision tree"},{"location":"dtc/#decision-tree-classifier-dengan-criterion-information-gain","text":"model_entropy= DecisionTreeClassifier(criterion=\"entropy\",random_state=1234) #learning model_entropy.fit(train_X,train_y) #Prediksi prediction_entropy=model_entropy.predict(test_X) #mengevaluasi(Accuracy) print(\"Accuracy:\",metrics.accuracy_score(prediction,test_y)) #evaluation(Confusion Metrix) print(\"Confusion Metrix:\\n\",metrics.confusion_matrix(prediction,test_y)) Output: Accuracy: 0.9298245614035088 Confusion Metrix: [[99 6] [ 6 60]]","title":"Decision Tree Classifier dengan criterion information gain"},{"location":"dtc/#visualisasi-decision-tree-dengan-criterion-information-gain","text":"dot_data = StringIO() export_graphviz(model_entropy, out_file=dot_data, filled=True, rounded=True, special_characters=True,feature_names = feature_cols,class_names=['M','L']) graph = pydotplus.graph_from_dot_data(dot_data.getvalue()) graph.write_png('entropy.png') Image(graph.create_png()) Output:","title":"Visualisasi Decision Tree dengan criterion information gain"},{"location":"dtc/#decision-tree-classifier-dengan-criterion-gini-index","text":"model_gini= DecisionTreeClassifier(criterion=\"gini\",random_state=1234) #learning model_gini.fit(train_X,train_y) #Prediksi prediction_gini=model_gini.predict(test_X) #mengevaluasi(Accuracy) print(\"Accuracy:\",metrics.accuracy_score(prediction,test_y)) #evaluation(Confusion Metrix) print(\"Confusion Metrix:\\n\",metrics.confusion_matrix(prediction,test_y)) Output: Accuracy: 0.9298245614035088 Confusion Metrix: [[99 6] [ 6 60]]","title":"Decision Tree Classifier dengan criterion gini index"},{"location":"dtc/#visualisasi-decision-tree-dengan-criterion-gini-index","text":"dot_data = StringIO() export_graphviz(model_gini, out_file=dot_data, filled=True, rounded=True, special_characters=True,feature_names = feature_cols,class_names=['M','L']) graph = pydotplus.graph_from_dot_data(dot_data.getvalue()) graph.write_png('gini.png') Image(graph.create_png()) Output:","title":"Visualisasi Decision Tree dengan criterion gini index"},{"location":"dtc/#menampilkan-hasil-prediksi-data-testing","text":"Pada langkah ini kita menampilkan hasil prediksi dari data testing dan hasil prediksi menggunakan information gain dan gini index. menampilan semua feature datates dan hasil prediksi menggunakan information gain dan gini index. datatest=pd.DataFrame(test_X) datatest['diagnosis']=test_y datatest['hasil prediksi entropy']=prediction_entropy datatest['hasil prediksi gini']=prediction_gini print(datatest) Output: selajutnya kita akan mensederhanakan agar lebih enak dilihat dengan hanya menampilkan diagnosis, dan hasil prediksi menggunakan information gain dan gini index. final=pd.DataFrame({\"diagnosis\":test_y,\"hasil prediksi entropy\":prediction_entropy,\"hasil prediksi gini\":prediction_gini}) print(final) Output: diagnosis hasil prediksi entropy hasil prediksi gini 531 0 0 0 166 0 0 0 485 0 0 0 66 0 0 0 220 0 0 0 356 0 0 0 414 1 1 1 525 0 0 0 77 1 1 1 239 1 1 1 254 1 1 1 447 0 0 0 301 0 0 0 133 0 0 0 187 0 0 0 78 1 1 1 319 0 0 0 412 0 0 0 349 0 0 0 11 1 1 1 240 0 0 0 29 1 1 1 302 1 1 1 521 1 1 1 373 1 1 1 481 0 0 0 100 1 1 1 304 0 0 0 159 0 0 0 360 0 0 1 .. ... ... ... 202 1 1 1 435 1 1 1 375 0 0 0 47 1 1 0 497 0 0 0 13 1 1 1 221 0 0 0 22 1 1 1 255 1 0 1 109 0 0 0 348 0 0 0 129 1 1 1 152 0 1 0 67 0 0 0 213 1 1 1 495 0 0 1 517 1 1 1 219 1 1 1 290 0 0 0 488 0 0 0 309 0 0 0 6 1 1 1 405 0 0 0 452 0 0 0 54 1 1 1 305 0 0 0 560 0 1 1 285 0 0 0 355 0 0 0 329 1 1 1 [171 rows x 3 columns]","title":"Menampilkan Hasil Prediksi Data testing"},{"location":"dtc/#referensi","text":"Mayu Shinohara. 2017., Hyper Parameters Tuning of DTree,RF,SVM,kNN di https://www.kaggle.com/mayu0116/hyper-parameters-tuning-of-dtree-rf-svm-knn sklearn.tree.DecisionTreeClassifier. di https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html Avinash Navlani. 2018. ,Decision Tree Classification in Python di https://www.datacamp.com/community/tutorials/decision-tree-classification-python Dafni Sidiropoulou Velidou., 2018. Interactive Visualization of Decision Trees with Jupyter Widgets di https://towardsdatascience.com/interactive-visualization-of-decision-trees-with-jupyter-widgets-ca15dd312084 Rishabh Jain., 2017. Decision Tree. It begins here. di https://medium.com/@rishabhjain_22692/decision-trees-it-begins-here-93ff54ef134 Decision Tree - Classification di https://www.saedsayad.com/decision_tree.htm","title":"Referensi"},{"location":"kmean/","text":"Pengertian K-Means Clustering \u00b6 Clustering merupakan suatu metode untuk mencari dan mengelompokkan data yang memiliki kemiripan karakteristik (similarity) antara satu data dengan data yang lain. Clustering merupakan salah satu metode data mining yang bersifat tanpa arahan (unsupervised). Yang dimaksud metode unsupervised yaitu metode ini diterapkan tanpa adanya latihan (training) dan guru (teacher) serta tidak memerlukan target output. Ada dua jenis data clustering yang sering dipergunakan dalam proses pengelompokan data yaitu hierarchical (hirarki) data clustering dan non-hierarchical (non hirarki) data clustering. K-Means merupakan salah satu metode data clustering non hirarki yang berusaha mempartisi data yang ada ke dalam bentuk satu atau lebih cluster/kelompok. Metode ini mempartisi data ke dalam cluster/kelompok sehingga data yang memiliki karakteristik yang sama dikelompokkan ke dalam satu cluster yang sama dan data yang mempunyai karakteristik yang berbeda dikelompokkan ke dalam kelompok yang lain. Adapun tujuan dari data clustering ini adalah untuk meminimalisasikan objective function yang diset dalam proses clustering, yang pada umumnya berusaha meminimalisasikan variasi di dalam suatu cluster dan memaksimalisasikan variasi antar cluster. Kelebihan dan Kekurangan K-Means Clustering \u00b6 Kelebihan K-Means Clustering \u00b6 Menggunakan prinsip yang sederhana, dapat dijelaskan dalam non-statistik Mudah untuk diimplementasikan dan dijalankan. Waktu yang dibutuhkan untuk menjalankan nya relatif cepat Sangat fleksibel, dapat dengan mudah diadaptasi. Sangat umum digunakan Kekurangan K-Means Clustering \u00b6 dalam non-statistik Karena menggunakan k buah acak, tidak di jamin untuk menemukan kumpulan cluster yang optimal dapat terjadinya curse of dimensionality, apabila jarak antara cluster yang satu dengan yang lain memiliki banyak dimesi. Tidak optimal digunakan untuk data yang jumlahnya terlalu banyak sampai bermiliyar. Algoritma K-Means Clustering \u00b6 Langkah-langkah algortima K-Means Clustering : \u00b6 Tentukan jumlah cluster(kelompok) yang kita inginkan. Inisiasi centroid untuk setiap cluster secara acak. Centroid adalah data yang merepresentasikan suatu kelompok. Hitung kedekatan suatu data terhadap centroid, kemudian masukkan data tersebut ke cluster yang centroid-nya memiliki sifat terdekat dengan dirinya(yang nilainya paling kecil). Pilih kembali centroid untuk masing-masing cluster ,yaitu mean (rata-rata) nilai data dari setiap cluster yang sama . Ulangi langkah ke-3 sampai tidak ada perubahan anggota untuk semua cluster, atau sampai batas yang ditentukan dari perulangan anggota yang sama. Contoh Perhitungan K-Means Sederhana \u00b6 Perhatikan gambar diatas, ada tabel yang mempunyai column X1 dan X2, dimana tabel tersebut akan kita pakai untuk mempelajari k-means clustering. Pertama-tama kita akan menentukan jumlah cluster (kelompok) yakni kita tentukan k = 2. tahap selanjutnya adalah inisiasi centroid karena k = 2 , maka kita akan memilih 2 observasi secara acak dari tabel, yakni A dan C. setelah itu kita akan menghitung jarak antar data dengan centroid yang telah ditentukan, dengan menggunakan formula euclidean distance. $$ \\begin{aligned} d(p, q)=d(q, p) &=\\sqrt{\\left(q_{1}-p_{1}\\right) {2}+\\left(q_{2}-p_{2}\\right) {2}+\\cdots+\\left(q_{n}-p_{n}\\right)^{2}} \\ &=\\sqrt{\\sum_{i=1} {n}\\left(q_{1}-p_{i}\\right) {2}} \\end{aligned} $$ Maka pertama-tama kita akan menghitung observasi A dengan centroid 1 : =\u221a(1-1)^2 + (1-1)^2 =\u221a(0)^2 + (0)^2 =\u221a0 =0 Setelah itu menghitung observasi A dengan centroid 2: =\u221a(1-0)^2 + (1-2)^2 =\u221a(1)^2 + (1)^2 =\u221a2 =1,4 Lakukan perhitungan sampai semua observasi dihitung. Hitung kedekatan suatu data terhadap centroid, kemudian masukkan data tersebut ke cluster yang centroid-nya memiliki sifat terdekat dengan dirinya (yang nilainya paling kecil). Pada observasi A yang paling kecil nilainya adalah 0 maka masuk ke dalam cluster 1. Pilih kembali centroid untuk masing-masing cluster ,yaitu mean (rata-rata) nilai data dari setiap cluster yang sama . observasi A dan B dalam cluster yang sama yakni cluster 1, maka dihitung untuk centroid 1 = (1+1 / 2 , 1+ 0 /2 ) -> (1,0.5). dan centroid 2 = (1.7,3.7). Lakukan penghitungan lagi dengan centroid yang baru. Pilih kembali centroid untuk masing-masing cluster ,yaitu mean (rata-rata) nilai data dari setiap cluster yang sama. Ulangi sampai tidak ada perubahan anggota untuk semua cluster, atau sampai batas yang ditentukan dari perulangan anggota yang sama. Software Requirements \u00b6 Python 3.0 atau versi yang lebih baru, disini saya menggunakan python 3.7 IDE Pycharm Library Python yang digunakan: \u00b6 Numpy Numpy merupakan sebuah library pada Python yang berfungsi untuk melakukan operasi vektor dan matriks dengan mengolah array dan array multidimensi. Biasanya NumPy digunakan untuk kebutuhan dalam menganalisis data. instal numpy: pip install numpy Pandas pandas adalah sebuah librari berlisensi BSD dan open source yang menyediakan struktur data dan analisis data yang mudah digunakan dan berkinerja tinggi untuk bahasa pemrograman Python. instal pandas: pip install pandas Matplotlib Matplotlib adalah library paling banyak digunakan oleh data science untuk menyajikan datanya ke dalam visual yang lebih baik. instal matplotlib: pip install matplotlib Scikit Learn Machine learning ada yang berbasis statistika ada juga yang tidak. Salah satunya adalah support vector machine dan regresi linier. Mungkin bagi sebagian orang sudah biasa menulis sendiri library untuk implementasi kedua algoritma tadi. Tapi untuk membuatnya dalam waktu singkat tentu butuh waktu yang tidak sedikit pula. Scikit-Learn memberikan sejumlah fitur untuk keperluan data science seperti: Algoritma Regresi Algoritma Naive Bayes Algoritma Clustering Algoritma Decision Tree Parameter Tuning Data Preprocessing Tool Export / Import Model Machine learning pipeline dan lainnya instal Scikit Learn : pip install scikit-learn Implementasi K-Means Clustering \u00b6 Import Library yang dibutuhkan \u00b6 Mengimportkan library untuk mendukung implementasi k means clustering import pandas as pd import numpy as np import matplotlib.pyplot as plt from sklearn.preprocessing import StandardScaler from sklearn.cluster import KMeans Memuat Dataset \u00b6 Mengimport dataset yang digunakan untuk pengimplemtasian k means clustering dataset bisa didownload disini dataset=pd.read_csv(\"../data.csv\",encoding = \"ISO-8859-1\") dataset.shape Data Preprocessing \u00b6 Menghilangkan duplikat entri pada dataset. print (dataset.duplicated().sum()) dataset.drop_duplicates(inplace = True) dataset.shape Menghilangkan missing value di kolom customer id. dataset.dropna(axis = 0, subset =['CustomerID'], inplace = True) dataset.shape mengecek data apakah ada yg null. jika tidak ada yang null bisa di comment (#) print (pd.DataFrame(dataset.isnull().sum())) menghilangkan order yang dicancel. dataset = dataset[(dataset.InvoiceNo).apply(lambda x:( 'C' not in x))] dataset.shape # print(dataset.shape) df_customerid_groups=dataset.groupby(\"CustomerID\") Membuat dataframe baru dan memilihnya \u00b6 Membuat dataframe baru 'Quantity', 'UnitPrice', 'CustomerID'. df_cluster=pd.DataFrame(columns=['Quantity','UnitPrice','CustomerID']) count=0 for k,v in (df_customerid_groups): df_cluster.loc[count] = [(v['Quantity'].sum()), v['UnitPrice'].sum(), k] count+=1 df_cluster.shape disini kita hanya memakai kolom 'Quantity', 'UnitPrice' untuk di kelompokan X = df_cluster.iloc[:, [0, 1]].values #mengambil data Menggunakan Elbow method untuk mencari jumlah cluster \u00b6 Menggunakan elbow method untuk mencari jumlah cluster yang ideal kemudian diplot for i in range(1,11): # kmeans = KMeans(n_clusters = i, init ='k-means++',max_iter=300,n_init=10) kmeans.fit(X) wcss.append(kmeans.inertia_) plt.plot(range(1,11) , wcss) plt.title('The Elbow Method') plt.xlabel('Jumlah Kelompok Customer (kelompok jenis customer)') plt.ylabel('With in cluster sum of squers(WCSS)') plt.show() Keterangan init Metode inisialisasi, default ke \u2018k-means ++\u2019: n_cluster Jumlah cluster serta jumlah centroid yang dihasilkan k-mean++ metode inisialisasi acak untuk centroid max_iter Jumlah maksimum iterasi dari algoritma k-means untuk sekali jalan n_init Jumlah waktu algoritma k-means akan dijalankan dengan centroid yang berbeda. Hasil ploting elbow method: Menghitung K-Mean \u00b6 Mefitting k-mean ke dataset kmeans = KMeans(n_clusters = int(input(\"Masukan Jumlah Clusters:\")), init = 'k-means++') y_kmeans = kmeans.fit_predict(X) mengvisualisasikan cluster plt.scatter(X[y_kmeans == 0, 0], X[y_kmeans == 0, 1], s = 50, c = 'red', label = 'Customer Type 1') plt.scatter(X[y_kmeans == 1, 0], X[y_kmeans == 1, 1], s = 50, c = 'blue', label = 'Customer Type 2') plt.scatter(X[y_kmeans == 2, 0], X[y_kmeans == 2, 1], s = 50, c = 'green', label = 'Customer Type 3') plt.scatter(kmeans.cluster_centers_[:, 0], kmeans.cluster_centers_[:, 1], s = 100,marker='x', c = 'black', label = 'Centroids') plt.title('Jumlah Kelompok Customer (kelompok jenis customer') plt.xlabel('Jumlah barang yang Dibeli(Quantity)') plt.ylabel('Harga produk per unit dalam sterling(Unit Price)') plt.legend() plt.show() Masukan jumlah cluster disini saya isikan 3 Hasilnya : Dataset dan Program bisa didownload dan dilihat disini Referensi \u00b6 informatika. 2017. Algoritma K-Means Clustering di https://informatikalogi.com/algorithm/k-means/ Naftali Harris. 2017. Visualizing K-Means Clustering di https://www.naftaliharris.com/blog/visualizing-k-means-clustering/ Mubaris NK. 2017. K-Means Clustering in Python di https://mubaris.com/posts/kmeans-clustering/","title":"K-Means Clustering"},{"location":"kmean/#pengertian-k-means-clustering","text":"Clustering merupakan suatu metode untuk mencari dan mengelompokkan data yang memiliki kemiripan karakteristik (similarity) antara satu data dengan data yang lain. Clustering merupakan salah satu metode data mining yang bersifat tanpa arahan (unsupervised). Yang dimaksud metode unsupervised yaitu metode ini diterapkan tanpa adanya latihan (training) dan guru (teacher) serta tidak memerlukan target output. Ada dua jenis data clustering yang sering dipergunakan dalam proses pengelompokan data yaitu hierarchical (hirarki) data clustering dan non-hierarchical (non hirarki) data clustering. K-Means merupakan salah satu metode data clustering non hirarki yang berusaha mempartisi data yang ada ke dalam bentuk satu atau lebih cluster/kelompok. Metode ini mempartisi data ke dalam cluster/kelompok sehingga data yang memiliki karakteristik yang sama dikelompokkan ke dalam satu cluster yang sama dan data yang mempunyai karakteristik yang berbeda dikelompokkan ke dalam kelompok yang lain. Adapun tujuan dari data clustering ini adalah untuk meminimalisasikan objective function yang diset dalam proses clustering, yang pada umumnya berusaha meminimalisasikan variasi di dalam suatu cluster dan memaksimalisasikan variasi antar cluster.","title":"Pengertian K-Means Clustering"},{"location":"kmean/#kelebihan-dan-kekurangan-k-means-clustering","text":"","title":"Kelebihan dan Kekurangan K-Means Clustering"},{"location":"kmean/#kelebihan-k-means-clustering","text":"Menggunakan prinsip yang sederhana, dapat dijelaskan dalam non-statistik Mudah untuk diimplementasikan dan dijalankan. Waktu yang dibutuhkan untuk menjalankan nya relatif cepat Sangat fleksibel, dapat dengan mudah diadaptasi. Sangat umum digunakan","title":"Kelebihan K-Means Clustering"},{"location":"kmean/#kekurangan-k-means-clustering","text":"dalam non-statistik Karena menggunakan k buah acak, tidak di jamin untuk menemukan kumpulan cluster yang optimal dapat terjadinya curse of dimensionality, apabila jarak antara cluster yang satu dengan yang lain memiliki banyak dimesi. Tidak optimal digunakan untuk data yang jumlahnya terlalu banyak sampai bermiliyar.","title":"Kekurangan K-Means Clustering"},{"location":"kmean/#algoritma-k-means-clustering","text":"","title":"Algoritma K-Means Clustering"},{"location":"kmean/#langkah-langkah-algortima-k-means-clustering","text":"Tentukan jumlah cluster(kelompok) yang kita inginkan. Inisiasi centroid untuk setiap cluster secara acak. Centroid adalah data yang merepresentasikan suatu kelompok. Hitung kedekatan suatu data terhadap centroid, kemudian masukkan data tersebut ke cluster yang centroid-nya memiliki sifat terdekat dengan dirinya(yang nilainya paling kecil). Pilih kembali centroid untuk masing-masing cluster ,yaitu mean (rata-rata) nilai data dari setiap cluster yang sama . Ulangi langkah ke-3 sampai tidak ada perubahan anggota untuk semua cluster, atau sampai batas yang ditentukan dari perulangan anggota yang sama.","title":"Langkah-langkah algortima K-Means Clustering :"},{"location":"kmean/#contoh-perhitungan-k-means-sederhana","text":"Perhatikan gambar diatas, ada tabel yang mempunyai column X1 dan X2, dimana tabel tersebut akan kita pakai untuk mempelajari k-means clustering. Pertama-tama kita akan menentukan jumlah cluster (kelompok) yakni kita tentukan k = 2. tahap selanjutnya adalah inisiasi centroid karena k = 2 , maka kita akan memilih 2 observasi secara acak dari tabel, yakni A dan C. setelah itu kita akan menghitung jarak antar data dengan centroid yang telah ditentukan, dengan menggunakan formula euclidean distance. $$ \\begin{aligned} d(p, q)=d(q, p) &=\\sqrt{\\left(q_{1}-p_{1}\\right) {2}+\\left(q_{2}-p_{2}\\right) {2}+\\cdots+\\left(q_{n}-p_{n}\\right)^{2}} \\ &=\\sqrt{\\sum_{i=1} {n}\\left(q_{1}-p_{i}\\right) {2}} \\end{aligned} $$ Maka pertama-tama kita akan menghitung observasi A dengan centroid 1 : =\u221a(1-1)^2 + (1-1)^2 =\u221a(0)^2 + (0)^2 =\u221a0 =0 Setelah itu menghitung observasi A dengan centroid 2: =\u221a(1-0)^2 + (1-2)^2 =\u221a(1)^2 + (1)^2 =\u221a2 =1,4 Lakukan perhitungan sampai semua observasi dihitung. Hitung kedekatan suatu data terhadap centroid, kemudian masukkan data tersebut ke cluster yang centroid-nya memiliki sifat terdekat dengan dirinya (yang nilainya paling kecil). Pada observasi A yang paling kecil nilainya adalah 0 maka masuk ke dalam cluster 1. Pilih kembali centroid untuk masing-masing cluster ,yaitu mean (rata-rata) nilai data dari setiap cluster yang sama . observasi A dan B dalam cluster yang sama yakni cluster 1, maka dihitung untuk centroid 1 = (1+1 / 2 , 1+ 0 /2 ) -> (1,0.5). dan centroid 2 = (1.7,3.7). Lakukan penghitungan lagi dengan centroid yang baru. Pilih kembali centroid untuk masing-masing cluster ,yaitu mean (rata-rata) nilai data dari setiap cluster yang sama. Ulangi sampai tidak ada perubahan anggota untuk semua cluster, atau sampai batas yang ditentukan dari perulangan anggota yang sama.","title":"Contoh Perhitungan K-Means Sederhana"},{"location":"kmean/#software-requirements","text":"Python 3.0 atau versi yang lebih baru, disini saya menggunakan python 3.7 IDE Pycharm","title":"Software Requirements"},{"location":"kmean/#library-python-yang-digunakan","text":"Numpy Numpy merupakan sebuah library pada Python yang berfungsi untuk melakukan operasi vektor dan matriks dengan mengolah array dan array multidimensi. Biasanya NumPy digunakan untuk kebutuhan dalam menganalisis data. instal numpy: pip install numpy Pandas pandas adalah sebuah librari berlisensi BSD dan open source yang menyediakan struktur data dan analisis data yang mudah digunakan dan berkinerja tinggi untuk bahasa pemrograman Python. instal pandas: pip install pandas Matplotlib Matplotlib adalah library paling banyak digunakan oleh data science untuk menyajikan datanya ke dalam visual yang lebih baik. instal matplotlib: pip install matplotlib Scikit Learn Machine learning ada yang berbasis statistika ada juga yang tidak. Salah satunya adalah support vector machine dan regresi linier. Mungkin bagi sebagian orang sudah biasa menulis sendiri library untuk implementasi kedua algoritma tadi. Tapi untuk membuatnya dalam waktu singkat tentu butuh waktu yang tidak sedikit pula. Scikit-Learn memberikan sejumlah fitur untuk keperluan data science seperti: Algoritma Regresi Algoritma Naive Bayes Algoritma Clustering Algoritma Decision Tree Parameter Tuning Data Preprocessing Tool Export / Import Model Machine learning pipeline dan lainnya instal Scikit Learn : pip install scikit-learn","title":"Library Python yang digunakan:"},{"location":"kmean/#implementasi-k-means-clustering","text":"","title":"Implementasi K-Means Clustering"},{"location":"kmean/#import-library-yang-dibutuhkan","text":"Mengimportkan library untuk mendukung implementasi k means clustering import pandas as pd import numpy as np import matplotlib.pyplot as plt from sklearn.preprocessing import StandardScaler from sklearn.cluster import KMeans","title":"Import Library yang dibutuhkan"},{"location":"kmean/#memuat-dataset","text":"Mengimport dataset yang digunakan untuk pengimplemtasian k means clustering dataset bisa didownload disini dataset=pd.read_csv(\"../data.csv\",encoding = \"ISO-8859-1\") dataset.shape","title":"Memuat Dataset"},{"location":"kmean/#data-preprocessing","text":"Menghilangkan duplikat entri pada dataset. print (dataset.duplicated().sum()) dataset.drop_duplicates(inplace = True) dataset.shape Menghilangkan missing value di kolom customer id. dataset.dropna(axis = 0, subset =['CustomerID'], inplace = True) dataset.shape mengecek data apakah ada yg null. jika tidak ada yang null bisa di comment (#) print (pd.DataFrame(dataset.isnull().sum())) menghilangkan order yang dicancel. dataset = dataset[(dataset.InvoiceNo).apply(lambda x:( 'C' not in x))] dataset.shape # print(dataset.shape) df_customerid_groups=dataset.groupby(\"CustomerID\")","title":"Data Preprocessing"},{"location":"kmean/#membuat-dataframe-baru-dan-memilihnya","text":"Membuat dataframe baru 'Quantity', 'UnitPrice', 'CustomerID'. df_cluster=pd.DataFrame(columns=['Quantity','UnitPrice','CustomerID']) count=0 for k,v in (df_customerid_groups): df_cluster.loc[count] = [(v['Quantity'].sum()), v['UnitPrice'].sum(), k] count+=1 df_cluster.shape disini kita hanya memakai kolom 'Quantity', 'UnitPrice' untuk di kelompokan X = df_cluster.iloc[:, [0, 1]].values #mengambil data","title":"Membuat dataframe baru dan memilihnya"},{"location":"kmean/#menggunakan-elbow-method-untuk-mencari-jumlah-cluster","text":"Menggunakan elbow method untuk mencari jumlah cluster yang ideal kemudian diplot for i in range(1,11): # kmeans = KMeans(n_clusters = i, init ='k-means++',max_iter=300,n_init=10) kmeans.fit(X) wcss.append(kmeans.inertia_) plt.plot(range(1,11) , wcss) plt.title('The Elbow Method') plt.xlabel('Jumlah Kelompok Customer (kelompok jenis customer)') plt.ylabel('With in cluster sum of squers(WCSS)') plt.show() Keterangan init Metode inisialisasi, default ke \u2018k-means ++\u2019: n_cluster Jumlah cluster serta jumlah centroid yang dihasilkan k-mean++ metode inisialisasi acak untuk centroid max_iter Jumlah maksimum iterasi dari algoritma k-means untuk sekali jalan n_init Jumlah waktu algoritma k-means akan dijalankan dengan centroid yang berbeda. Hasil ploting elbow method:","title":"Menggunakan Elbow method untuk mencari jumlah cluster"},{"location":"kmean/#menghitung-k-mean","text":"Mefitting k-mean ke dataset kmeans = KMeans(n_clusters = int(input(\"Masukan Jumlah Clusters:\")), init = 'k-means++') y_kmeans = kmeans.fit_predict(X) mengvisualisasikan cluster plt.scatter(X[y_kmeans == 0, 0], X[y_kmeans == 0, 1], s = 50, c = 'red', label = 'Customer Type 1') plt.scatter(X[y_kmeans == 1, 0], X[y_kmeans == 1, 1], s = 50, c = 'blue', label = 'Customer Type 2') plt.scatter(X[y_kmeans == 2, 0], X[y_kmeans == 2, 1], s = 50, c = 'green', label = 'Customer Type 3') plt.scatter(kmeans.cluster_centers_[:, 0], kmeans.cluster_centers_[:, 1], s = 100,marker='x', c = 'black', label = 'Centroids') plt.title('Jumlah Kelompok Customer (kelompok jenis customer') plt.xlabel('Jumlah barang yang Dibeli(Quantity)') plt.ylabel('Harga produk per unit dalam sterling(Unit Price)') plt.legend() plt.show() Masukan jumlah cluster disini saya isikan 3 Hasilnya : Dataset dan Program bisa didownload dan dilihat disini","title":"Menghitung K-Mean"},{"location":"kmean/#referensi","text":"informatika. 2017. Algoritma K-Means Clustering di https://informatikalogi.com/algorithm/k-means/ Naftali Harris. 2017. Visualizing K-Means Clustering di https://www.naftaliharris.com/blog/visualizing-k-means-clustering/ Mubaris NK. 2017. K-Means Clustering in Python di https://mubaris.com/posts/kmeans-clustering/","title":"Referensi"},{"location":"kmean1/","text":"Pengertian K-Means Clustering \u00b6 Clustering merupakan suatu metode untuk mencari dan mengelompokkan data yang memiliki kemiripan karakteristik (similarity) antara satu data dengan data yang lain. Clustering merupakan salah satu metode data mining yang bersifat tanpa arahan (unsupervised). Yang dimaksud metode unsupervised yaitu metode ini diterapkan tanpa adanya latihan (training) dan guru (teacher) serta tidak memerlukan target output. Ada dua jenis data clustering yang sering dipergunakan dalam proses pengelompokan data yaitu hierarchical (hirarki) data clustering dan non-hierarchical (non hirarki) data clustering. K-Means merupakan salah satu metode data clustering non hirarki yang berusaha mempartisi data yang ada ke dalam bentuk satu atau lebih cluster/kelompok. Metode ini mempartisi data ke dalam cluster/kelompok sehingga data yang memiliki karakteristik yang sama dikelompokkan ke dalam satu cluster yang sama dan data yang mempunyai karakteristik yang berbeda dikelompokkan ke dalam kelompok yang lain. Adapun tujuan dari data clustering ini adalah untuk meminimalisasikan objective function yang diset dalam proses clustering, yang pada umumnya berusaha meminimalisasikan variasi di dalam suatu cluster dan memaksimalisasikan variasi antar cluster. Kelebihan dan Kekurangan K-Means Clustering \u00b6 Kelebihan K-Means Clustering \u00b6 Menggunakan prinsip yang sederhana, dapat dijelaskan dalam non-statistik Mudah untuk diimplementasikan dan dijalankan. Waktu yang dibutuhkan untuk menjalankan nya relatif cepat Sangat fleksibel, dapat dengan mudah diadaptasi. Sangat umum digunakan Kekurangan K-Means Clustering \u00b6 dalam non-statistik Karena menggunakan k buah acak, tidak di jamin untuk menemukan kumpulan cluster yang optimal dapat terjadinya curse of dimensionality, apabila jarak antara cluster yang satu dengan yang lain memiliki banyak dimesi. Tidak optimal digunakan untuk data yang jumlahnya terlalu banyak sampai bermiliyar. Algoritma K-Means Clustering \u00b6 Langkah-langkah algortima K-Means Clustering : \u00b6 Tentukan jumlah cluster(kelompok) yang kita inginkan. Inisiasi centroid untuk setiap cluster secara acak. Centroid adalah data yang merepresentasikan suatu kelompok. Hitung kedekatan suatu data terhadap centroid, kemudian masukkan data tersebut ke cluster yang centroid-nya memiliki sifat terdekat dengan dirinya(yang nilainya paling kecil). Pilih kembali centroid untuk masing-masing cluster ,yaitu mean (rata-rata) nilai data dari setiap cluster yang sama . Ulangi langkah ke-3 sampai tidak ada perubahan anggota untuk semua cluster, atau sampai batas yang ditentukan dari perulangan anggota yang sama. Contoh Perhitungan K-Means Sederhana \u00b6 Perhatikan gambar diatas, ada tabel yang mempunyai column X1 dan X2, dimana tabel tersebut akan kita pakai untuk mempelajari k-means clustering. Pertama-tama kita akan menentukan jumlah cluster (kelompok) yakni kita tentukan k = 2. tahap selanjutnya adalah inisiasi centroid karena k = 2 , maka kita akan memilih 2 observasi secara acak dari tabel, yakni A dan C. setelah itu kita akan menghitung jarak antar data dengan centroid yang telah ditentukan, dengan menggunakan formula euclidean distance. $$ \\begin{aligned} d(p, q)=d(q, p) &=\\sqrt{\\left(q_{1}-p_{1}\\right) {2}+\\left(q_{2}-p_{2}\\right) {2}+\\cdots+\\left(q_{n}-p_{n}\\right)^{2}} \\ &=\\sqrt{\\sum_{i=1} {n}\\left(q_{1}-p_{i}\\right) {2}} \\end{aligned} $$ Maka pertama-tama kita akan menghitung observasi A dengan centroid 1 : =\u221a(1-1)^2 + (1-1)^2 =\u221a(0)^2 + (0)^2 =\u221a0 =0 Setelah itu menghitung observasi A dengan centroid 2: =\u221a(1-0)^2 + (1-2)^2 =\u221a(1)^2 + (1)^2 =\u221a2 =1,4 Lakukan perhitungan sampai semua observasi dihitung. Hitung kedekatan suatu data terhadap centroid, kemudian masukkan data tersebut ke cluster yang centroid-nya memiliki sifat terdekat dengan dirinya (yang nilainya paling kecil). Pada observasi A yang paling kecil nilainya adalah 0 maka masuk ke dalam cluster 1. Pilih kembali centroid untuk masing-masing cluster ,yaitu mean (rata-rata) nilai data dari setiap cluster yang sama . observasi A dan B dalam cluster yang sama yakni cluster 1, maka dihitung untuk centroid 1 = (1+1 / 2 , 1+ 0 /2 ) -> (1,0.5). dan centroid 2 = (1.7,3.7). Lakukan penghitungan lagi dengan centroid yang baru. Pilih kembali centroid untuk masing-masing cluster ,yaitu mean (rata-rata) nilai data dari setiap cluster yang sama. Ulangi sampai tidak ada perubahan anggota untuk semua cluster, atau sampai batas yang ditentukan dari perulangan anggota yang sama. Software Requirements \u00b6 Python 3.0 atau versi yang lebih baru, disini saya menggunakan python 3.7 IDE Pycharm Library Python yang digunakan: \u00b6 Numpy Numpy merupakan sebuah library pada Python yang berfungsi untuk melakukan operasi vektor dan matriks dengan mengolah array dan array multidimensi. Biasanya NumPy digunakan untuk kebutuhan dalam menganalisis data. instal numpy: pip install numpy Pandas pandas adalah sebuah librari berlisensi BSD dan open source yang menyediakan struktur data dan analisis data yang mudah digunakan dan berkinerja tinggi untuk bahasa pemrograman Python. instal pandas: pip install pandas Matplotlib Matplotlib adalah library paling banyak digunakan oleh data science untuk menyajikan datanya ke dalam visual yang lebih baik. instal matplotlib: pip install matplotlib Scikit Learn Machine learning ada yang berbasis statistika ada juga yang tidak. Salah satunya adalah support vector machine dan regresi linier. Mungkin bagi sebagian orang sudah biasa menulis sendiri library untuk implementasi kedua algoritma tadi. Tapi untuk membuatnya dalam waktu singkat tentu butuh waktu yang tidak sedikit pula. Scikit-Learn memberikan sejumlah fitur untuk keperluan data science seperti: Algoritma Regresi Algoritma Naive Bayes Algoritma Clustering Algoritma Decision Tree Parameter Tuning Data Preprocessing Tool Export / Import Model Machine learning pipeline dan lainnya instal Scikit Learn : pip install scikit-learn Implementasi K-Means Clustering \u00b6 Import Library yang dibutuhkan \u00b6 Mengimportkan library untuk mendukung implementasi k means clustering import pandas as pd import numpy as np import matplotlib.pyplot as plt from sklearn.preprocessing import StandardScaler from sklearn.cluster import KMeans Memuat Dataset \u00b6 Mengimport dataset yang digunakan untuk pengimplemtasian k means clustering dataset bisa didownload disini dataset=pd.read_csv(\"../data.csv\",encoding = \"ISO-8859-1\") dataset.shape Data Preprocessing \u00b6 Menghilangkan duplikat entri pada dataset. print (dataset.duplicated().sum()) dataset.drop_duplicates(inplace = True) dataset.shape Menghilangkan missing value di kolom customer id. dataset.dropna(axis = 0, subset =['CustomerID'], inplace = True) dataset.shape mengecek data apakah ada yg null. jika tidak ada yang null bisa di comment (#) print (pd.DataFrame(dataset.isnull().sum())) menghilangkan order yang dicancel. dataset = dataset[(dataset.InvoiceNo).apply(lambda x:( 'C' not in x))] dataset.shape # print(dataset.shape) df_customerid_groups=dataset.groupby(\"CustomerID\") Membuat dataframe baru dan memilihnya \u00b6 Membuat dataframe baru 'Quantity', 'UnitPrice', 'CustomerID'. df_cluster=pd.DataFrame(columns=['Quantity','UnitPrice','CustomerID']) count=0 for k,v in (df_customerid_groups): df_cluster.loc[count] = [(v['Quantity'].sum()), v['UnitPrice'].sum(), k] count+=1 df_cluster.shape disini kita hanya memakai kolom 'Quantity', 'UnitPrice' untuk di kelompokan X = df_cluster.iloc[:, [0, 1]].values #mengambil data Menggunakan Elbow method untuk mencari jumlah cluster \u00b6 Menggunakan elbow method untuk mencari jumlah cluster yang ideal kemudian diplot for i in range(1,11): # kmeans = KMeans(n_clusters = i, init ='k-means++',max_iter=300,n_init=10) kmeans.fit(X) wcss.append(kmeans.inertia_) plt.plot(range(1,11) , wcss) plt.title('The Elbow Method') plt.xlabel('Jumlah Kelompok Customer (kelompok jenis customer)') plt.ylabel('With in cluster sum of squers(WCSS)') plt.show() Keterangan init Metode inisialisasi, default ke \u2018k-means ++\u2019: n_cluster Jumlah cluster serta jumlah centroid yang dihasilkan k-mean++ metode inisialisasi acak untuk centroid max_iter Jumlah maksimum iterasi dari algoritma k-means untuk sekali jalan n_init Jumlah waktu algoritma k-means akan dijalankan dengan centroid yang berbeda. Hasil ploting elbow method: Menghitung K-Mean \u00b6 Mefitting k-mean ke dataset kmeans = KMeans(n_clusters = int(input(\"Masukan Jumlah Clusters:\")), init = 'k-means++') y_kmeans = kmeans.fit_predict(X) mengvisualisasikan cluster plt.scatter(X[y_kmeans == 0, 0], X[y_kmeans == 0, 1], s = 50, c = 'red', label = 'Customer Type 1') plt.scatter(X[y_kmeans == 1, 0], X[y_kmeans == 1, 1], s = 50, c = 'blue', label = 'Customer Type 2') plt.scatter(X[y_kmeans == 2, 0], X[y_kmeans == 2, 1], s = 50, c = 'green', label = 'Customer Type 3') plt.scatter(kmeans.cluster_centers_[:, 0], kmeans.cluster_centers_[:, 1], s = 100,marker='x', c = 'black', label = 'Centroids') plt.title('Jumlah Kelompok Customer (kelompok jenis customer') plt.xlabel('Jumlah barang yang Dibeli(Quantity)') plt.ylabel('Harga produk per unit dalam sterling(Unit Price)') plt.legend() plt.show() Masukan jumlah cluster disini saya isikan 3 Hasilnya : Dataset dan Program bisa didownload dan dilihat disini Referensi \u00b6 informatika. 2017. Algoritma K-Means Clustering di https://informatikalogi.com/algorithm/k-means/ Naftali Harris. 2017. Visualizing K-Means Clustering di https://www.naftaliharris.com/blog/visualizing-k-means-clustering/ Mubaris NK. 2017. K-Means Clustering in Python di https://mubaris.com/posts/kmeans-clustering/","title":"Kmean1"},{"location":"kmean1/#pengertian-k-means-clustering","text":"Clustering merupakan suatu metode untuk mencari dan mengelompokkan data yang memiliki kemiripan karakteristik (similarity) antara satu data dengan data yang lain. Clustering merupakan salah satu metode data mining yang bersifat tanpa arahan (unsupervised). Yang dimaksud metode unsupervised yaitu metode ini diterapkan tanpa adanya latihan (training) dan guru (teacher) serta tidak memerlukan target output. Ada dua jenis data clustering yang sering dipergunakan dalam proses pengelompokan data yaitu hierarchical (hirarki) data clustering dan non-hierarchical (non hirarki) data clustering. K-Means merupakan salah satu metode data clustering non hirarki yang berusaha mempartisi data yang ada ke dalam bentuk satu atau lebih cluster/kelompok. Metode ini mempartisi data ke dalam cluster/kelompok sehingga data yang memiliki karakteristik yang sama dikelompokkan ke dalam satu cluster yang sama dan data yang mempunyai karakteristik yang berbeda dikelompokkan ke dalam kelompok yang lain. Adapun tujuan dari data clustering ini adalah untuk meminimalisasikan objective function yang diset dalam proses clustering, yang pada umumnya berusaha meminimalisasikan variasi di dalam suatu cluster dan memaksimalisasikan variasi antar cluster.","title":"Pengertian K-Means Clustering"},{"location":"kmean1/#kelebihan-dan-kekurangan-k-means-clustering","text":"","title":"Kelebihan dan Kekurangan K-Means Clustering"},{"location":"kmean1/#kelebihan-k-means-clustering","text":"Menggunakan prinsip yang sederhana, dapat dijelaskan dalam non-statistik Mudah untuk diimplementasikan dan dijalankan. Waktu yang dibutuhkan untuk menjalankan nya relatif cepat Sangat fleksibel, dapat dengan mudah diadaptasi. Sangat umum digunakan","title":"Kelebihan K-Means Clustering"},{"location":"kmean1/#kekurangan-k-means-clustering","text":"dalam non-statistik Karena menggunakan k buah acak, tidak di jamin untuk menemukan kumpulan cluster yang optimal dapat terjadinya curse of dimensionality, apabila jarak antara cluster yang satu dengan yang lain memiliki banyak dimesi. Tidak optimal digunakan untuk data yang jumlahnya terlalu banyak sampai bermiliyar.","title":"Kekurangan K-Means Clustering"},{"location":"kmean1/#algoritma-k-means-clustering","text":"","title":"Algoritma K-Means Clustering"},{"location":"kmean1/#langkah-langkah-algortima-k-means-clustering","text":"Tentukan jumlah cluster(kelompok) yang kita inginkan. Inisiasi centroid untuk setiap cluster secara acak. Centroid adalah data yang merepresentasikan suatu kelompok. Hitung kedekatan suatu data terhadap centroid, kemudian masukkan data tersebut ke cluster yang centroid-nya memiliki sifat terdekat dengan dirinya(yang nilainya paling kecil). Pilih kembali centroid untuk masing-masing cluster ,yaitu mean (rata-rata) nilai data dari setiap cluster yang sama . Ulangi langkah ke-3 sampai tidak ada perubahan anggota untuk semua cluster, atau sampai batas yang ditentukan dari perulangan anggota yang sama.","title":"Langkah-langkah algortima K-Means Clustering :"},{"location":"kmean1/#contoh-perhitungan-k-means-sederhana","text":"Perhatikan gambar diatas, ada tabel yang mempunyai column X1 dan X2, dimana tabel tersebut akan kita pakai untuk mempelajari k-means clustering. Pertama-tama kita akan menentukan jumlah cluster (kelompok) yakni kita tentukan k = 2. tahap selanjutnya adalah inisiasi centroid karena k = 2 , maka kita akan memilih 2 observasi secara acak dari tabel, yakni A dan C. setelah itu kita akan menghitung jarak antar data dengan centroid yang telah ditentukan, dengan menggunakan formula euclidean distance. $$ \\begin{aligned} d(p, q)=d(q, p) &=\\sqrt{\\left(q_{1}-p_{1}\\right) {2}+\\left(q_{2}-p_{2}\\right) {2}+\\cdots+\\left(q_{n}-p_{n}\\right)^{2}} \\ &=\\sqrt{\\sum_{i=1} {n}\\left(q_{1}-p_{i}\\right) {2}} \\end{aligned} $$ Maka pertama-tama kita akan menghitung observasi A dengan centroid 1 : =\u221a(1-1)^2 + (1-1)^2 =\u221a(0)^2 + (0)^2 =\u221a0 =0 Setelah itu menghitung observasi A dengan centroid 2: =\u221a(1-0)^2 + (1-2)^2 =\u221a(1)^2 + (1)^2 =\u221a2 =1,4 Lakukan perhitungan sampai semua observasi dihitung. Hitung kedekatan suatu data terhadap centroid, kemudian masukkan data tersebut ke cluster yang centroid-nya memiliki sifat terdekat dengan dirinya (yang nilainya paling kecil). Pada observasi A yang paling kecil nilainya adalah 0 maka masuk ke dalam cluster 1. Pilih kembali centroid untuk masing-masing cluster ,yaitu mean (rata-rata) nilai data dari setiap cluster yang sama . observasi A dan B dalam cluster yang sama yakni cluster 1, maka dihitung untuk centroid 1 = (1+1 / 2 , 1+ 0 /2 ) -> (1,0.5). dan centroid 2 = (1.7,3.7). Lakukan penghitungan lagi dengan centroid yang baru. Pilih kembali centroid untuk masing-masing cluster ,yaitu mean (rata-rata) nilai data dari setiap cluster yang sama. Ulangi sampai tidak ada perubahan anggota untuk semua cluster, atau sampai batas yang ditentukan dari perulangan anggota yang sama.","title":"Contoh Perhitungan K-Means Sederhana"},{"location":"kmean1/#software-requirements","text":"Python 3.0 atau versi yang lebih baru, disini saya menggunakan python 3.7 IDE Pycharm","title":"Software Requirements"},{"location":"kmean1/#library-python-yang-digunakan","text":"Numpy Numpy merupakan sebuah library pada Python yang berfungsi untuk melakukan operasi vektor dan matriks dengan mengolah array dan array multidimensi. Biasanya NumPy digunakan untuk kebutuhan dalam menganalisis data. instal numpy: pip install numpy Pandas pandas adalah sebuah librari berlisensi BSD dan open source yang menyediakan struktur data dan analisis data yang mudah digunakan dan berkinerja tinggi untuk bahasa pemrograman Python. instal pandas: pip install pandas Matplotlib Matplotlib adalah library paling banyak digunakan oleh data science untuk menyajikan datanya ke dalam visual yang lebih baik. instal matplotlib: pip install matplotlib Scikit Learn Machine learning ada yang berbasis statistika ada juga yang tidak. Salah satunya adalah support vector machine dan regresi linier. Mungkin bagi sebagian orang sudah biasa menulis sendiri library untuk implementasi kedua algoritma tadi. Tapi untuk membuatnya dalam waktu singkat tentu butuh waktu yang tidak sedikit pula. Scikit-Learn memberikan sejumlah fitur untuk keperluan data science seperti: Algoritma Regresi Algoritma Naive Bayes Algoritma Clustering Algoritma Decision Tree Parameter Tuning Data Preprocessing Tool Export / Import Model Machine learning pipeline dan lainnya instal Scikit Learn : pip install scikit-learn","title":"Library Python yang digunakan:"},{"location":"kmean1/#implementasi-k-means-clustering","text":"","title":"Implementasi K-Means Clustering"},{"location":"kmean1/#import-library-yang-dibutuhkan","text":"Mengimportkan library untuk mendukung implementasi k means clustering import pandas as pd import numpy as np import matplotlib.pyplot as plt from sklearn.preprocessing import StandardScaler from sklearn.cluster import KMeans","title":"Import Library yang dibutuhkan"},{"location":"kmean1/#memuat-dataset","text":"Mengimport dataset yang digunakan untuk pengimplemtasian k means clustering dataset bisa didownload disini dataset=pd.read_csv(\"../data.csv\",encoding = \"ISO-8859-1\") dataset.shape","title":"Memuat Dataset"},{"location":"kmean1/#data-preprocessing","text":"Menghilangkan duplikat entri pada dataset. print (dataset.duplicated().sum()) dataset.drop_duplicates(inplace = True) dataset.shape Menghilangkan missing value di kolom customer id. dataset.dropna(axis = 0, subset =['CustomerID'], inplace = True) dataset.shape mengecek data apakah ada yg null. jika tidak ada yang null bisa di comment (#) print (pd.DataFrame(dataset.isnull().sum())) menghilangkan order yang dicancel. dataset = dataset[(dataset.InvoiceNo).apply(lambda x:( 'C' not in x))] dataset.shape # print(dataset.shape) df_customerid_groups=dataset.groupby(\"CustomerID\")","title":"Data Preprocessing"},{"location":"kmean1/#membuat-dataframe-baru-dan-memilihnya","text":"Membuat dataframe baru 'Quantity', 'UnitPrice', 'CustomerID'. df_cluster=pd.DataFrame(columns=['Quantity','UnitPrice','CustomerID']) count=0 for k,v in (df_customerid_groups): df_cluster.loc[count] = [(v['Quantity'].sum()), v['UnitPrice'].sum(), k] count+=1 df_cluster.shape disini kita hanya memakai kolom 'Quantity', 'UnitPrice' untuk di kelompokan X = df_cluster.iloc[:, [0, 1]].values #mengambil data","title":"Membuat dataframe baru dan memilihnya"},{"location":"kmean1/#menggunakan-elbow-method-untuk-mencari-jumlah-cluster","text":"Menggunakan elbow method untuk mencari jumlah cluster yang ideal kemudian diplot for i in range(1,11): # kmeans = KMeans(n_clusters = i, init ='k-means++',max_iter=300,n_init=10) kmeans.fit(X) wcss.append(kmeans.inertia_) plt.plot(range(1,11) , wcss) plt.title('The Elbow Method') plt.xlabel('Jumlah Kelompok Customer (kelompok jenis customer)') plt.ylabel('With in cluster sum of squers(WCSS)') plt.show() Keterangan init Metode inisialisasi, default ke \u2018k-means ++\u2019: n_cluster Jumlah cluster serta jumlah centroid yang dihasilkan k-mean++ metode inisialisasi acak untuk centroid max_iter Jumlah maksimum iterasi dari algoritma k-means untuk sekali jalan n_init Jumlah waktu algoritma k-means akan dijalankan dengan centroid yang berbeda. Hasil ploting elbow method:","title":"Menggunakan Elbow method untuk mencari jumlah cluster"},{"location":"kmean1/#menghitung-k-mean","text":"Mefitting k-mean ke dataset kmeans = KMeans(n_clusters = int(input(\"Masukan Jumlah Clusters:\")), init = 'k-means++') y_kmeans = kmeans.fit_predict(X) mengvisualisasikan cluster plt.scatter(X[y_kmeans == 0, 0], X[y_kmeans == 0, 1], s = 50, c = 'red', label = 'Customer Type 1') plt.scatter(X[y_kmeans == 1, 0], X[y_kmeans == 1, 1], s = 50, c = 'blue', label = 'Customer Type 2') plt.scatter(X[y_kmeans == 2, 0], X[y_kmeans == 2, 1], s = 50, c = 'green', label = 'Customer Type 3') plt.scatter(kmeans.cluster_centers_[:, 0], kmeans.cluster_centers_[:, 1], s = 100,marker='x', c = 'black', label = 'Centroids') plt.title('Jumlah Kelompok Customer (kelompok jenis customer') plt.xlabel('Jumlah barang yang Dibeli(Quantity)') plt.ylabel('Harga produk per unit dalam sterling(Unit Price)') plt.legend() plt.show() Masukan jumlah cluster disini saya isikan 3 Hasilnya : Dataset dan Program bisa didownload dan dilihat disini","title":"Menghitung K-Mean"},{"location":"kmean1/#referensi","text":"informatika. 2017. Algoritma K-Means Clustering di https://informatikalogi.com/algorithm/k-means/ Naftali Harris. 2017. Visualizing K-Means Clustering di https://www.naftaliharris.com/blog/visualizing-k-means-clustering/ Mubaris NK. 2017. K-Means Clustering in Python di https://mubaris.com/posts/kmeans-clustering/","title":"Referensi"},{"location":"kmean2/","text":"Pengertian K-Means Clustering \u00b6 Clustering merupakan suatu metode untuk mencari dan mengelompokkan data yang memiliki kemiripan karakteristik (similarity) antara satu data dengan data yang lain. Clustering merupakan salah satu metode data mining yang bersifat tanpa arahan (unsupervised). Yang dimaksud metode unsupervised yaitu metode ini diterapkan tanpa adanya latihan (training) dan guru (teacher) serta tidak memerlukan target output. Ada dua jenis data clustering yang sering dipergunakan dalam proses pengelompokan data yaitu hierarchical (hirarki) data clustering dan non-hierarchical (non hirarki) data clustering. K-Means merupakan salah satu metode data clustering non hirarki yang berusaha mempartisi data yang ada ke dalam bentuk satu atau lebih cluster/kelompok. Metode ini mempartisi data ke dalam cluster/kelompok sehingga data yang memiliki karakteristik yang sama dikelompokkan ke dalam satu cluster yang sama dan data yang mempunyai karakteristik yang berbeda dikelompokkan ke dalam kelompok yang lain. Adapun tujuan dari data clustering ini adalah untuk meminimalisasikan objective function yang diset dalam proses clustering, yang pada umumnya berusaha meminimalisasikan variasi di dalam suatu cluster dan memaksimalisasikan variasi antar cluster. Kelebihan dan Kekurangan K-Means Clustering \u00b6 Kelebihan K-Means Clustering \u00b6 Menggunakan prinsip yang sederhana, dapat dijelaskan dalam non-statistik Mudah untuk diimplementasikan dan dijalankan. Waktu yang dibutuhkan untuk menjalankan nya relatif cepat Sangat fleksibel, dapat dengan mudah diadaptasi. Sangat umum digunakan Kekurangan K-Means Clustering \u00b6 dalam non-statistik Karena menggunakan k buah acak, tidak di jamin untuk menemukan kumpulan cluster yang optimal dapat terjadinya curse of dimensionality, apabila jarak antara cluster yang satu dengan yang lain memiliki banyak dimesi. Tidak optimal digunakan untuk data yang jumlahnya terlalu banyak sampai bermiliyar. Algoritma K-Means Clustering \u00b6 Langkah-langkah algortima K-Means Clustering : \u00b6 Tentukan jumlah cluster(kelompok) yang kita inginkan. Inisiasi centroid untuk setiap cluster secara acak. Centroid adalah data yang merepresentasikan suatu kelompok. Hitung kedekatan suatu data terhadap centroid, kemudian masukkan data tersebut ke cluster yang centroid-nya memiliki sifat terdekat dengan dirinya(yang nilainya paling kecil). Pilih kembali centroid untuk masing-masing cluster ,yaitu mean (rata-rata) nilai data dari setiap cluster yang sama . Ulangi langkah ke-3 sampai tidak ada perubahan anggota untuk semua cluster, atau sampai batas yang ditentukan dari perulangan anggota yang sama. Contoh Perhitungan K-Means Sederhana \u00b6 Perhatikan gambar diatas, ada tabel yang mempunyai column X1 dan X2, dimana tabel tersebut akan kita pakai untuk mempelajari k-means clustering. Pertama-tama kita akan menentukan jumlah cluster (kelompok) yakni kita tentukan k = 2. tahap selanjutnya adalah inisiasi centroid karena k = 2 , maka kita akan memilih 2 observasi secara acak dari tabel, yakni A dan C. setelah itu kita akan menghitung jarak antar data dengan centroid yang telah ditentukan, dengan menggunakan formula euclidean distance. $$ \\begin{aligned} d(p, q)=d(q, p) &=\\sqrt{\\left(q_{1}-p_{1}\\right) {2}+\\left(q_{2}-p_{2}\\right) {2}+\\cdots+\\left(q_{n}-p_{n}\\right)^{2}} \\ &=\\sqrt{\\sum_{i=1} {n}\\left(q_{1}-p_{i}\\right) {2}} \\end{aligned} $$ Maka pertama-tama kita akan menghitung observasi A dengan centroid 1 : =\u221a(1-1)^2 + (1-1)^2 =\u221a(0)^2 + (0)^2 =\u221a0 =0 Setelah itu menghitung observasi A dengan centroid 2: =\u221a(1-0)^2 + (1-2)^2 =\u221a(1)^2 + (1)^2 =\u221a2 =1,4 Lakukan perhitungan sampai semua observasi dihitung. Hitung kedekatan suatu data terhadap centroid, kemudian masukkan data tersebut ke cluster yang centroid-nya memiliki sifat terdekat dengan dirinya (yang nilainya paling kecil). Pada observasi A yang paling kecil nilainya adalah 0 maka masuk ke dalam cluster 1. Pilih kembali centroid untuk masing-masing cluster ,yaitu mean (rata-rata) nilai data dari setiap cluster yang sama . observasi A dan B dalam cluster yang sama yakni cluster 1, maka dihitung untuk centroid 1 = (1+1 / 2 , 1+ 0 /2 ) -> (1,0.5). dan centroid 2 = (1.7,3.7). Lakukan penghitungan lagi dengan centroid yang baru. Pilih kembali centroid untuk masing-masing cluster ,yaitu mean (rata-rata) nilai data dari setiap cluster yang sama. Ulangi sampai tidak ada perubahan anggota untuk semua cluster, atau sampai batas yang ditentukan dari perulangan anggota yang sama. Software Requirements \u00b6 Python 3.0 atau versi yang lebih baru, disini saya menggunakan python 3.7 IDE Pycharm Library Python yang digunakan: \u00b6 Numpy Numpy merupakan sebuah library pada Python yang berfungsi untuk melakukan operasi vektor dan matriks dengan mengolah array dan array multidimensi. Biasanya NumPy digunakan untuk kebutuhan dalam menganalisis data. instal numpy: pip install numpy Pandas pandas adalah sebuah librari berlisensi BSD dan open source yang menyediakan struktur data dan analisis data yang mudah digunakan dan berkinerja tinggi untuk bahasa pemrograman Python. instal pandas: pip install pandas Matplotlib Matplotlib adalah library paling banyak digunakan oleh data science untuk menyajikan datanya ke dalam visual yang lebih baik. instal matplotlib: pip install matplotlib Scikit Learn Machine learning ada yang berbasis statistika ada juga yang tidak. Salah satunya adalah support vector machine dan regresi linier. Mungkin bagi sebagian orang sudah biasa menulis sendiri library untuk implementasi kedua algoritma tadi. Tapi untuk membuatnya dalam waktu singkat tentu butuh waktu yang tidak sedikit pula. Scikit-Learn memberikan sejumlah fitur untuk keperluan data science seperti: Algoritma Regresi Algoritma Naive Bayes Algoritma Clustering Algoritma Decision Tree Parameter Tuning Data Preprocessing Tool Export / Import Model Machine learning pipeline dan lainnya instal Scikit Learn : pip install scikit-learn # Implementasi K-Means Clustering E-Commerce Data Actual transactions from UK retailer Dataset ## Import Library yang dibutuhkan Mengimportkan library untuk mendukung implementasi k means clustering import pandas as pd import numpy as np import matplotlib.pyplot as plt from sklearn.preprocessing import StandardScaler from sklearn.cluster import KMeans ## Memuat Dataset Mengimport dataset yang digunakan untuk pengimplemtasian k means clustering dataset bisa didownload disini dataset=pd.read_csv(\"../data.csv\",encoding = \"ISO-8859-1\") dataset.shape ## Data Preprocessing Menghilangkan duplikat entri pada dataset. print (dataset.duplicated().sum()) dataset.drop_duplicates(inplace = True) dataset.shape Menghilangkan missing value di kolom customer id. dataset.dropna(axis = 0, subset =['CustomerID'], inplace = True) dataset.shape mengecek data apakah ada yg null. jika tidak ada yang null bisa di comment (#) print (pd.DataFrame(dataset.isnull().sum())) menghilangkan order yang dicancel. dataset = dataset[(dataset.InvoiceNo).apply(lambda x:( 'C' not in x))] dataset.shape # print(dataset.shape) df_customerid_groups=dataset.groupby(\"CustomerID\") ## Membuat dataframe baru dan memilihnya Membuat dataframe baru 'Quantity', 'UnitPrice', 'CustomerID'. df_cluster=pd.DataFrame(columns=['Quantity','UnitPrice','CustomerID']) count=0 for k,v in (df_customerid_groups): df_cluster.loc[count] = [(v['Quantity'].sum()), v['UnitPrice'].sum(), k] count+=1 df_cluster.shape disini kita hanya memakai kolom 'Quantity', 'UnitPrice' untuk di kelompokan X = df_cluster.iloc[:, [0, 1]].values #mengambil data ## Menggunakan Elbow method untuk mencari jumlah cluster Menggunakan elbow method untuk mencari jumlah cluster yang ideal kemudian diplot for i in range(1,11): # kmeans = KMeans(n_clusters = i, init ='k-means++',max_iter=300,n_init=10) kmeans.fit(X) wcss.append(kmeans.inertia_) plt.plot(range(1,11) , wcss) plt.title('The Elbow Method') plt.xlabel('Jumlah Kelompok Customer (kelompok jenis customer)') plt.ylabel('With in cluster sum of squers(WCSS)') plt.show() Keterangan init Metode inisialisasi, default ke \u2018k-means ++\u2019: n_cluster Jumlah cluster serta jumlah centroid yang dihasilkan k-mean++ metode inisialisasi acak untuk centroid max_iter Jumlah maksimum iterasi dari algoritma k-means untuk sekali jalan n_init Jumlah waktu algoritma k-means akan dijalankan dengan centroid yang berbeda. Hasil ploting elbow method: ## Menghitung K-Mean Mefitting k-mean ke dataset kmeans = KMeans(n_clusters = int(input(\"Masukan Jumlah Clusters:\")), init = 'k-means++') y_kmeans = kmeans.fit_predict(X) mengvisualisasikan cluster plt.scatter(X[y_kmeans == 0, 0], X[y_kmeans == 0, 1], s = 50, c = 'red', label = 'Customer Type 1') plt.scatter(X[y_kmeans == 1, 0], X[y_kmeans == 1, 1], s = 50, c = 'blue', label = 'Customer Type 2') plt.scatter(X[y_kmeans == 2, 0], X[y_kmeans == 2, 1], s = 50, c = 'green', label = 'Customer Type 3') plt.scatter(kmeans.cluster_centers_[:, 0], kmeans.cluster_centers_[:, 1], s = 100,marker='x', c = 'black', label = 'Centroids') plt.title('Jumlah Kelompok Customer (kelompok jenis customer') plt.xlabel('Jumlah barang yang Dibeli(Quantity)') plt.ylabel('Harga produk per unit dalam sterling(Unit Price)') plt.legend() plt.show() Masukan jumlah cluster disini saya isikan 3 Hasilnya : Dataset dan Program bisa didownload dan dilihat disini Referensi \u00b6 informatika. 2017. Algoritma K-Means Clustering di https://informatikalogi.com/algorithm/k-means/ Naftali Harris. 2017. Visualizing K-Means Clustering di https://www.naftaliharris.com/blog/visualizing-k-means-clustering/ Mubaris NK. 2017. K-Means Clustering in Python di https://mubaris.com/posts/kmeans-clustering/","title":"Pengertian K-Means Clustering"},{"location":"kmean2/#pengertian-k-means-clustering","text":"Clustering merupakan suatu metode untuk mencari dan mengelompokkan data yang memiliki kemiripan karakteristik (similarity) antara satu data dengan data yang lain. Clustering merupakan salah satu metode data mining yang bersifat tanpa arahan (unsupervised). Yang dimaksud metode unsupervised yaitu metode ini diterapkan tanpa adanya latihan (training) dan guru (teacher) serta tidak memerlukan target output. Ada dua jenis data clustering yang sering dipergunakan dalam proses pengelompokan data yaitu hierarchical (hirarki) data clustering dan non-hierarchical (non hirarki) data clustering. K-Means merupakan salah satu metode data clustering non hirarki yang berusaha mempartisi data yang ada ke dalam bentuk satu atau lebih cluster/kelompok. Metode ini mempartisi data ke dalam cluster/kelompok sehingga data yang memiliki karakteristik yang sama dikelompokkan ke dalam satu cluster yang sama dan data yang mempunyai karakteristik yang berbeda dikelompokkan ke dalam kelompok yang lain. Adapun tujuan dari data clustering ini adalah untuk meminimalisasikan objective function yang diset dalam proses clustering, yang pada umumnya berusaha meminimalisasikan variasi di dalam suatu cluster dan memaksimalisasikan variasi antar cluster.","title":"Pengertian K-Means Clustering"},{"location":"kmean2/#kelebihan-dan-kekurangan-k-means-clustering","text":"","title":"Kelebihan dan Kekurangan K-Means Clustering"},{"location":"kmean2/#kelebihan-k-means-clustering","text":"Menggunakan prinsip yang sederhana, dapat dijelaskan dalam non-statistik Mudah untuk diimplementasikan dan dijalankan. Waktu yang dibutuhkan untuk menjalankan nya relatif cepat Sangat fleksibel, dapat dengan mudah diadaptasi. Sangat umum digunakan","title":"Kelebihan K-Means Clustering"},{"location":"kmean2/#kekurangan-k-means-clustering","text":"dalam non-statistik Karena menggunakan k buah acak, tidak di jamin untuk menemukan kumpulan cluster yang optimal dapat terjadinya curse of dimensionality, apabila jarak antara cluster yang satu dengan yang lain memiliki banyak dimesi. Tidak optimal digunakan untuk data yang jumlahnya terlalu banyak sampai bermiliyar.","title":"Kekurangan K-Means Clustering"},{"location":"kmean2/#algoritma-k-means-clustering","text":"","title":"Algoritma K-Means Clustering"},{"location":"kmean2/#langkah-langkah-algortima-k-means-clustering","text":"Tentukan jumlah cluster(kelompok) yang kita inginkan. Inisiasi centroid untuk setiap cluster secara acak. Centroid adalah data yang merepresentasikan suatu kelompok. Hitung kedekatan suatu data terhadap centroid, kemudian masukkan data tersebut ke cluster yang centroid-nya memiliki sifat terdekat dengan dirinya(yang nilainya paling kecil). Pilih kembali centroid untuk masing-masing cluster ,yaitu mean (rata-rata) nilai data dari setiap cluster yang sama . Ulangi langkah ke-3 sampai tidak ada perubahan anggota untuk semua cluster, atau sampai batas yang ditentukan dari perulangan anggota yang sama.","title":"Langkah-langkah algortima K-Means Clustering :"},{"location":"kmean2/#contoh-perhitungan-k-means-sederhana","text":"Perhatikan gambar diatas, ada tabel yang mempunyai column X1 dan X2, dimana tabel tersebut akan kita pakai untuk mempelajari k-means clustering. Pertama-tama kita akan menentukan jumlah cluster (kelompok) yakni kita tentukan k = 2. tahap selanjutnya adalah inisiasi centroid karena k = 2 , maka kita akan memilih 2 observasi secara acak dari tabel, yakni A dan C. setelah itu kita akan menghitung jarak antar data dengan centroid yang telah ditentukan, dengan menggunakan formula euclidean distance. $$ \\begin{aligned} d(p, q)=d(q, p) &=\\sqrt{\\left(q_{1}-p_{1}\\right) {2}+\\left(q_{2}-p_{2}\\right) {2}+\\cdots+\\left(q_{n}-p_{n}\\right)^{2}} \\ &=\\sqrt{\\sum_{i=1} {n}\\left(q_{1}-p_{i}\\right) {2}} \\end{aligned} $$ Maka pertama-tama kita akan menghitung observasi A dengan centroid 1 : =\u221a(1-1)^2 + (1-1)^2 =\u221a(0)^2 + (0)^2 =\u221a0 =0 Setelah itu menghitung observasi A dengan centroid 2: =\u221a(1-0)^2 + (1-2)^2 =\u221a(1)^2 + (1)^2 =\u221a2 =1,4 Lakukan perhitungan sampai semua observasi dihitung. Hitung kedekatan suatu data terhadap centroid, kemudian masukkan data tersebut ke cluster yang centroid-nya memiliki sifat terdekat dengan dirinya (yang nilainya paling kecil). Pada observasi A yang paling kecil nilainya adalah 0 maka masuk ke dalam cluster 1. Pilih kembali centroid untuk masing-masing cluster ,yaitu mean (rata-rata) nilai data dari setiap cluster yang sama . observasi A dan B dalam cluster yang sama yakni cluster 1, maka dihitung untuk centroid 1 = (1+1 / 2 , 1+ 0 /2 ) -> (1,0.5). dan centroid 2 = (1.7,3.7). Lakukan penghitungan lagi dengan centroid yang baru. Pilih kembali centroid untuk masing-masing cluster ,yaitu mean (rata-rata) nilai data dari setiap cluster yang sama. Ulangi sampai tidak ada perubahan anggota untuk semua cluster, atau sampai batas yang ditentukan dari perulangan anggota yang sama.","title":"Contoh Perhitungan K-Means Sederhana"},{"location":"kmean2/#software-requirements","text":"Python 3.0 atau versi yang lebih baru, disini saya menggunakan python 3.7 IDE Pycharm","title":"Software Requirements"},{"location":"kmean2/#library-python-yang-digunakan","text":"Numpy Numpy merupakan sebuah library pada Python yang berfungsi untuk melakukan operasi vektor dan matriks dengan mengolah array dan array multidimensi. Biasanya NumPy digunakan untuk kebutuhan dalam menganalisis data. instal numpy: pip install numpy Pandas pandas adalah sebuah librari berlisensi BSD dan open source yang menyediakan struktur data dan analisis data yang mudah digunakan dan berkinerja tinggi untuk bahasa pemrograman Python. instal pandas: pip install pandas Matplotlib Matplotlib adalah library paling banyak digunakan oleh data science untuk menyajikan datanya ke dalam visual yang lebih baik. instal matplotlib: pip install matplotlib Scikit Learn Machine learning ada yang berbasis statistika ada juga yang tidak. Salah satunya adalah support vector machine dan regresi linier. Mungkin bagi sebagian orang sudah biasa menulis sendiri library untuk implementasi kedua algoritma tadi. Tapi untuk membuatnya dalam waktu singkat tentu butuh waktu yang tidak sedikit pula. Scikit-Learn memberikan sejumlah fitur untuk keperluan data science seperti: Algoritma Regresi Algoritma Naive Bayes Algoritma Clustering Algoritma Decision Tree Parameter Tuning Data Preprocessing Tool Export / Import Model Machine learning pipeline dan lainnya instal Scikit Learn : pip install scikit-learn # Implementasi K-Means Clustering E-Commerce Data Actual transactions from UK retailer Dataset ## Import Library yang dibutuhkan Mengimportkan library untuk mendukung implementasi k means clustering import pandas as pd import numpy as np import matplotlib.pyplot as plt from sklearn.preprocessing import StandardScaler from sklearn.cluster import KMeans ## Memuat Dataset Mengimport dataset yang digunakan untuk pengimplemtasian k means clustering dataset bisa didownload disini dataset=pd.read_csv(\"../data.csv\",encoding = \"ISO-8859-1\") dataset.shape ## Data Preprocessing Menghilangkan duplikat entri pada dataset. print (dataset.duplicated().sum()) dataset.drop_duplicates(inplace = True) dataset.shape Menghilangkan missing value di kolom customer id. dataset.dropna(axis = 0, subset =['CustomerID'], inplace = True) dataset.shape mengecek data apakah ada yg null. jika tidak ada yang null bisa di comment (#) print (pd.DataFrame(dataset.isnull().sum())) menghilangkan order yang dicancel. dataset = dataset[(dataset.InvoiceNo).apply(lambda x:( 'C' not in x))] dataset.shape # print(dataset.shape) df_customerid_groups=dataset.groupby(\"CustomerID\") ## Membuat dataframe baru dan memilihnya Membuat dataframe baru 'Quantity', 'UnitPrice', 'CustomerID'. df_cluster=pd.DataFrame(columns=['Quantity','UnitPrice','CustomerID']) count=0 for k,v in (df_customerid_groups): df_cluster.loc[count] = [(v['Quantity'].sum()), v['UnitPrice'].sum(), k] count+=1 df_cluster.shape disini kita hanya memakai kolom 'Quantity', 'UnitPrice' untuk di kelompokan X = df_cluster.iloc[:, [0, 1]].values #mengambil data ## Menggunakan Elbow method untuk mencari jumlah cluster Menggunakan elbow method untuk mencari jumlah cluster yang ideal kemudian diplot for i in range(1,11): # kmeans = KMeans(n_clusters = i, init ='k-means++',max_iter=300,n_init=10) kmeans.fit(X) wcss.append(kmeans.inertia_) plt.plot(range(1,11) , wcss) plt.title('The Elbow Method') plt.xlabel('Jumlah Kelompok Customer (kelompok jenis customer)') plt.ylabel('With in cluster sum of squers(WCSS)') plt.show() Keterangan init Metode inisialisasi, default ke \u2018k-means ++\u2019: n_cluster Jumlah cluster serta jumlah centroid yang dihasilkan k-mean++ metode inisialisasi acak untuk centroid max_iter Jumlah maksimum iterasi dari algoritma k-means untuk sekali jalan n_init Jumlah waktu algoritma k-means akan dijalankan dengan centroid yang berbeda. Hasil ploting elbow method: ## Menghitung K-Mean Mefitting k-mean ke dataset kmeans = KMeans(n_clusters = int(input(\"Masukan Jumlah Clusters:\")), init = 'k-means++') y_kmeans = kmeans.fit_predict(X) mengvisualisasikan cluster plt.scatter(X[y_kmeans == 0, 0], X[y_kmeans == 0, 1], s = 50, c = 'red', label = 'Customer Type 1') plt.scatter(X[y_kmeans == 1, 0], X[y_kmeans == 1, 1], s = 50, c = 'blue', label = 'Customer Type 2') plt.scatter(X[y_kmeans == 2, 0], X[y_kmeans == 2, 1], s = 50, c = 'green', label = 'Customer Type 3') plt.scatter(kmeans.cluster_centers_[:, 0], kmeans.cluster_centers_[:, 1], s = 100,marker='x', c = 'black', label = 'Centroids') plt.title('Jumlah Kelompok Customer (kelompok jenis customer') plt.xlabel('Jumlah barang yang Dibeli(Quantity)') plt.ylabel('Harga produk per unit dalam sterling(Unit Price)') plt.legend() plt.show() Masukan jumlah cluster disini saya isikan 3 Hasilnya : Dataset dan Program bisa didownload dan dilihat disini","title":"Library Python yang digunakan:"},{"location":"kmean2/#referensi","text":"informatika. 2017. Algoritma K-Means Clustering di https://informatikalogi.com/algorithm/k-means/ Naftali Harris. 2017. Visualizing K-Means Clustering di https://www.naftaliharris.com/blog/visualizing-k-means-clustering/ Mubaris NK. 2017. K-Means Clustering in Python di https://mubaris.com/posts/kmeans-clustering/","title":"Referensi"},{"location":"knn/","text":"Pengertian K-Nearest Neighbor \u00b6 Algoritma k-Nearest Neighbor adalah algoritma supervised learning dimana hasil dari instance yang baru diklasifikasikan berdasarkan mayoritas dari kategori k-tetangga terdekat. Tujuan dari algoritma ini adalah untuk mengklasifikasikan obyek baru berdasarkan atribut dan sample-sample dari training data. Algoritma k-Nearest Neighbor menggunakan Neighborhood Classification sebagai nilai prediksi dari nilai instance yang baru. Kelebihan dan Kekurangan K-Nearest Neighbor \u00b6 Kelebihan K-Nearest Neighbor \u00b6 Pelatihan sangat cepat Sederhana dan mudah dipelajari Tahan terhadap data pelatihan yang memiliki derau Efektif jika data pelatihan besar. Kekurangan K-Nearest Neighbor \u00b6 Nilai k bias Komputasi kompleks Keterbatasan memori, Mudah tertipu dengan atribut yang tidak relevan. Algoritma K-Nearest Neighbor \u00b6 Langkah-langkah K-Nearest Neighbor : \u00b6 Menentukan parameter K sebagai banyaknya jumlah tetangga terdekat dengan objek baru. Menghitung jarak antar objek/data baru terhadap semua objek/data yan gtelah di training. Urutkan hasil perhitungan tersebut. Tentukan tetangga terdekat berdasarkan jarak minimum ke K. Tentukan kategori dari tetangga terdekat dengan objek/data. Gunakan kategori mayoritas sebagai klasifikasi objek/data baru. Contoh Perhitungan KNN Sederhana \u00b6 Misalnya ada sebuah rumah yang berada tepat di tengah perbatasan antara Kota Bandung dan Kabupaten Bandung, sehingga pemerintah kesulitan untuk menentukan apakah rumah tersebut termasuk kedalam wilayah Kota Bandung atau Kabupaten Bandung . Kita bisa menentukannya dengan menggunakan Algoritma k-NN , yaitu dengan melibatkan jarak antara rumah tersebut dengan rumah-rumah yang ada disekitarnya (tetangganya). Pertama , kita harus menentukan jumlah tetangga yg akan kita perhitungkan (k), misalnya kita tentukan 3 tetangga terdekat ( k = 3 ). Kedua , hitung jarak setiap tetangga terhadap rumah tersebut, lalu urutkan hasilnya berdasarkan jarak, mulai dari yang terkecil ke yang terbesar. Ketiga , ambil 3 (k) tetangga yg paling dekat, lalu kita lihat masing-masing dari tetangga tersebut apakah termasuk kedalam wilayah Kota atau Kabupaten. Ada 2 kemungkinan: Bila dari 3 tetangga tersebut terdapat ada 2 rumah yg termasuk kedalam wilayah Kota Bandung, maka rumah tersebut termasuk kedalam wilayah Kota Bandung. Sebaliknya, bila dari 3 tetangga tersebut terdapat 2 rumah yg termasuk kedalam wilayah Kabupaten Bandung, maka rumah tersebut termasuk kedalam wilayah Kabupaten Bandung. Dalam menentukan nilai k* , bila* jumlah klasifikasi kita genap maka sebaiknya kita gunakan nilai k ganjil* , dan begitu pula sebaliknya bila* jumlah klasifikasi kita ganjil maka sebaiknya gunakan nilai k genap* , karena jika tidak begitu, ada kemungkinan kita* tidak akan mendapatkan jawaban* .* Pembahasan Lebih Detil \u00b6 Pada kasus diatas, kita menghitung jarak suatu rumah terhadap tetangga-tetangganya, itu berarti kita harus mengetahui posisi dari setiap rumah. Kita bisa menggunakan latitude dan longitude (atau garis lintang dan garis bujur) sebagai posisi. Untuk mempermudah pemahaman, saya akan coba menggunakan data yang nilainya sederhana. Data yang akan digunakan adalah sebagai berikut: Dari data diatas, kita mendapatkan beberapa informasi, diantaranya: Independent Variables , yaitu variable yang nilainya tidak dipengaruhi oleh variable lain. Pada contoh data diatas, yang termasuk independent variable adalah Lat , dan Long . Dependent Variables , yaitu variable yang nilainya dipengaruhi oleh variable lain. Pada contoh data diatas, yang termasuk *dependent variable*adalah Lokasi . Rumah A-E adalah rumah yang masuk ke dalam wilayah Kota . Rumah F-G adalah rumah yang masuk ke dalam wilayah Kabupaten . Rumah X adalah rumah yang akan kita prediksi menggunakan algoritma kNN apakah termasuk ke dalam wilayah Kota atau Kabupaten. Didalam dunia Machine Learning* ,* Independent Variables sering disebut juga sebagai Features Selanjutnya kita hitung jarak antara rumah X terhadap rumah A-G dengan menggunakan rumus Euclidean Distance : $$ \\begin{aligned} d(p, q)=d(q, p) &=\\sqrt{\\left(q_{1}-p_{1}\\right) {2}+\\left(q_{2}-p_{2}\\right) {2}+\\cdots+\\left(q_{n}-p_{n}\\right)^{2}} \\ &=\\sqrt{\\sum_{i=1} {n}\\left(q_{1}-p_{i}\\right) {2}} \\end{aligned} $$ Perhitungan jarak rumah X dengan rumah A: =\u221a(11-19)^2 + (26-25)^2 =\u221a(-8)^2 + (1)^2 =\u221a65 =8,062257748298550 setelah jarak rumah X dengan rumah A ditemukan tinggal menghitung jarak antara rumah X terhadap rumah B-G Setelah dihitung, selanjutnya adalah urutkan jarak tersebut dari yang paling kecil ke yang paling besar , hasilnya adalah sebagai berikut: Dapat dilihat dari hasil perhitungan diatas, bahwa ternyata 3 tetangga terdekat dari rumah X adalah: Rumah H (Kabupaten) yang memiliki jarak 2.24 , Rumah C (Kota) yang memiliki jarak 3 , dan Rumah E (Kota) yang memiliki jarak 3.16 . Dari ke-3 tetangga terdekat, terdapat 2 rumah yang termasuk kedalam wilayah Kota dan 1 rumah yang masuk ke dalam wilayah Kabupaten . Sehingga dapat disimpulkan, bahwa Rumah X adalah rumah yang termasuk kedalam wilayah Kota Implementasi K-Nearest Neighbor Breast Cancer Wisconsin (Diagnostic) Dataset \u00b6 Import Library yang dibutuhkan \u00b6 import numpy as np import numpy as np import pandas as pd import matplotlib.pyplot as plt from sklearn.neighbors import KNeighborsClassifier from sklearn.model_selection import train_test_split from sklearn import metrics #importing modul metrik Memuat Dataset \u00b6 Mengimport dataset yang digunakan untuk pengimplemtasian K-Nearest Neighbor dataset bisa didownload disini atau langsung dari kaggle disini # Memuat dataset data = pd.read_csv(\"E:\\Semester 4\\data mining/bc.csv\",encoding = \"ISO-8859-1\") Data Preprocessing \u00b6 Menampilkan 5 Data teratas \u00b6 Menampilkan jumlah masing-masing diagnosis dan kolom diagnosis digunakan sebagai class nantinya. data.head(5) Output: Menampilkan ringkasan dataset serta menghilangkan kolom yang tidak berguna \u00b6 # ringkasan dataset data.info() Output: <class 'pandas.core.frame.DataFrame'> RangeIndex: 569 entries, 0 to 568 Data columns (total 33 columns): id 569 non-null int64 diagnosis 569 non-null object radius_mean 569 non-null float64 texture_mean 569 non-null float64 perimeter_mean 569 non-null float64 area_mean 569 non-null float64 smoothness_mean 569 non-null float64 compactness_mean 569 non-null float64 concavity_mean 569 non-null float64 concave points_mean 569 non-null float64 symmetry_mean 569 non-null float64 fractal_dimension_mean 569 non-null float64 radius_se 569 non-null float64 texture_se 569 non-null float64 perimeter_se 569 non-null float64 area_se 569 non-null float64 smoothness_se 569 non-null float64 compactness_se 569 non-null float64 concavity_se 569 non-null float64 concave points_se 569 non-null float64 symmetry_se 569 non-null float64 fractal_dimension_se 569 non-null float64 radius_worst 569 non-null float64 texture_worst 569 non-null float64 perimeter_worst 569 non-null float64 area_worst 569 non-null float64 smoothness_worst 569 non-null float64 compactness_worst 569 non-null float64 concavity_worst 569 non-null float64 concave points_worst 569 non-null float64 symmetry_worst 569 non-null float64 fractal_dimension_worst 569 non-null float64 Unnamed: 32 0 non-null float64 dtypes: float64(31), int64(1), object(1) memory usage: 146.8+ KB bisa dilihat dari hasil output diatas ada kolom yang tidak bernama (\"Unnamed\") akan kita hilangkan, serta menghilangkan kolom yang tidak berguna yaitu kolom ()\"id\") : #menghapus kolom yang tidak berguna #menghapus kolom \"id\" data.drop(\"id\",axis=1,inplace=True) #menghapus the \"Unnamed: 32\" column data.drop(\"Unnamed: 32\",axis=1,inplace=True) #hasil data.info() Output: bisa dilihat kolom \"id\" dan kolom yang tidak mempunyai nama \"unnamed\" telah hilang <class 'pandas.core.frame.DataFrame'> RangeIndex: 569 entries, 0 to 568 Data columns (total 31 columns): diagnosis 569 non-null object radius_mean 569 non-null float64 texture_mean 569 non-null float64 perimeter_mean 569 non-null float64 area_mean 569 non-null float64 smoothness_mean 569 non-null float64 compactness_mean 569 non-null float64 concavity_mean 569 non-null float64 concave points_mean 569 non-null float64 symmetry_mean 569 non-null float64 fractal_dimension_mean 569 non-null float64 radius_se 569 non-null float64 texture_se 569 non-null float64 perimeter_se 569 non-null float64 area_se 569 non-null float64 smoothness_se 569 non-null float64 compactness_se 569 non-null float64 concavity_se 569 non-null float64 concave points_se 569 non-null float64 symmetry_se 569 non-null float64 fractal_dimension_se 569 non-null float64 radius_worst 569 non-null float64 texture_worst 569 non-null float64 perimeter_worst 569 non-null float64 area_worst 569 non-null float64 smoothness_worst 569 non-null float64 compactness_worst 569 non-null float64 concavity_worst 569 non-null float64 concave points_worst 569 non-null float64 symmetry_worst 569 non-null float64 fractal_dimension_worst 569 non-null float64 dtypes: float64(30), object(1) memory usage: 137.9+ KB Kemudian kita lihat lagi kolom beserta 5 data pertama: # 5 baris pertama data.head(5) Output: Mengganti class M dan B menjadi 0 dan 1 \u00b6 diagnosis adalah variabel yang bertanggung jawab untuk klasifikasi disini kita mengganti M dan B masing-masing dengan 1 dan 0 #diagnosis adalah variabel yang bertanggung jawab untuk klasifikasi #mengganti M dan B masing-masing dengan 1 dan 0 data.diagnosis=data.diagnosis.map({'M':1,'B':0}) Kemudian kita hitung berapa banyak jumlah masing- masing feature #menghitung variabel diagnosis data.diagnosis.value_counts() output: 0 357 1 212 Name: diagnosis, dtype: int64 Membagi data 30% sebagai data testing dan 70% sebagai data training \u00b6 # preprocessing dataset selesai #splitting dataset ke training dan testing train, test = train_test_split(data, test_size = 0.3,random_state=1234) #mencari hasil print(train.shape) print(test.shape) (398, 31) (171, 31) Membuat variabel independen dan responsible \u00b6 variabel independen dan responsible nantinya akan digunakan dalam proses prediksi variable independen mengambil dari semua kolom dan variable responsible dari diagnosis #membuat variabel independen untuk training train_X = train.iloc[:, 1:31] #membuat variabel responsible untuk training train_y=train.diagnosis #membuat variabel independen untuk testing test_X= test.iloc[:, 1:31] #membuat variabel responsible untuk ttesting test_y =test.diagnosis kita cek dulu berapa jumlahnya #mencari hasil print(train_X.shape) print(train_y.shape) print(test_X.shape) print(test_y.shape) output: (398, 30) (398,) (171, 30) (171,) Mencari K yang terbaik(ideal) \u00b6 K yang terbaik disini akan menghasilkan akurasi yang tinggi, Pada langkah ini saya memberikan range / jarak 1 sampai 30 untuk mencari K terbaik dan akurasi terbaik. Sehingga saya melakukan pengulangan 1 - 30 dan mencari nilai akurasi tertinggi. neighbors=np.arange(1,31) accuracy_train=[] accuracy_test=[] for i,k in enumerate(neighbors): knn=KNeighborsClassifier(n_neighbors=k) knn.fit(train_X,train_y) accuracy_train.append(knn.score(train_X,train_y)) accuracy_test.append(knn.score(test_X,test_y)) plt.figure(figsize=(13,8)) plt.plot(neighbors,accuracy_train,label=\"Akurasi training\") plt.plot(neighbors,accuracy_test,label=\"Akurasi testing\") acideal=np.max(accuracy_test) kideal=1+accuracy_test.index(np.max(accuracy_test)) plt.title('Tingkat Akurasi') plt.xlabel(\"K yang dipilih\") plt.ylabel(\"Tingkat Akurasi\") plt.xticks(neighbors) plt.legend() plt.show() print(\"Akurasi ideal:\",acideal) print(\"K ideal:\",kideal) Output: Akurasi ideal: 0.9415204678362573 K ideal: 7 Akurasi yang ideal 0.9415204678362573 pada k = 7 Menghitung KNN \u00b6 #membuat instance model = KNeighborsClassifier(n_neighbors=int(input(\"Masukan jumlah k:\"))) #learning model.fit(train_X,train_y) #Prediksi prediction=model.predict(test_X) #evaluation (Akurasi) print(\"Akurasi:\",metrics.accuracy_score(prediction,test_y)) #evaluation(Confusion Metrix) print(\"Confusion Metrix:\\n\",metrics.confusion_matrix(prediction,test_y)) Output: Masukan jumlah k:7 Akurasi: 0.9415204678362573 Confusion Metrix: [[102 7] [ 3 59]] Menampilkan Hasil Prediksi Datatesting \u00b6 Pada langkah ini kita menggabungkan data testing, diagnosis dan prediction . sehingga kita dapat melihat hasil prediksi dari semua data. datatest=pd.DataFrame(test_X) datatest['diagnosis']=test_y datatest['prediksi']=prediction print(datatest) Output: radius_mean texture_mean perimeter_mean area_mean smoothness_mean \\ 531 11.670 20.02 75.21 416.2 0.10160 166 10.800 9.71 68.77 357.6 0.09594 485 12.450 16.41 82.85 476.7 0.09514 66 9.465 21.01 60.11 269.4 0.10440 220 13.650 13.16 87.88 568.9 0.09646 356 13.050 18.59 85.09 512.0 0.10820 414 15.130 29.81 96.71 719.5 0.08320 525 8.571 13.10 54.53 221.3 0.10360 77 18.050 16.15 120.20 1006.0 0.10650 239 17.460 39.28 113.40 920.6 0.09812 254 19.450 19.33 126.50 1169.0 0.10350 447 14.800 17.66 95.88 674.8 0.09179 301 12.460 19.89 80.43 471.3 0.08451 133 15.710 13.93 102.00 761.7 0.09462 187 11.710 17.19 74.68 420.3 0.09774 78 20.180 23.97 143.70 1245.0 0.12860 319 12.430 17.00 78.60 477.3 0.07557 412 9.397 21.68 59.75 268.8 0.07969 349 11.950 14.96 77.23 426.7 0.11580 11 15.780 17.89 103.60 781.0 0.09710 240 13.640 15.60 87.38 575.3 0.09423 29 17.570 15.05 115.00 955.1 0.09847 302 20.090 23.86 134.70 1247.0 0.10800 521 24.630 21.60 165.50 1841.0 0.10300 373 20.640 17.35 134.80 1335.0 0.09446 481 13.900 19.24 88.73 602.9 0.07991 100 13.610 24.98 88.05 582.7 0.09488 304 11.460 18.16 73.59 403.1 0.08853 159 10.900 12.96 68.69 366.8 0.07515 360 12.540 18.07 79.42 491.9 0.07436 .. ... ... ... ... ... 202 23.290 26.67 158.90 1685.0 0.11410 435 13.980 19.62 91.12 599.5 0.10600 375 16.170 16.07 106.30 788.5 0.09880 47 13.170 18.66 85.98 534.6 0.11580 497 12.470 17.31 80.45 480.1 0.08928 13 15.850 23.95 103.70 782.7 0.08401 221 13.560 13.90 88.59 561.3 0.10510 22 15.340 14.26 102.50 704.4 0.10730 255 13.960 17.05 91.43 602.4 0.10960 109 11.340 21.26 72.48 396.5 0.08759 348 11.470 16.03 73.02 402.7 0.09076 129 19.790 25.12 130.40 1192.0 0.10150 152 9.731 15.34 63.78 300.2 0.10720 67 11.310 19.04 71.80 394.1 0.08139 213 17.420 25.56 114.50 948.0 0.10060 495 14.870 20.21 96.12 680.9 0.09587 517 19.890 20.26 130.50 1214.0 0.10370 219 19.530 32.47 128.00 1223.0 0.08420 290 14.410 19.73 96.03 651.0 0.08757 488 11.680 16.17 75.49 420.5 0.11280 309 13.050 13.84 82.71 530.6 0.08352 6 18.250 19.98 119.60 1040.0 0.09463 405 10.940 18.59 70.39 370.0 0.10040 452 12.000 28.23 76.77 442.5 0.08437 54 15.100 22.02 97.26 712.8 0.09056 305 11.600 24.49 74.23 417.2 0.07474 560 14.050 27.15 91.38 600.4 0.09929 285 12.580 18.40 79.83 489.0 0.08393 355 12.560 19.07 81.92 485.8 0.08760 329 16.260 21.88 107.50 826.8 0.11650 compactness_mean concavity_mean concave points_mean symmetry_mean \\ 531 0.09453 0.042000 0.021570 0.1859 166 0.05736 0.025310 0.016980 0.1381 485 0.15110 0.154400 0.048460 0.2082 66 0.07773 0.021720 0.015040 0.1717 220 0.08711 0.038880 0.025630 0.1360 356 0.13040 0.096030 0.056030 0.2035 414 0.04605 0.046860 0.027390 0.1852 525 0.07632 0.025650 0.015100 0.1678 77 0.21460 0.168400 0.108000 0.2152 239 0.12980 0.141700 0.088110 0.1809 254 0.11880 0.137900 0.085910 0.1776 447 0.08890 0.040690 0.022600 0.1893 301 0.10140 0.068300 0.030990 0.1781 133 0.09462 0.071350 0.059330 0.1816 187 0.06141 0.038090 0.032390 0.1516 78 0.34540 0.375400 0.160400 0.2906 319 0.03454 0.013420 0.016990 0.1472 412 0.06053 0.037350 0.005128 0.1274 349 0.12060 0.011710 0.017870 0.2459 11 0.12920 0.099540 0.066060 0.1842 240 0.06630 0.047050 0.037310 0.1717 29 0.11570 0.098750 0.079530 0.1739 302 0.18380 0.228300 0.128000 0.2249 521 0.21060 0.231000 0.147100 0.1991 373 0.10760 0.152700 0.089410 0.1571 481 0.05326 0.029950 0.020700 0.1579 100 0.08511 0.086250 0.044890 0.1609 304 0.07694 0.033440 0.015020 0.1411 159 0.03718 0.003090 0.006588 0.1442 360 0.02650 0.001194 0.005449 0.1528 .. ... ... ... ... 202 0.20840 0.352300 0.162000 0.2200 435 0.11330 0.112600 0.064630 0.1669 375 0.14380 0.066510 0.053970 0.1990 47 0.12310 0.122600 0.073400 0.2128 497 0.07630 0.036090 0.023690 0.1526 13 0.10020 0.099380 0.053640 0.1847 221 0.11920 0.078600 0.044510 0.1962 22 0.21350 0.207700 0.097560 0.2521 255 0.12790 0.097890 0.052460 0.1908 109 0.06575 0.051330 0.018990 0.1487 348 0.05886 0.025870 0.023220 0.1634 129 0.15890 0.254500 0.114900 0.2202 152 0.15990 0.410800 0.078570 0.2548 67 0.04701 0.037090 0.022300 0.1516 213 0.11460 0.168200 0.065970 0.1308 495 0.08345 0.068240 0.049510 0.1487 517 0.13100 0.141100 0.094310 0.1802 219 0.11300 0.114500 0.066370 0.1428 290 0.16760 0.136200 0.066020 0.1714 488 0.09263 0.042790 0.031320 0.1853 309 0.03735 0.004559 0.008829 0.1453 6 0.10900 0.112700 0.074000 0.1794 405 0.07460 0.049440 0.029320 0.1486 452 0.06450 0.040550 0.019450 0.1615 54 0.07081 0.052530 0.033340 0.1616 305 0.05688 0.019740 0.013130 0.1935 560 0.11260 0.044620 0.043040 0.1537 285 0.04216 0.001860 0.002924 0.1697 355 0.10380 0.103000 0.043910 0.1533 329 0.12830 0.179900 0.079810 0.1869 fractal_dimension_mean ... perimeter_worst area_worst \\ 531 0.06461 ... 87.00 550.6 166 0.06400 ... 73.66 414.0 485 0.07325 ... 97.82 580.6 66 0.06899 ... 67.03 330.7 220 0.06344 ... 99.71 706.2 356 0.06501 ... 94.22 591.2 414 0.05294 ... 110.10 931.4 525 0.07126 ... 63.30 275.6 77 0.06673 ... 150.10 1610.0 239 0.05966 ... 141.20 1408.0 254 0.05647 ... 163.10 1972.0 447 0.05886 ... 105.90 829.5 301 0.06249 ... 88.13 551.3 133 0.05723 ... 114.30 922.8 187 0.06095 ... 84.42 521.5 78 0.08142 ... 170.30 1623.0 319 0.05561 ... 81.76 515.9 412 0.06724 ... 66.61 301.0 349 0.06581 ... 83.09 496.2 11 0.06082 ... 136.50 1299.0 240 0.05660 ... 94.11 683.4 29 0.06149 ... 134.90 1227.0 302 0.07469 ... 158.80 1696.0 521 0.06739 ... 205.70 2642.0 373 0.05478 ... 166.80 1946.0 481 0.05594 ... 104.40 830.5 100 0.05871 ... 108.60 906.5 304 0.06243 ... 82.69 489.8 159 0.05743 ... 78.07 470.0 360 0.05185 ... 86.82 585.7 .. ... ... ... ... 202 0.06229 ... 177.00 1986.0 435 0.06544 ... 113.90 869.3 375 0.06572 ... 113.10 861.5 47 0.06777 ... 102.80 759.4 497 0.06046 ... 92.82 607.3 13 0.05338 ... 112.00 876.5 221 0.06303 ... 101.10 686.6 22 0.07032 ... 125.10 980.9 255 0.06130 ... 108.10 826.0 109 0.06529 ... 83.99 518.1 348 0.06372 ... 79.67 475.8 129 0.06113 ... 148.70 1589.0 152 0.09296 ... 71.04 380.5 67 0.05667 ... 78.00 466.7 213 0.05866 ... 120.40 1021.0 495 0.05748 ... 103.90 783.6 517 0.06188 ... 160.50 1646.0 219 0.05313 ... 180.20 2477.0 290 0.07192 ... 101.70 767.3 488 0.06401 ... 86.57 549.8 309 0.05518 ... 93.96 672.4 6 0.05742 ... 153.20 1606.0 405 0.06615 ... 82.76 472.4 452 0.06104 ... 85.07 523.7 54 0.05684 ... 117.70 1030.0 305 0.05878 ... 81.39 476.5 560 0.06171 ... 100.20 706.7 285 0.05855 ... 85.56 564.1 355 0.06184 ... 89.02 547.4 329 0.06532 ... 113.70 975.2 smoothness_worst compactness_worst concavity_worst \\ 531 0.15500 0.29640 0.275800 166 0.14360 0.12570 0.104700 485 0.11750 0.40610 0.489600 66 0.15480 0.16640 0.094120 220 0.13110 0.24740 0.175900 356 0.13430 0.26580 0.257300 414 0.11480 0.09866 0.154700 525 0.16410 0.22350 0.175400 77 0.14780 0.56340 0.378600 239 0.13650 0.37350 0.324100 254 0.14970 0.31610 0.431700 447 0.12260 0.18810 0.206000 301 0.10500 0.21580 0.190400 133 0.12230 0.19490 0.170900 187 0.13230 0.10400 0.152100 78 0.16390 0.61640 0.768100 319 0.08409 0.04712 0.022370 412 0.10860 0.18870 0.186800 349 0.12930 0.18850 0.031220 11 0.13960 0.56090 0.396500 240 0.12780 0.12910 0.153300 29 0.12550 0.28120 0.248900 302 0.13470 0.33910 0.493200 521 0.13420 0.41880 0.465800 373 0.15620 0.30550 0.415900 481 0.10640 0.14150 0.167300 100 0.12650 0.19430 0.316900 304 0.11440 0.17890 0.122600 159 0.11710 0.08294 0.018540 360 0.09293 0.04327 0.003581 .. ... ... ... 202 0.15360 0.41670 0.789200 435 0.16130 0.35680 0.406900 375 0.12350 0.25500 0.211400 47 0.17860 0.41660 0.500600 497 0.12760 0.25060 0.202800 13 0.11310 0.19240 0.232200 221 0.13760 0.26980 0.257700 22 0.13900 0.59540 0.630500 255 0.15120 0.32620 0.320900 109 0.16990 0.21960 0.312000 348 0.15310 0.11200 0.098230 129 0.12750 0.38610 0.567300 152 0.12920 0.27720 0.821600 67 0.12900 0.09148 0.144400 213 0.12430 0.17930 0.280300 495 0.12160 0.13880 0.170000 517 0.14170 0.33090 0.418500 219 0.14080 0.40970 0.399500 290 0.09983 0.24720 0.222000 488 0.15260 0.14770 0.149000 309 0.10160 0.05847 0.018240 6 0.14420 0.25760 0.378400 405 0.13630 0.16440 0.141200 452 0.12080 0.18560 0.181100 54 0.13890 0.20570 0.271200 305 0.09545 0.13610 0.072390 560 0.12410 0.22640 0.132600 285 0.10380 0.06624 0.005579 355 0.10960 0.20020 0.238800 329 0.14260 0.21160 0.334400 concave points_worst symmetry_worst fractal_dimension_worst diagnosis \\ 531 0.081200 0.3206 0.08950 0 166 0.046030 0.2090 0.07699 0 485 0.134200 0.3231 0.10340 0 66 0.065170 0.2878 0.09211 0 220 0.080560 0.2380 0.08718 0 356 0.125800 0.3113 0.08317 0 414 0.065750 0.3233 0.06165 1 525 0.085120 0.2983 0.10490 0 77 0.210200 0.3751 0.11080 1 239 0.206600 0.2853 0.08496 1 254 0.199900 0.3379 0.08950 1 447 0.083080 0.3600 0.07285 0 301 0.076250 0.2685 0.07764 0 133 0.137400 0.2723 0.07071 0 187 0.109900 0.2572 0.07097 0 78 0.250800 0.5440 0.09964 1 319 0.028320 0.1901 0.05932 0 412 0.025640 0.2376 0.09206 0 349 0.047660 0.3124 0.07590 0 11 0.181000 0.3792 0.10480 1 240 0.092220 0.2530 0.06510 0 29 0.145600 0.2756 0.07919 1 302 0.192300 0.3294 0.09469 1 521 0.247500 0.3157 0.09671 1 373 0.211200 0.2689 0.07055 1 481 0.081500 0.2356 0.07603 0 100 0.118400 0.2651 0.07397 1 304 0.055090 0.2208 0.07638 0 159 0.039530 0.2738 0.07685 0 360 0.016350 0.2233 0.05521 0 .. ... ... ... ... 202 0.273300 0.3198 0.08762 1 435 0.182700 0.3179 0.10550 1 375 0.125100 0.3153 0.08960 0 47 0.208800 0.3900 0.11790 1 497 0.105300 0.3035 0.07661 0 13 0.111900 0.2809 0.06287 1 221 0.090900 0.3065 0.08177 0 22 0.239300 0.4667 0.09946 1 255 0.137400 0.3068 0.07957 1 109 0.082780 0.2829 0.08832 0 348 0.065480 0.2851 0.08763 0 129 0.173200 0.3305 0.08465 1 152 0.157100 0.3108 0.12590 0 67 0.069610 0.2400 0.06641 0 213 0.109900 0.1603 0.06818 1 495 0.101700 0.2369 0.06599 0 517 0.161300 0.2549 0.09136 1 219 0.162500 0.2713 0.07568 1 290 0.102100 0.2272 0.08799 0 488 0.098150 0.2804 0.08024 0 309 0.035320 0.2107 0.06580 0 6 0.193200 0.3063 0.08368 1 405 0.078870 0.2251 0.07732 0 452 0.071160 0.2447 0.08194 0 54 0.153000 0.2675 0.07873 1 305 0.048150 0.3244 0.06745 0 560 0.104800 0.2250 0.08321 0 285 0.008772 0.2505 0.06431 0 355 0.092650 0.2121 0.07188 0 329 0.104700 0.2736 0.07953 1 prediksi 531 0 166 0 485 0 66 0 220 0 356 0 414 1 525 0 77 1 239 1 254 1 447 0 301 0 133 1 187 0 78 1 319 0 412 0 349 0 11 1 240 0 29 1 302 1 521 1 373 1 481 1 100 1 304 0 159 0 360 0 .. ... 202 1 435 0 375 0 47 1 497 0 13 0 221 0 22 1 255 1 109 0 348 0 129 1 152 0 67 0 213 1 495 0 517 1 219 1 290 0 488 0 309 0 6 1 405 0 452 0 54 1 305 0 560 0 285 0 355 0 329 1 [171 rows x 32 columns] Agar lebih enak dilihat kita sederhanakan dulu,kita hasila menampilkan kolom \"diagnosis\" dari data testing dan hasil prediksi \"prediction\" data1=pd.DataFrame({\"diagnosis\":test_y,\"hasil prediksi\":prediction}) print(data1) Output: diagnosis hasil prediksi 531 0 0 166 0 0 485 0 0 66 0 0 220 0 0 356 0 0 414 1 1 525 0 0 77 1 1 239 1 1 254 1 1 447 0 0 301 0 0 133 0 1 187 0 0 78 1 1 319 0 0 412 0 0 349 0 0 11 1 1 240 0 0 29 1 1 302 1 1 521 1 1 373 1 1 481 0 1 100 1 1 304 0 0 159 0 0 360 0 0 .. ... ... 202 1 1 435 1 0 375 0 0 47 1 1 497 0 0 13 1 0 221 0 0 22 1 1 255 1 1 109 0 0 348 0 0 129 1 1 152 0 0 67 0 0 213 1 1 495 0 0 517 1 1 219 1 1 290 0 0 488 0 0 309 0 0 6 1 1 405 0 0 452 0 0 54 1 1 305 0 0 560 0 0 285 0 0 355 0 0 329 1 1 [171 rows x 2 columns] Untuk program dan notebook lengkapnya bisa diunduh disini Referensi \u00b6 Siti Mutrofin, Abidatul Izzah, Arrie Kurniawardhani, Mukhamad Masrur. 2014. OPTIMASI TEKNIK KLASIFIKASI MODIFIED K NEAREST NEIGHBOR. JURNAL GAMMA. 10(1): 1-5 Mayu Shinohara. 2017. Hyper Parameters Tuning of DTree,RF,SVM,kNN di https://www.kaggle.com/mayu0116/hyper-parameters-tuning-of-dtree-rf-svm-knn informatika. 2017. Algoritma K-Nearest Neighbor (K-NN) di https://informatikalogi.com/algoritma-k-nn-k-nearest-neighbor/ Asep Maulana Ismail. 2018. Cara Kerja Algoritma k-Nearest Neighbor (k-NN) di https://medium.com/bee-solution-partners/cara-kerja-algoritma-k-nearest-neighbor-k-nn-389297de543e","title":"K-Nearest Neighbor"},{"location":"knn/#pengertian-k-nearest-neighbor","text":"Algoritma k-Nearest Neighbor adalah algoritma supervised learning dimana hasil dari instance yang baru diklasifikasikan berdasarkan mayoritas dari kategori k-tetangga terdekat. Tujuan dari algoritma ini adalah untuk mengklasifikasikan obyek baru berdasarkan atribut dan sample-sample dari training data. Algoritma k-Nearest Neighbor menggunakan Neighborhood Classification sebagai nilai prediksi dari nilai instance yang baru.","title":"Pengertian K-Nearest Neighbor"},{"location":"knn/#kelebihan-dan-kekurangan-k-nearest-neighbor","text":"","title":"Kelebihan dan Kekurangan K-Nearest Neighbor"},{"location":"knn/#kelebihan-k-nearest-neighbor","text":"Pelatihan sangat cepat Sederhana dan mudah dipelajari Tahan terhadap data pelatihan yang memiliki derau Efektif jika data pelatihan besar.","title":"Kelebihan K-Nearest Neighbor"},{"location":"knn/#kekurangan-k-nearest-neighbor","text":"Nilai k bias Komputasi kompleks Keterbatasan memori, Mudah tertipu dengan atribut yang tidak relevan.","title":"Kekurangan K-Nearest Neighbor"},{"location":"knn/#algoritma-k-nearest-neighbor","text":"","title":"Algoritma K-Nearest Neighbor"},{"location":"knn/#langkah-langkah-k-nearest-neighbor","text":"Menentukan parameter K sebagai banyaknya jumlah tetangga terdekat dengan objek baru. Menghitung jarak antar objek/data baru terhadap semua objek/data yan gtelah di training. Urutkan hasil perhitungan tersebut. Tentukan tetangga terdekat berdasarkan jarak minimum ke K. Tentukan kategori dari tetangga terdekat dengan objek/data. Gunakan kategori mayoritas sebagai klasifikasi objek/data baru.","title":"Langkah-langkah K-Nearest Neighbor  :"},{"location":"knn/#contoh-perhitungan-knn-sederhana","text":"Misalnya ada sebuah rumah yang berada tepat di tengah perbatasan antara Kota Bandung dan Kabupaten Bandung, sehingga pemerintah kesulitan untuk menentukan apakah rumah tersebut termasuk kedalam wilayah Kota Bandung atau Kabupaten Bandung . Kita bisa menentukannya dengan menggunakan Algoritma k-NN , yaitu dengan melibatkan jarak antara rumah tersebut dengan rumah-rumah yang ada disekitarnya (tetangganya). Pertama , kita harus menentukan jumlah tetangga yg akan kita perhitungkan (k), misalnya kita tentukan 3 tetangga terdekat ( k = 3 ). Kedua , hitung jarak setiap tetangga terhadap rumah tersebut, lalu urutkan hasilnya berdasarkan jarak, mulai dari yang terkecil ke yang terbesar. Ketiga , ambil 3 (k) tetangga yg paling dekat, lalu kita lihat masing-masing dari tetangga tersebut apakah termasuk kedalam wilayah Kota atau Kabupaten. Ada 2 kemungkinan: Bila dari 3 tetangga tersebut terdapat ada 2 rumah yg termasuk kedalam wilayah Kota Bandung, maka rumah tersebut termasuk kedalam wilayah Kota Bandung. Sebaliknya, bila dari 3 tetangga tersebut terdapat 2 rumah yg termasuk kedalam wilayah Kabupaten Bandung, maka rumah tersebut termasuk kedalam wilayah Kabupaten Bandung. Dalam menentukan nilai k* , bila* jumlah klasifikasi kita genap maka sebaiknya kita gunakan nilai k ganjil* , dan begitu pula sebaliknya bila* jumlah klasifikasi kita ganjil maka sebaiknya gunakan nilai k genap* , karena jika tidak begitu, ada kemungkinan kita* tidak akan mendapatkan jawaban* .*","title":"Contoh Perhitungan KNN Sederhana"},{"location":"knn/#pembahasan-lebih-detil","text":"Pada kasus diatas, kita menghitung jarak suatu rumah terhadap tetangga-tetangganya, itu berarti kita harus mengetahui posisi dari setiap rumah. Kita bisa menggunakan latitude dan longitude (atau garis lintang dan garis bujur) sebagai posisi. Untuk mempermudah pemahaman, saya akan coba menggunakan data yang nilainya sederhana. Data yang akan digunakan adalah sebagai berikut: Dari data diatas, kita mendapatkan beberapa informasi, diantaranya: Independent Variables , yaitu variable yang nilainya tidak dipengaruhi oleh variable lain. Pada contoh data diatas, yang termasuk independent variable adalah Lat , dan Long . Dependent Variables , yaitu variable yang nilainya dipengaruhi oleh variable lain. Pada contoh data diatas, yang termasuk *dependent variable*adalah Lokasi . Rumah A-E adalah rumah yang masuk ke dalam wilayah Kota . Rumah F-G adalah rumah yang masuk ke dalam wilayah Kabupaten . Rumah X adalah rumah yang akan kita prediksi menggunakan algoritma kNN apakah termasuk ke dalam wilayah Kota atau Kabupaten. Didalam dunia Machine Learning* ,* Independent Variables sering disebut juga sebagai Features Selanjutnya kita hitung jarak antara rumah X terhadap rumah A-G dengan menggunakan rumus Euclidean Distance : $$ \\begin{aligned} d(p, q)=d(q, p) &=\\sqrt{\\left(q_{1}-p_{1}\\right) {2}+\\left(q_{2}-p_{2}\\right) {2}+\\cdots+\\left(q_{n}-p_{n}\\right)^{2}} \\ &=\\sqrt{\\sum_{i=1} {n}\\left(q_{1}-p_{i}\\right) {2}} \\end{aligned} $$ Perhitungan jarak rumah X dengan rumah A: =\u221a(11-19)^2 + (26-25)^2 =\u221a(-8)^2 + (1)^2 =\u221a65 =8,062257748298550 setelah jarak rumah X dengan rumah A ditemukan tinggal menghitung jarak antara rumah X terhadap rumah B-G Setelah dihitung, selanjutnya adalah urutkan jarak tersebut dari yang paling kecil ke yang paling besar , hasilnya adalah sebagai berikut: Dapat dilihat dari hasil perhitungan diatas, bahwa ternyata 3 tetangga terdekat dari rumah X adalah: Rumah H (Kabupaten) yang memiliki jarak 2.24 , Rumah C (Kota) yang memiliki jarak 3 , dan Rumah E (Kota) yang memiliki jarak 3.16 . Dari ke-3 tetangga terdekat, terdapat 2 rumah yang termasuk kedalam wilayah Kota dan 1 rumah yang masuk ke dalam wilayah Kabupaten . Sehingga dapat disimpulkan, bahwa Rumah X adalah rumah yang termasuk kedalam wilayah Kota","title":"Pembahasan Lebih Detil"},{"location":"knn/#implementasi-k-nearest-neighbor-breast-cancer-wisconsin-diagnostic-dataset","text":"","title":"Implementasi K-Nearest Neighbor Breast Cancer Wisconsin (Diagnostic) Dataset"},{"location":"knn/#import-library-yang-dibutuhkan","text":"import numpy as np import numpy as np import pandas as pd import matplotlib.pyplot as plt from sklearn.neighbors import KNeighborsClassifier from sklearn.model_selection import train_test_split from sklearn import metrics #importing modul metrik","title":"Import Library yang dibutuhkan"},{"location":"knn/#memuat-dataset","text":"Mengimport dataset yang digunakan untuk pengimplemtasian K-Nearest Neighbor dataset bisa didownload disini atau langsung dari kaggle disini # Memuat dataset data = pd.read_csv(\"E:\\Semester 4\\data mining/bc.csv\",encoding = \"ISO-8859-1\")","title":"Memuat Dataset"},{"location":"knn/#data-preprocessing","text":"","title":"Data Preprocessing"},{"location":"knn/#menampilkan-5-data-teratas","text":"Menampilkan jumlah masing-masing diagnosis dan kolom diagnosis digunakan sebagai class nantinya. data.head(5) Output:","title":"Menampilkan 5 Data teratas"},{"location":"knn/#menampilkan-ringkasan-dataset-serta-menghilangkan-kolom-yang-tidak-berguna","text":"# ringkasan dataset data.info() Output: <class 'pandas.core.frame.DataFrame'> RangeIndex: 569 entries, 0 to 568 Data columns (total 33 columns): id 569 non-null int64 diagnosis 569 non-null object radius_mean 569 non-null float64 texture_mean 569 non-null float64 perimeter_mean 569 non-null float64 area_mean 569 non-null float64 smoothness_mean 569 non-null float64 compactness_mean 569 non-null float64 concavity_mean 569 non-null float64 concave points_mean 569 non-null float64 symmetry_mean 569 non-null float64 fractal_dimension_mean 569 non-null float64 radius_se 569 non-null float64 texture_se 569 non-null float64 perimeter_se 569 non-null float64 area_se 569 non-null float64 smoothness_se 569 non-null float64 compactness_se 569 non-null float64 concavity_se 569 non-null float64 concave points_se 569 non-null float64 symmetry_se 569 non-null float64 fractal_dimension_se 569 non-null float64 radius_worst 569 non-null float64 texture_worst 569 non-null float64 perimeter_worst 569 non-null float64 area_worst 569 non-null float64 smoothness_worst 569 non-null float64 compactness_worst 569 non-null float64 concavity_worst 569 non-null float64 concave points_worst 569 non-null float64 symmetry_worst 569 non-null float64 fractal_dimension_worst 569 non-null float64 Unnamed: 32 0 non-null float64 dtypes: float64(31), int64(1), object(1) memory usage: 146.8+ KB bisa dilihat dari hasil output diatas ada kolom yang tidak bernama (\"Unnamed\") akan kita hilangkan, serta menghilangkan kolom yang tidak berguna yaitu kolom ()\"id\") : #menghapus kolom yang tidak berguna #menghapus kolom \"id\" data.drop(\"id\",axis=1,inplace=True) #menghapus the \"Unnamed: 32\" column data.drop(\"Unnamed: 32\",axis=1,inplace=True) #hasil data.info() Output: bisa dilihat kolom \"id\" dan kolom yang tidak mempunyai nama \"unnamed\" telah hilang <class 'pandas.core.frame.DataFrame'> RangeIndex: 569 entries, 0 to 568 Data columns (total 31 columns): diagnosis 569 non-null object radius_mean 569 non-null float64 texture_mean 569 non-null float64 perimeter_mean 569 non-null float64 area_mean 569 non-null float64 smoothness_mean 569 non-null float64 compactness_mean 569 non-null float64 concavity_mean 569 non-null float64 concave points_mean 569 non-null float64 symmetry_mean 569 non-null float64 fractal_dimension_mean 569 non-null float64 radius_se 569 non-null float64 texture_se 569 non-null float64 perimeter_se 569 non-null float64 area_se 569 non-null float64 smoothness_se 569 non-null float64 compactness_se 569 non-null float64 concavity_se 569 non-null float64 concave points_se 569 non-null float64 symmetry_se 569 non-null float64 fractal_dimension_se 569 non-null float64 radius_worst 569 non-null float64 texture_worst 569 non-null float64 perimeter_worst 569 non-null float64 area_worst 569 non-null float64 smoothness_worst 569 non-null float64 compactness_worst 569 non-null float64 concavity_worst 569 non-null float64 concave points_worst 569 non-null float64 symmetry_worst 569 non-null float64 fractal_dimension_worst 569 non-null float64 dtypes: float64(30), object(1) memory usage: 137.9+ KB Kemudian kita lihat lagi kolom beserta 5 data pertama: # 5 baris pertama data.head(5) Output:","title":"Menampilkan ringkasan dataset serta menghilangkan kolom yang tidak berguna"},{"location":"knn/#mengganti-class-m-dan-b-menjadi-0-dan-1","text":"diagnosis adalah variabel yang bertanggung jawab untuk klasifikasi disini kita mengganti M dan B masing-masing dengan 1 dan 0 #diagnosis adalah variabel yang bertanggung jawab untuk klasifikasi #mengganti M dan B masing-masing dengan 1 dan 0 data.diagnosis=data.diagnosis.map({'M':1,'B':0}) Kemudian kita hitung berapa banyak jumlah masing- masing feature #menghitung variabel diagnosis data.diagnosis.value_counts() output: 0 357 1 212 Name: diagnosis, dtype: int64","title":"Mengganti class M dan B menjadi 0 dan 1"},{"location":"knn/#membagi-data-30-sebagai-data-testing-dan-70-sebagai-data-training","text":"# preprocessing dataset selesai #splitting dataset ke training dan testing train, test = train_test_split(data, test_size = 0.3,random_state=1234) #mencari hasil print(train.shape) print(test.shape) (398, 31) (171, 31)","title":"Membagi data 30% sebagai data testing dan 70% sebagai data training"},{"location":"knn/#membuat-variabel-independen-dan-responsible","text":"variabel independen dan responsible nantinya akan digunakan dalam proses prediksi variable independen mengambil dari semua kolom dan variable responsible dari diagnosis #membuat variabel independen untuk training train_X = train.iloc[:, 1:31] #membuat variabel responsible untuk training train_y=train.diagnosis #membuat variabel independen untuk testing test_X= test.iloc[:, 1:31] #membuat variabel responsible untuk ttesting test_y =test.diagnosis kita cek dulu berapa jumlahnya #mencari hasil print(train_X.shape) print(train_y.shape) print(test_X.shape) print(test_y.shape) output: (398, 30) (398,) (171, 30) (171,)","title":"Membuat variabel independen dan responsible"},{"location":"knn/#mencari-k-yang-terbaikideal","text":"K yang terbaik disini akan menghasilkan akurasi yang tinggi, Pada langkah ini saya memberikan range / jarak 1 sampai 30 untuk mencari K terbaik dan akurasi terbaik. Sehingga saya melakukan pengulangan 1 - 30 dan mencari nilai akurasi tertinggi. neighbors=np.arange(1,31) accuracy_train=[] accuracy_test=[] for i,k in enumerate(neighbors): knn=KNeighborsClassifier(n_neighbors=k) knn.fit(train_X,train_y) accuracy_train.append(knn.score(train_X,train_y)) accuracy_test.append(knn.score(test_X,test_y)) plt.figure(figsize=(13,8)) plt.plot(neighbors,accuracy_train,label=\"Akurasi training\") plt.plot(neighbors,accuracy_test,label=\"Akurasi testing\") acideal=np.max(accuracy_test) kideal=1+accuracy_test.index(np.max(accuracy_test)) plt.title('Tingkat Akurasi') plt.xlabel(\"K yang dipilih\") plt.ylabel(\"Tingkat Akurasi\") plt.xticks(neighbors) plt.legend() plt.show() print(\"Akurasi ideal:\",acideal) print(\"K ideal:\",kideal) Output: Akurasi ideal: 0.9415204678362573 K ideal: 7 Akurasi yang ideal 0.9415204678362573 pada k = 7","title":"Mencari K yang terbaik(ideal)"},{"location":"knn/#menghitung-knn","text":"#membuat instance model = KNeighborsClassifier(n_neighbors=int(input(\"Masukan jumlah k:\"))) #learning model.fit(train_X,train_y) #Prediksi prediction=model.predict(test_X) #evaluation (Akurasi) print(\"Akurasi:\",metrics.accuracy_score(prediction,test_y)) #evaluation(Confusion Metrix) print(\"Confusion Metrix:\\n\",metrics.confusion_matrix(prediction,test_y)) Output: Masukan jumlah k:7 Akurasi: 0.9415204678362573 Confusion Metrix: [[102 7] [ 3 59]]","title":"Menghitung KNN"},{"location":"knn/#menampilkan-hasil-prediksi-datatesting","text":"Pada langkah ini kita menggabungkan data testing, diagnosis dan prediction . sehingga kita dapat melihat hasil prediksi dari semua data. datatest=pd.DataFrame(test_X) datatest['diagnosis']=test_y datatest['prediksi']=prediction print(datatest) Output: radius_mean texture_mean perimeter_mean area_mean smoothness_mean \\ 531 11.670 20.02 75.21 416.2 0.10160 166 10.800 9.71 68.77 357.6 0.09594 485 12.450 16.41 82.85 476.7 0.09514 66 9.465 21.01 60.11 269.4 0.10440 220 13.650 13.16 87.88 568.9 0.09646 356 13.050 18.59 85.09 512.0 0.10820 414 15.130 29.81 96.71 719.5 0.08320 525 8.571 13.10 54.53 221.3 0.10360 77 18.050 16.15 120.20 1006.0 0.10650 239 17.460 39.28 113.40 920.6 0.09812 254 19.450 19.33 126.50 1169.0 0.10350 447 14.800 17.66 95.88 674.8 0.09179 301 12.460 19.89 80.43 471.3 0.08451 133 15.710 13.93 102.00 761.7 0.09462 187 11.710 17.19 74.68 420.3 0.09774 78 20.180 23.97 143.70 1245.0 0.12860 319 12.430 17.00 78.60 477.3 0.07557 412 9.397 21.68 59.75 268.8 0.07969 349 11.950 14.96 77.23 426.7 0.11580 11 15.780 17.89 103.60 781.0 0.09710 240 13.640 15.60 87.38 575.3 0.09423 29 17.570 15.05 115.00 955.1 0.09847 302 20.090 23.86 134.70 1247.0 0.10800 521 24.630 21.60 165.50 1841.0 0.10300 373 20.640 17.35 134.80 1335.0 0.09446 481 13.900 19.24 88.73 602.9 0.07991 100 13.610 24.98 88.05 582.7 0.09488 304 11.460 18.16 73.59 403.1 0.08853 159 10.900 12.96 68.69 366.8 0.07515 360 12.540 18.07 79.42 491.9 0.07436 .. ... ... ... ... ... 202 23.290 26.67 158.90 1685.0 0.11410 435 13.980 19.62 91.12 599.5 0.10600 375 16.170 16.07 106.30 788.5 0.09880 47 13.170 18.66 85.98 534.6 0.11580 497 12.470 17.31 80.45 480.1 0.08928 13 15.850 23.95 103.70 782.7 0.08401 221 13.560 13.90 88.59 561.3 0.10510 22 15.340 14.26 102.50 704.4 0.10730 255 13.960 17.05 91.43 602.4 0.10960 109 11.340 21.26 72.48 396.5 0.08759 348 11.470 16.03 73.02 402.7 0.09076 129 19.790 25.12 130.40 1192.0 0.10150 152 9.731 15.34 63.78 300.2 0.10720 67 11.310 19.04 71.80 394.1 0.08139 213 17.420 25.56 114.50 948.0 0.10060 495 14.870 20.21 96.12 680.9 0.09587 517 19.890 20.26 130.50 1214.0 0.10370 219 19.530 32.47 128.00 1223.0 0.08420 290 14.410 19.73 96.03 651.0 0.08757 488 11.680 16.17 75.49 420.5 0.11280 309 13.050 13.84 82.71 530.6 0.08352 6 18.250 19.98 119.60 1040.0 0.09463 405 10.940 18.59 70.39 370.0 0.10040 452 12.000 28.23 76.77 442.5 0.08437 54 15.100 22.02 97.26 712.8 0.09056 305 11.600 24.49 74.23 417.2 0.07474 560 14.050 27.15 91.38 600.4 0.09929 285 12.580 18.40 79.83 489.0 0.08393 355 12.560 19.07 81.92 485.8 0.08760 329 16.260 21.88 107.50 826.8 0.11650 compactness_mean concavity_mean concave points_mean symmetry_mean \\ 531 0.09453 0.042000 0.021570 0.1859 166 0.05736 0.025310 0.016980 0.1381 485 0.15110 0.154400 0.048460 0.2082 66 0.07773 0.021720 0.015040 0.1717 220 0.08711 0.038880 0.025630 0.1360 356 0.13040 0.096030 0.056030 0.2035 414 0.04605 0.046860 0.027390 0.1852 525 0.07632 0.025650 0.015100 0.1678 77 0.21460 0.168400 0.108000 0.2152 239 0.12980 0.141700 0.088110 0.1809 254 0.11880 0.137900 0.085910 0.1776 447 0.08890 0.040690 0.022600 0.1893 301 0.10140 0.068300 0.030990 0.1781 133 0.09462 0.071350 0.059330 0.1816 187 0.06141 0.038090 0.032390 0.1516 78 0.34540 0.375400 0.160400 0.2906 319 0.03454 0.013420 0.016990 0.1472 412 0.06053 0.037350 0.005128 0.1274 349 0.12060 0.011710 0.017870 0.2459 11 0.12920 0.099540 0.066060 0.1842 240 0.06630 0.047050 0.037310 0.1717 29 0.11570 0.098750 0.079530 0.1739 302 0.18380 0.228300 0.128000 0.2249 521 0.21060 0.231000 0.147100 0.1991 373 0.10760 0.152700 0.089410 0.1571 481 0.05326 0.029950 0.020700 0.1579 100 0.08511 0.086250 0.044890 0.1609 304 0.07694 0.033440 0.015020 0.1411 159 0.03718 0.003090 0.006588 0.1442 360 0.02650 0.001194 0.005449 0.1528 .. ... ... ... ... 202 0.20840 0.352300 0.162000 0.2200 435 0.11330 0.112600 0.064630 0.1669 375 0.14380 0.066510 0.053970 0.1990 47 0.12310 0.122600 0.073400 0.2128 497 0.07630 0.036090 0.023690 0.1526 13 0.10020 0.099380 0.053640 0.1847 221 0.11920 0.078600 0.044510 0.1962 22 0.21350 0.207700 0.097560 0.2521 255 0.12790 0.097890 0.052460 0.1908 109 0.06575 0.051330 0.018990 0.1487 348 0.05886 0.025870 0.023220 0.1634 129 0.15890 0.254500 0.114900 0.2202 152 0.15990 0.410800 0.078570 0.2548 67 0.04701 0.037090 0.022300 0.1516 213 0.11460 0.168200 0.065970 0.1308 495 0.08345 0.068240 0.049510 0.1487 517 0.13100 0.141100 0.094310 0.1802 219 0.11300 0.114500 0.066370 0.1428 290 0.16760 0.136200 0.066020 0.1714 488 0.09263 0.042790 0.031320 0.1853 309 0.03735 0.004559 0.008829 0.1453 6 0.10900 0.112700 0.074000 0.1794 405 0.07460 0.049440 0.029320 0.1486 452 0.06450 0.040550 0.019450 0.1615 54 0.07081 0.052530 0.033340 0.1616 305 0.05688 0.019740 0.013130 0.1935 560 0.11260 0.044620 0.043040 0.1537 285 0.04216 0.001860 0.002924 0.1697 355 0.10380 0.103000 0.043910 0.1533 329 0.12830 0.179900 0.079810 0.1869 fractal_dimension_mean ... perimeter_worst area_worst \\ 531 0.06461 ... 87.00 550.6 166 0.06400 ... 73.66 414.0 485 0.07325 ... 97.82 580.6 66 0.06899 ... 67.03 330.7 220 0.06344 ... 99.71 706.2 356 0.06501 ... 94.22 591.2 414 0.05294 ... 110.10 931.4 525 0.07126 ... 63.30 275.6 77 0.06673 ... 150.10 1610.0 239 0.05966 ... 141.20 1408.0 254 0.05647 ... 163.10 1972.0 447 0.05886 ... 105.90 829.5 301 0.06249 ... 88.13 551.3 133 0.05723 ... 114.30 922.8 187 0.06095 ... 84.42 521.5 78 0.08142 ... 170.30 1623.0 319 0.05561 ... 81.76 515.9 412 0.06724 ... 66.61 301.0 349 0.06581 ... 83.09 496.2 11 0.06082 ... 136.50 1299.0 240 0.05660 ... 94.11 683.4 29 0.06149 ... 134.90 1227.0 302 0.07469 ... 158.80 1696.0 521 0.06739 ... 205.70 2642.0 373 0.05478 ... 166.80 1946.0 481 0.05594 ... 104.40 830.5 100 0.05871 ... 108.60 906.5 304 0.06243 ... 82.69 489.8 159 0.05743 ... 78.07 470.0 360 0.05185 ... 86.82 585.7 .. ... ... ... ... 202 0.06229 ... 177.00 1986.0 435 0.06544 ... 113.90 869.3 375 0.06572 ... 113.10 861.5 47 0.06777 ... 102.80 759.4 497 0.06046 ... 92.82 607.3 13 0.05338 ... 112.00 876.5 221 0.06303 ... 101.10 686.6 22 0.07032 ... 125.10 980.9 255 0.06130 ... 108.10 826.0 109 0.06529 ... 83.99 518.1 348 0.06372 ... 79.67 475.8 129 0.06113 ... 148.70 1589.0 152 0.09296 ... 71.04 380.5 67 0.05667 ... 78.00 466.7 213 0.05866 ... 120.40 1021.0 495 0.05748 ... 103.90 783.6 517 0.06188 ... 160.50 1646.0 219 0.05313 ... 180.20 2477.0 290 0.07192 ... 101.70 767.3 488 0.06401 ... 86.57 549.8 309 0.05518 ... 93.96 672.4 6 0.05742 ... 153.20 1606.0 405 0.06615 ... 82.76 472.4 452 0.06104 ... 85.07 523.7 54 0.05684 ... 117.70 1030.0 305 0.05878 ... 81.39 476.5 560 0.06171 ... 100.20 706.7 285 0.05855 ... 85.56 564.1 355 0.06184 ... 89.02 547.4 329 0.06532 ... 113.70 975.2 smoothness_worst compactness_worst concavity_worst \\ 531 0.15500 0.29640 0.275800 166 0.14360 0.12570 0.104700 485 0.11750 0.40610 0.489600 66 0.15480 0.16640 0.094120 220 0.13110 0.24740 0.175900 356 0.13430 0.26580 0.257300 414 0.11480 0.09866 0.154700 525 0.16410 0.22350 0.175400 77 0.14780 0.56340 0.378600 239 0.13650 0.37350 0.324100 254 0.14970 0.31610 0.431700 447 0.12260 0.18810 0.206000 301 0.10500 0.21580 0.190400 133 0.12230 0.19490 0.170900 187 0.13230 0.10400 0.152100 78 0.16390 0.61640 0.768100 319 0.08409 0.04712 0.022370 412 0.10860 0.18870 0.186800 349 0.12930 0.18850 0.031220 11 0.13960 0.56090 0.396500 240 0.12780 0.12910 0.153300 29 0.12550 0.28120 0.248900 302 0.13470 0.33910 0.493200 521 0.13420 0.41880 0.465800 373 0.15620 0.30550 0.415900 481 0.10640 0.14150 0.167300 100 0.12650 0.19430 0.316900 304 0.11440 0.17890 0.122600 159 0.11710 0.08294 0.018540 360 0.09293 0.04327 0.003581 .. ... ... ... 202 0.15360 0.41670 0.789200 435 0.16130 0.35680 0.406900 375 0.12350 0.25500 0.211400 47 0.17860 0.41660 0.500600 497 0.12760 0.25060 0.202800 13 0.11310 0.19240 0.232200 221 0.13760 0.26980 0.257700 22 0.13900 0.59540 0.630500 255 0.15120 0.32620 0.320900 109 0.16990 0.21960 0.312000 348 0.15310 0.11200 0.098230 129 0.12750 0.38610 0.567300 152 0.12920 0.27720 0.821600 67 0.12900 0.09148 0.144400 213 0.12430 0.17930 0.280300 495 0.12160 0.13880 0.170000 517 0.14170 0.33090 0.418500 219 0.14080 0.40970 0.399500 290 0.09983 0.24720 0.222000 488 0.15260 0.14770 0.149000 309 0.10160 0.05847 0.018240 6 0.14420 0.25760 0.378400 405 0.13630 0.16440 0.141200 452 0.12080 0.18560 0.181100 54 0.13890 0.20570 0.271200 305 0.09545 0.13610 0.072390 560 0.12410 0.22640 0.132600 285 0.10380 0.06624 0.005579 355 0.10960 0.20020 0.238800 329 0.14260 0.21160 0.334400 concave points_worst symmetry_worst fractal_dimension_worst diagnosis \\ 531 0.081200 0.3206 0.08950 0 166 0.046030 0.2090 0.07699 0 485 0.134200 0.3231 0.10340 0 66 0.065170 0.2878 0.09211 0 220 0.080560 0.2380 0.08718 0 356 0.125800 0.3113 0.08317 0 414 0.065750 0.3233 0.06165 1 525 0.085120 0.2983 0.10490 0 77 0.210200 0.3751 0.11080 1 239 0.206600 0.2853 0.08496 1 254 0.199900 0.3379 0.08950 1 447 0.083080 0.3600 0.07285 0 301 0.076250 0.2685 0.07764 0 133 0.137400 0.2723 0.07071 0 187 0.109900 0.2572 0.07097 0 78 0.250800 0.5440 0.09964 1 319 0.028320 0.1901 0.05932 0 412 0.025640 0.2376 0.09206 0 349 0.047660 0.3124 0.07590 0 11 0.181000 0.3792 0.10480 1 240 0.092220 0.2530 0.06510 0 29 0.145600 0.2756 0.07919 1 302 0.192300 0.3294 0.09469 1 521 0.247500 0.3157 0.09671 1 373 0.211200 0.2689 0.07055 1 481 0.081500 0.2356 0.07603 0 100 0.118400 0.2651 0.07397 1 304 0.055090 0.2208 0.07638 0 159 0.039530 0.2738 0.07685 0 360 0.016350 0.2233 0.05521 0 .. ... ... ... ... 202 0.273300 0.3198 0.08762 1 435 0.182700 0.3179 0.10550 1 375 0.125100 0.3153 0.08960 0 47 0.208800 0.3900 0.11790 1 497 0.105300 0.3035 0.07661 0 13 0.111900 0.2809 0.06287 1 221 0.090900 0.3065 0.08177 0 22 0.239300 0.4667 0.09946 1 255 0.137400 0.3068 0.07957 1 109 0.082780 0.2829 0.08832 0 348 0.065480 0.2851 0.08763 0 129 0.173200 0.3305 0.08465 1 152 0.157100 0.3108 0.12590 0 67 0.069610 0.2400 0.06641 0 213 0.109900 0.1603 0.06818 1 495 0.101700 0.2369 0.06599 0 517 0.161300 0.2549 0.09136 1 219 0.162500 0.2713 0.07568 1 290 0.102100 0.2272 0.08799 0 488 0.098150 0.2804 0.08024 0 309 0.035320 0.2107 0.06580 0 6 0.193200 0.3063 0.08368 1 405 0.078870 0.2251 0.07732 0 452 0.071160 0.2447 0.08194 0 54 0.153000 0.2675 0.07873 1 305 0.048150 0.3244 0.06745 0 560 0.104800 0.2250 0.08321 0 285 0.008772 0.2505 0.06431 0 355 0.092650 0.2121 0.07188 0 329 0.104700 0.2736 0.07953 1 prediksi 531 0 166 0 485 0 66 0 220 0 356 0 414 1 525 0 77 1 239 1 254 1 447 0 301 0 133 1 187 0 78 1 319 0 412 0 349 0 11 1 240 0 29 1 302 1 521 1 373 1 481 1 100 1 304 0 159 0 360 0 .. ... 202 1 435 0 375 0 47 1 497 0 13 0 221 0 22 1 255 1 109 0 348 0 129 1 152 0 67 0 213 1 495 0 517 1 219 1 290 0 488 0 309 0 6 1 405 0 452 0 54 1 305 0 560 0 285 0 355 0 329 1 [171 rows x 32 columns] Agar lebih enak dilihat kita sederhanakan dulu,kita hasila menampilkan kolom \"diagnosis\" dari data testing dan hasil prediksi \"prediction\" data1=pd.DataFrame({\"diagnosis\":test_y,\"hasil prediksi\":prediction}) print(data1) Output: diagnosis hasil prediksi 531 0 0 166 0 0 485 0 0 66 0 0 220 0 0 356 0 0 414 1 1 525 0 0 77 1 1 239 1 1 254 1 1 447 0 0 301 0 0 133 0 1 187 0 0 78 1 1 319 0 0 412 0 0 349 0 0 11 1 1 240 0 0 29 1 1 302 1 1 521 1 1 373 1 1 481 0 1 100 1 1 304 0 0 159 0 0 360 0 0 .. ... ... 202 1 1 435 1 0 375 0 0 47 1 1 497 0 0 13 1 0 221 0 0 22 1 1 255 1 1 109 0 0 348 0 0 129 1 1 152 0 0 67 0 0 213 1 1 495 0 0 517 1 1 219 1 1 290 0 0 488 0 0 309 0 0 6 1 1 405 0 0 452 0 0 54 1 1 305 0 0 560 0 0 285 0 0 355 0 0 329 1 1 [171 rows x 2 columns] Untuk program dan notebook lengkapnya bisa diunduh disini","title":"Menampilkan Hasil Prediksi Datatesting"},{"location":"knn/#referensi","text":"Siti Mutrofin, Abidatul Izzah, Arrie Kurniawardhani, Mukhamad Masrur. 2014. OPTIMASI TEKNIK KLASIFIKASI MODIFIED K NEAREST NEIGHBOR. JURNAL GAMMA. 10(1): 1-5 Mayu Shinohara. 2017. Hyper Parameters Tuning of DTree,RF,SVM,kNN di https://www.kaggle.com/mayu0116/hyper-parameters-tuning-of-dtree-rf-svm-knn informatika. 2017. Algoritma K-Nearest Neighbor (K-NN) di https://informatikalogi.com/algoritma-k-nn-k-nearest-neighbor/ Asep Maulana Ismail. 2018. Cara Kerja Algoritma k-Nearest Neighbor (k-NN) di https://medium.com/bee-solution-partners/cara-kerja-algoritma-k-nearest-neighbor-k-nn-389297de543e","title":"Referensi"},{"location":"knn1/","text":"Pengertian K-Nearest Neighbor \u00b6 Algoritma k-Nearest Neighbor adalah algoritma supervised learning dimana hasil dari instance yang baru diklasifikasikan berdasarkan mayoritas dari kategori k-tetangga terdekat. Tujuan dari algoritma ini adalah untuk mengklasifikasikan obyek baru berdasarkan atribut dan sample-sample dari training data. Algoritma k-Nearest Neighbor menggunakan Neighborhood Classification sebagai nilai prediksi dari nilai instance yang baru. Kelebihan dan Kekurangan K-Nearest Neighbor \u00b6 Kelebihan K-Nearest Neighbor \u00b6 Pelatihan sangat cepat Sederhana dan mudah dipelajari Tahan terhadap data pelatihan yang memiliki derau Efektif jika data pelatihan besar. Kekurangan K-Nearest Neighbor \u00b6 Nilai k bias Komputasi kompleks Keterbatasan memori, Mudah tertipu dengan atribut yang tidak relevan. Algoritma K-Nearest Neighbor \u00b6 Langkah-langkah K-Nearest Neighbor : \u00b6 Menentukan parameter K sebagai banyaknya jumlah tetangga terdekat dengan objek baru. Menghitung jarak antar objek/data baru terhadap semua objek/data yan gtelah di training. Urutkan hasil perhitungan tersebut. Tentukan tetangga terdekat berdasarkan jarak minimum ke K. Tentukan kategori dari tetangga terdekat dengan objek/data. Gunakan kategori mayoritas sebagai klasifikasi objek/data baru. Contoh Perhitungan KNN Sederhana \u00b6 Misalnya ada sebuah rumah yang berada tepat di tengah perbatasan antara Kota Bandung dan Kabupaten Bandung, sehingga pemerintah kesulitan untuk menentukan apakah rumah tersebut termasuk kedalam wilayah Kota Bandung atau Kabupaten Bandung . Kita bisa menentukannya dengan menggunakan Algoritma k-NN , yaitu dengan melibatkan jarak antara rumah tersebut dengan rumah-rumah yang ada disekitarnya (tetangganya). Pertama , kita harus menentukan jumlah tetangga yg akan kita perhitungkan (k), misalnya kita tentukan 3 tetangga terdekat ( k = 3 ). Kedua , hitung jarak setiap tetangga terhadap rumah tersebut, lalu urutkan hasilnya berdasarkan jarak, mulai dari yang terkecil ke yang terbesar. Ketiga , ambil 3 (k) tetangga yg paling dekat, lalu kita lihat masing-masing dari tetangga tersebut apakah termasuk kedalam wilayah Kota atau Kabupaten. Ada 2 kemungkinan: Bila dari 3 tetangga tersebut terdapat ada 2 rumah yg termasuk kedalam wilayah Kota Bandung, maka rumah tersebut termasuk kedalam wilayah Kota Bandung. Sebaliknya, bila dari 3 tetangga tersebut terdapat 2 rumah yg termasuk kedalam wilayah Kabupaten Bandung, maka rumah tersebut termasuk kedalam wilayah Kabupaten Bandung. Dalam menentukan nilai k* , bila* jumlah klasifikasi kita genap maka sebaiknya kita gunakan nilai k ganjil* , dan begitu pula sebaliknya bila* jumlah klasifikasi kita ganjil maka sebaiknya gunakan nilai k genap* , karena jika tidak begitu, ada kemungkinan kita* tidak akan mendapatkan jawaban* .* Pembahasan Lebih Detil \u00b6 Pada kasus diatas, kita menghitung jarak suatu rumah terhadap tetangga-tetangganya, itu berarti kita harus mengetahui posisi dari setiap rumah. Kita bisa menggunakan latitude dan longitude (atau garis lintang dan garis bujur) sebagai posisi. Untuk mempermudah pemahaman, saya akan coba menggunakan data yang nilainya sederhana. Data yang akan digunakan adalah sebagai berikut: Dari data diatas, kita mendapatkan beberapa informasi, diantaranya: Independent Variables , yaitu variable yang nilainya tidak dipengaruhi oleh variable lain. Pada contoh data diatas, yang termasuk independent variable adalah Lat , dan Long . Dependent Variables , yaitu variable yang nilainya dipengaruhi oleh variable lain. Pada contoh data diatas, yang termasuk *dependent variable*adalah Lokasi . Rumah A-E adalah rumah yang masuk ke dalam wilayah Kota . Rumah F-G adalah rumah yang masuk ke dalam wilayah Kabupaten . Rumah X adalah rumah yang akan kita prediksi menggunakan algoritma kNN apakah termasuk ke dalam wilayah Kota atau Kabupaten. Didalam dunia Machine Learning* ,* Independent Variables sering disebut juga sebagai Features Selanjutnya kita hitung jarak antara rumah X terhadap rumah A-G dengan menggunakan rumus Euclidean Distance : $$ \\begin{aligned} d(p, q)=d(q, p) &=\\sqrt{\\left(q_{1}-p_{1}\\right) {2}+\\left(q_{2}-p_{2}\\right) {2}+\\cdots+\\left(q_{n}-p_{n}\\right)^{2}} \\ &=\\sqrt{\\sum_{i=1} {n}\\left(q_{1}-p_{i}\\right) {2}} \\end{aligned} $$ Perhitungan jarak rumah X dengan rumah A: =\u221a(11-19)^2 + (26-25)^2 =\u221a(-8)^2 + (1)^2 =\u221a65 =8,062257748298550 setelah jarak rumah X dengan rumah A ditemukan tinggal menghitung jarak antara rumah X terhadap rumah B-G Setelah dihitung, selanjutnya adalah urutkan jarak tersebut dari yang paling kecil ke yang paling besar , hasilnya adalah sebagai berikut: Dapat dilihat dari hasil perhitungan diatas, bahwa ternyata 3 tetangga terdekat dari rumah X adalah: Rumah H (Kabupaten) yang memiliki jarak 2.24 , Rumah C (Kota) yang memiliki jarak 3 , dan Rumah E (Kota) yang memiliki jarak 3.16 . Dari ke-3 tetangga terdekat, terdapat 2 rumah yang termasuk kedalam wilayah Kota dan 1 rumah yang masuk ke dalam wilayah Kabupaten . Sehingga dapat disimpulkan, bahwa Rumah X adalah rumah yang termasuk kedalam wilayah Kota Implementasi K-Nearest Neighbor Breast Cancer Wisconsin (Diagnostic) Dataset \u00b6 Import Library yang dibutuhkan \u00b6 import numpy as np import numpy as np import pandas as pd import matplotlib.pyplot as plt from sklearn.neighbors import KNeighborsClassifier from sklearn.model_selection import train_test_split from sklearn import metrics #importing modul metrik Memuat Dataset \u00b6 Mengimport dataset yang digunakan untuk pengimplemtasian K-Nearest Neighbor dataset bisa didownload disini atau langsung dari kaggle disini # Memuat dataset data = pd.read_csv(\"E:\\Semester 4\\data mining/bc.csv\",encoding = \"ISO-8859-1\") Data Preprocessing \u00b6 Menampilkan 5 Data teratas \u00b6 Menampilkan jumlah masing-masing diagnosis dan kolom diagnosis digunakan sebagai class nantinya. data.head(5) Output: Menampilkan ringkasan dataset serta menghilangkan kolom yang tidak berguna \u00b6 # ringkasan dataset data.info() Output: <class 'pandas.core.frame.DataFrame'> RangeIndex: 569 entries, 0 to 568 Data columns (total 33 columns): id 569 non-null int64 diagnosis 569 non-null object radius_mean 569 non-null float64 texture_mean 569 non-null float64 perimeter_mean 569 non-null float64 area_mean 569 non-null float64 smoothness_mean 569 non-null float64 compactness_mean 569 non-null float64 concavity_mean 569 non-null float64 concave points_mean 569 non-null float64 symmetry_mean 569 non-null float64 fractal_dimension_mean 569 non-null float64 radius_se 569 non-null float64 texture_se 569 non-null float64 perimeter_se 569 non-null float64 area_se 569 non-null float64 smoothness_se 569 non-null float64 compactness_se 569 non-null float64 concavity_se 569 non-null float64 concave points_se 569 non-null float64 symmetry_se 569 non-null float64 fractal_dimension_se 569 non-null float64 radius_worst 569 non-null float64 texture_worst 569 non-null float64 perimeter_worst 569 non-null float64 area_worst 569 non-null float64 smoothness_worst 569 non-null float64 compactness_worst 569 non-null float64 concavity_worst 569 non-null float64 concave points_worst 569 non-null float64 symmetry_worst 569 non-null float64 fractal_dimension_worst 569 non-null float64 Unnamed: 32 0 non-null float64 dtypes: float64(31), int64(1), object(1) memory usage: 146.8+ KB bisa dilihat dari hasil output diatas ada kolom yang tidak bernama (\"Unnamed\") akan kita hilangkan, serta menghilangkan kolom yang tidak berguna yaitu kolom ()\"id\") : #menghapus kolom yang tidak berguna #menghapus kolom \"id\" data.drop(\"id\",axis=1,inplace=True) #menghapus the \"Unnamed: 32\" column data.drop(\"Unnamed: 32\",axis=1,inplace=True) #hasil data.info() Output: bisa dilihat kolom \"id\" dan kolom yang tidak mempunyai nama \"unnamed\" telah hilang <class 'pandas.core.frame.DataFrame'> RangeIndex: 569 entries, 0 to 568 Data columns (total 31 columns): diagnosis 569 non-null object radius_mean 569 non-null float64 texture_mean 569 non-null float64 perimeter_mean 569 non-null float64 area_mean 569 non-null float64 smoothness_mean 569 non-null float64 compactness_mean 569 non-null float64 concavity_mean 569 non-null float64 concave points_mean 569 non-null float64 symmetry_mean 569 non-null float64 fractal_dimension_mean 569 non-null float64 radius_se 569 non-null float64 texture_se 569 non-null float64 perimeter_se 569 non-null float64 area_se 569 non-null float64 smoothness_se 569 non-null float64 compactness_se 569 non-null float64 concavity_se 569 non-null float64 concave points_se 569 non-null float64 symmetry_se 569 non-null float64 fractal_dimension_se 569 non-null float64 radius_worst 569 non-null float64 texture_worst 569 non-null float64 perimeter_worst 569 non-null float64 area_worst 569 non-null float64 smoothness_worst 569 non-null float64 compactness_worst 569 non-null float64 concavity_worst 569 non-null float64 concave points_worst 569 non-null float64 symmetry_worst 569 non-null float64 fractal_dimension_worst 569 non-null float64 dtypes: float64(30), object(1) memory usage: 137.9+ KB Kemudian kita lihat lagi kolom beserta 5 data pertama: # 5 baris pertama data.head(5) Output: Mengganti class M dan B menjadi 0 dan 1 \u00b6 diagnosis adalah variabel yang bertanggung jawab untuk klasifikasi disini kita mengganti M dan B masing-masing dengan 1 dan 0 #diagnosis adalah variabel yang bertanggung jawab untuk klasifikasi #mengganti M dan B masing-masing dengan 1 dan 0 data.diagnosis=data.diagnosis.map({'M':1,'B':0}) Kemudian kita hitung berapa banyak jumlah masing- masing feature #menghitung variabel diagnosis data.diagnosis.value_counts() output: 0 357 1 212 Name: diagnosis, dtype: int64 Membagi data 30% sebagai data testing dan 70% sebagai data training \u00b6 # preprocessing dataset selesai #splitting dataset ke training dan testing train, test = train_test_split(data, test_size = 0.3,random_state=1234) #mencari hasil print(train.shape) print(test.shape) (398, 31) (171, 31) Membuat variabel independen dan responsible \u00b6 variabel independen dan responsible nantinya akan digunakan dalam proses prediksi variable independen mengambil dari semua kolom dan variable responsible dari diagnosis #membuat variabel independen untuk training train_X = train.iloc[:, 1:31] #membuat variabel responsible untuk training train_y=train.diagnosis #membuat variabel independen untuk testing test_X= test.iloc[:, 1:31] #membuat variabel responsible untuk ttesting test_y =test.diagnosis kita cek dulu berapa jumlahnya #mencari hasil print(train_X.shape) print(train_y.shape) print(test_X.shape) print(test_y.shape) output: (398, 30) (398,) (171, 30) (171,) Mencari K yang terbaik(ideal) \u00b6 K yang terbaik disini akan menghasilkan akurasi yang tinggi, Pada langkah ini saya memberikan range / jarak 1 sampai 30 untuk mencari K terbaik dan akurasi terbaik. Sehingga saya melakukan pengulangan 1 - 30 dan mencari nilai akurasi tertinggi. neighbors=np.arange(1,31) accuracy_train=[] accuracy_test=[] for i,k in enumerate(neighbors): knn=KNeighborsClassifier(n_neighbors=k) knn.fit(train_X,train_y) accuracy_train.append(knn.score(train_X,train_y)) accuracy_test.append(knn.score(test_X,test_y)) plt.figure(figsize=(13,8)) plt.plot(neighbors,accuracy_train,label=\"Akurasi training\") plt.plot(neighbors,accuracy_test,label=\"Akurasi testing\") acideal=np.max(accuracy_test) kideal=1+accuracy_test.index(np.max(accuracy_test)) plt.title('Tingkat Akurasi') plt.xlabel(\"K yang dipilih\") plt.ylabel(\"Tingkat Akurasi\") plt.xticks(neighbors) plt.legend() plt.show() print(\"Akurasi ideal:\",acideal) print(\"K ideal:\",kideal) Output: Akurasi ideal: 0.9415204678362573 K ideal: 7 Akurasi yang ideal 0.9415204678362573 pada k = 7 Menghitung KNN \u00b6 #membuat instance model = KNeighborsClassifier(n_neighbors=int(input(\"Masukan jumlah k:\"))) #learning model.fit(train_X,train_y) #Prediksi prediction=model.predict(test_X) #evaluation (Akurasi) print(\"Akurasi:\",metrics.accuracy_score(prediction,test_y)) #evaluation(Confusion Metrix) print(\"Confusion Metrix:\\n\",metrics.confusion_matrix(prediction,test_y)) Output: Masukan jumlah k:7 Akurasi: 0.9415204678362573 Confusion Metrix: [[102 7] [ 3 59]] Menampilkan Hasil Prediksi Datatesting \u00b6 Pada langkah ini kita menggabungkan data testing, diagnosis dan prediction . sehingga kita dapat melihat hasil prediksi dari semua data. datatest=pd.DataFrame(test_X) datatest['diagnosis']=test_y datatest['prediksi']=prediction print(datatest) Output: radius_mean texture_mean perimeter_mean area_mean smoothness_mean \\ 531 11.670 20.02 75.21 416.2 0.10160 166 10.800 9.71 68.77 357.6 0.09594 485 12.450 16.41 82.85 476.7 0.09514 66 9.465 21.01 60.11 269.4 0.10440 220 13.650 13.16 87.88 568.9 0.09646 356 13.050 18.59 85.09 512.0 0.10820 414 15.130 29.81 96.71 719.5 0.08320 525 8.571 13.10 54.53 221.3 0.10360 77 18.050 16.15 120.20 1006.0 0.10650 239 17.460 39.28 113.40 920.6 0.09812 254 19.450 19.33 126.50 1169.0 0.10350 447 14.800 17.66 95.88 674.8 0.09179 301 12.460 19.89 80.43 471.3 0.08451 133 15.710 13.93 102.00 761.7 0.09462 187 11.710 17.19 74.68 420.3 0.09774 78 20.180 23.97 143.70 1245.0 0.12860 319 12.430 17.00 78.60 477.3 0.07557 412 9.397 21.68 59.75 268.8 0.07969 349 11.950 14.96 77.23 426.7 0.11580 11 15.780 17.89 103.60 781.0 0.09710 240 13.640 15.60 87.38 575.3 0.09423 29 17.570 15.05 115.00 955.1 0.09847 302 20.090 23.86 134.70 1247.0 0.10800 521 24.630 21.60 165.50 1841.0 0.10300 373 20.640 17.35 134.80 1335.0 0.09446 481 13.900 19.24 88.73 602.9 0.07991 100 13.610 24.98 88.05 582.7 0.09488 304 11.460 18.16 73.59 403.1 0.08853 159 10.900 12.96 68.69 366.8 0.07515 360 12.540 18.07 79.42 491.9 0.07436 .. ... ... ... ... ... 202 23.290 26.67 158.90 1685.0 0.11410 435 13.980 19.62 91.12 599.5 0.10600 375 16.170 16.07 106.30 788.5 0.09880 47 13.170 18.66 85.98 534.6 0.11580 497 12.470 17.31 80.45 480.1 0.08928 13 15.850 23.95 103.70 782.7 0.08401 221 13.560 13.90 88.59 561.3 0.10510 22 15.340 14.26 102.50 704.4 0.10730 255 13.960 17.05 91.43 602.4 0.10960 109 11.340 21.26 72.48 396.5 0.08759 348 11.470 16.03 73.02 402.7 0.09076 129 19.790 25.12 130.40 1192.0 0.10150 152 9.731 15.34 63.78 300.2 0.10720 67 11.310 19.04 71.80 394.1 0.08139 213 17.420 25.56 114.50 948.0 0.10060 495 14.870 20.21 96.12 680.9 0.09587 517 19.890 20.26 130.50 1214.0 0.10370 219 19.530 32.47 128.00 1223.0 0.08420 290 14.410 19.73 96.03 651.0 0.08757 488 11.680 16.17 75.49 420.5 0.11280 309 13.050 13.84 82.71 530.6 0.08352 6 18.250 19.98 119.60 1040.0 0.09463 405 10.940 18.59 70.39 370.0 0.10040 452 12.000 28.23 76.77 442.5 0.08437 54 15.100 22.02 97.26 712.8 0.09056 305 11.600 24.49 74.23 417.2 0.07474 560 14.050 27.15 91.38 600.4 0.09929 285 12.580 18.40 79.83 489.0 0.08393 355 12.560 19.07 81.92 485.8 0.08760 329 16.260 21.88 107.50 826.8 0.11650 compactness_mean concavity_mean concave points_mean symmetry_mean \\ 531 0.09453 0.042000 0.021570 0.1859 166 0.05736 0.025310 0.016980 0.1381 485 0.15110 0.154400 0.048460 0.2082 66 0.07773 0.021720 0.015040 0.1717 220 0.08711 0.038880 0.025630 0.1360 356 0.13040 0.096030 0.056030 0.2035 414 0.04605 0.046860 0.027390 0.1852 525 0.07632 0.025650 0.015100 0.1678 77 0.21460 0.168400 0.108000 0.2152 239 0.12980 0.141700 0.088110 0.1809 254 0.11880 0.137900 0.085910 0.1776 447 0.08890 0.040690 0.022600 0.1893 301 0.10140 0.068300 0.030990 0.1781 133 0.09462 0.071350 0.059330 0.1816 187 0.06141 0.038090 0.032390 0.1516 78 0.34540 0.375400 0.160400 0.2906 319 0.03454 0.013420 0.016990 0.1472 412 0.06053 0.037350 0.005128 0.1274 349 0.12060 0.011710 0.017870 0.2459 11 0.12920 0.099540 0.066060 0.1842 240 0.06630 0.047050 0.037310 0.1717 29 0.11570 0.098750 0.079530 0.1739 302 0.18380 0.228300 0.128000 0.2249 521 0.21060 0.231000 0.147100 0.1991 373 0.10760 0.152700 0.089410 0.1571 481 0.05326 0.029950 0.020700 0.1579 100 0.08511 0.086250 0.044890 0.1609 304 0.07694 0.033440 0.015020 0.1411 159 0.03718 0.003090 0.006588 0.1442 360 0.02650 0.001194 0.005449 0.1528 .. ... ... ... ... 202 0.20840 0.352300 0.162000 0.2200 435 0.11330 0.112600 0.064630 0.1669 375 0.14380 0.066510 0.053970 0.1990 47 0.12310 0.122600 0.073400 0.2128 497 0.07630 0.036090 0.023690 0.1526 13 0.10020 0.099380 0.053640 0.1847 221 0.11920 0.078600 0.044510 0.1962 22 0.21350 0.207700 0.097560 0.2521 255 0.12790 0.097890 0.052460 0.1908 109 0.06575 0.051330 0.018990 0.1487 348 0.05886 0.025870 0.023220 0.1634 129 0.15890 0.254500 0.114900 0.2202 152 0.15990 0.410800 0.078570 0.2548 67 0.04701 0.037090 0.022300 0.1516 213 0.11460 0.168200 0.065970 0.1308 495 0.08345 0.068240 0.049510 0.1487 517 0.13100 0.141100 0.094310 0.1802 219 0.11300 0.114500 0.066370 0.1428 290 0.16760 0.136200 0.066020 0.1714 488 0.09263 0.042790 0.031320 0.1853 309 0.03735 0.004559 0.008829 0.1453 6 0.10900 0.112700 0.074000 0.1794 405 0.07460 0.049440 0.029320 0.1486 452 0.06450 0.040550 0.019450 0.1615 54 0.07081 0.052530 0.033340 0.1616 305 0.05688 0.019740 0.013130 0.1935 560 0.11260 0.044620 0.043040 0.1537 285 0.04216 0.001860 0.002924 0.1697 355 0.10380 0.103000 0.043910 0.1533 329 0.12830 0.179900 0.079810 0.1869 fractal_dimension_mean ... perimeter_worst area_worst \\ 531 0.06461 ... 87.00 550.6 166 0.06400 ... 73.66 414.0 485 0.07325 ... 97.82 580.6 66 0.06899 ... 67.03 330.7 220 0.06344 ... 99.71 706.2 356 0.06501 ... 94.22 591.2 414 0.05294 ... 110.10 931.4 525 0.07126 ... 63.30 275.6 77 0.06673 ... 150.10 1610.0 239 0.05966 ... 141.20 1408.0 254 0.05647 ... 163.10 1972.0 447 0.05886 ... 105.90 829.5 301 0.06249 ... 88.13 551.3 133 0.05723 ... 114.30 922.8 187 0.06095 ... 84.42 521.5 78 0.08142 ... 170.30 1623.0 319 0.05561 ... 81.76 515.9 412 0.06724 ... 66.61 301.0 349 0.06581 ... 83.09 496.2 11 0.06082 ... 136.50 1299.0 240 0.05660 ... 94.11 683.4 29 0.06149 ... 134.90 1227.0 302 0.07469 ... 158.80 1696.0 521 0.06739 ... 205.70 2642.0 373 0.05478 ... 166.80 1946.0 481 0.05594 ... 104.40 830.5 100 0.05871 ... 108.60 906.5 304 0.06243 ... 82.69 489.8 159 0.05743 ... 78.07 470.0 360 0.05185 ... 86.82 585.7 .. ... ... ... ... 202 0.06229 ... 177.00 1986.0 435 0.06544 ... 113.90 869.3 375 0.06572 ... 113.10 861.5 47 0.06777 ... 102.80 759.4 497 0.06046 ... 92.82 607.3 13 0.05338 ... 112.00 876.5 221 0.06303 ... 101.10 686.6 22 0.07032 ... 125.10 980.9 255 0.06130 ... 108.10 826.0 109 0.06529 ... 83.99 518.1 348 0.06372 ... 79.67 475.8 129 0.06113 ... 148.70 1589.0 152 0.09296 ... 71.04 380.5 67 0.05667 ... 78.00 466.7 213 0.05866 ... 120.40 1021.0 495 0.05748 ... 103.90 783.6 517 0.06188 ... 160.50 1646.0 219 0.05313 ... 180.20 2477.0 290 0.07192 ... 101.70 767.3 488 0.06401 ... 86.57 549.8 309 0.05518 ... 93.96 672.4 6 0.05742 ... 153.20 1606.0 405 0.06615 ... 82.76 472.4 452 0.06104 ... 85.07 523.7 54 0.05684 ... 117.70 1030.0 305 0.05878 ... 81.39 476.5 560 0.06171 ... 100.20 706.7 285 0.05855 ... 85.56 564.1 355 0.06184 ... 89.02 547.4 329 0.06532 ... 113.70 975.2 smoothness_worst compactness_worst concavity_worst \\ 531 0.15500 0.29640 0.275800 166 0.14360 0.12570 0.104700 485 0.11750 0.40610 0.489600 66 0.15480 0.16640 0.094120 220 0.13110 0.24740 0.175900 356 0.13430 0.26580 0.257300 414 0.11480 0.09866 0.154700 525 0.16410 0.22350 0.175400 77 0.14780 0.56340 0.378600 239 0.13650 0.37350 0.324100 254 0.14970 0.31610 0.431700 447 0.12260 0.18810 0.206000 301 0.10500 0.21580 0.190400 133 0.12230 0.19490 0.170900 187 0.13230 0.10400 0.152100 78 0.16390 0.61640 0.768100 319 0.08409 0.04712 0.022370 412 0.10860 0.18870 0.186800 349 0.12930 0.18850 0.031220 11 0.13960 0.56090 0.396500 240 0.12780 0.12910 0.153300 29 0.12550 0.28120 0.248900 302 0.13470 0.33910 0.493200 521 0.13420 0.41880 0.465800 373 0.15620 0.30550 0.415900 481 0.10640 0.14150 0.167300 100 0.12650 0.19430 0.316900 304 0.11440 0.17890 0.122600 159 0.11710 0.08294 0.018540 360 0.09293 0.04327 0.003581 .. ... ... ... 202 0.15360 0.41670 0.789200 435 0.16130 0.35680 0.406900 375 0.12350 0.25500 0.211400 47 0.17860 0.41660 0.500600 497 0.12760 0.25060 0.202800 13 0.11310 0.19240 0.232200 221 0.13760 0.26980 0.257700 22 0.13900 0.59540 0.630500 255 0.15120 0.32620 0.320900 109 0.16990 0.21960 0.312000 348 0.15310 0.11200 0.098230 129 0.12750 0.38610 0.567300 152 0.12920 0.27720 0.821600 67 0.12900 0.09148 0.144400 213 0.12430 0.17930 0.280300 495 0.12160 0.13880 0.170000 517 0.14170 0.33090 0.418500 219 0.14080 0.40970 0.399500 290 0.09983 0.24720 0.222000 488 0.15260 0.14770 0.149000 309 0.10160 0.05847 0.018240 6 0.14420 0.25760 0.378400 405 0.13630 0.16440 0.141200 452 0.12080 0.18560 0.181100 54 0.13890 0.20570 0.271200 305 0.09545 0.13610 0.072390 560 0.12410 0.22640 0.132600 285 0.10380 0.06624 0.005579 355 0.10960 0.20020 0.238800 329 0.14260 0.21160 0.334400 concave points_worst symmetry_worst fractal_dimension_worst diagnosis \\ 531 0.081200 0.3206 0.08950 0 166 0.046030 0.2090 0.07699 0 485 0.134200 0.3231 0.10340 0 66 0.065170 0.2878 0.09211 0 220 0.080560 0.2380 0.08718 0 356 0.125800 0.3113 0.08317 0 414 0.065750 0.3233 0.06165 1 525 0.085120 0.2983 0.10490 0 77 0.210200 0.3751 0.11080 1 239 0.206600 0.2853 0.08496 1 254 0.199900 0.3379 0.08950 1 447 0.083080 0.3600 0.07285 0 301 0.076250 0.2685 0.07764 0 133 0.137400 0.2723 0.07071 0 187 0.109900 0.2572 0.07097 0 78 0.250800 0.5440 0.09964 1 319 0.028320 0.1901 0.05932 0 412 0.025640 0.2376 0.09206 0 349 0.047660 0.3124 0.07590 0 11 0.181000 0.3792 0.10480 1 240 0.092220 0.2530 0.06510 0 29 0.145600 0.2756 0.07919 1 302 0.192300 0.3294 0.09469 1 521 0.247500 0.3157 0.09671 1 373 0.211200 0.2689 0.07055 1 481 0.081500 0.2356 0.07603 0 100 0.118400 0.2651 0.07397 1 304 0.055090 0.2208 0.07638 0 159 0.039530 0.2738 0.07685 0 360 0.016350 0.2233 0.05521 0 .. ... ... ... ... 202 0.273300 0.3198 0.08762 1 435 0.182700 0.3179 0.10550 1 375 0.125100 0.3153 0.08960 0 47 0.208800 0.3900 0.11790 1 497 0.105300 0.3035 0.07661 0 13 0.111900 0.2809 0.06287 1 221 0.090900 0.3065 0.08177 0 22 0.239300 0.4667 0.09946 1 255 0.137400 0.3068 0.07957 1 109 0.082780 0.2829 0.08832 0 348 0.065480 0.2851 0.08763 0 129 0.173200 0.3305 0.08465 1 152 0.157100 0.3108 0.12590 0 67 0.069610 0.2400 0.06641 0 213 0.109900 0.1603 0.06818 1 495 0.101700 0.2369 0.06599 0 517 0.161300 0.2549 0.09136 1 219 0.162500 0.2713 0.07568 1 290 0.102100 0.2272 0.08799 0 488 0.098150 0.2804 0.08024 0 309 0.035320 0.2107 0.06580 0 6 0.193200 0.3063 0.08368 1 405 0.078870 0.2251 0.07732 0 452 0.071160 0.2447 0.08194 0 54 0.153000 0.2675 0.07873 1 305 0.048150 0.3244 0.06745 0 560 0.104800 0.2250 0.08321 0 285 0.008772 0.2505 0.06431 0 355 0.092650 0.2121 0.07188 0 329 0.104700 0.2736 0.07953 1 prediksi 531 0 166 0 485 0 66 0 220 0 356 0 414 1 525 0 77 1 239 1 254 1 447 0 301 0 133 1 187 0 78 1 319 0 412 0 349 0 11 1 240 0 29 1 302 1 521 1 373 1 481 1 100 1 304 0 159 0 360 0 .. ... 202 1 435 0 375 0 47 1 497 0 13 0 221 0 22 1 255 1 109 0 348 0 129 1 152 0 67 0 213 1 495 0 517 1 219 1 290 0 488 0 309 0 6 1 405 0 452 0 54 1 305 0 560 0 285 0 355 0 329 1 [171 rows x 32 columns] Agar lebih enak dilihat kita sederhanakan dulu,kita hasila menampilkan kolom \"diagnosis\" dari data testing dan hasil prediksi \"prediction\" data1=pd.DataFrame({\"diagnosis\":test_y,\"hasil prediksi\":prediction}) print(data1) Output: diagnosis hasil prediksi 531 0 0 166 0 0 485 0 0 66 0 0 220 0 0 356 0 0 414 1 1 525 0 0 77 1 1 239 1 1 254 1 1 447 0 0 301 0 0 133 0 1 187 0 0 78 1 1 319 0 0 412 0 0 349 0 0 11 1 1 240 0 0 29 1 1 302 1 1 521 1 1 373 1 1 481 0 1 100 1 1 304 0 0 159 0 0 360 0 0 .. ... ... 202 1 1 435 1 0 375 0 0 47 1 1 497 0 0 13 1 0 221 0 0 22 1 1 255 1 1 109 0 0 348 0 0 129 1 1 152 0 0 67 0 0 213 1 1 495 0 0 517 1 1 219 1 1 290 0 0 488 0 0 309 0 0 6 1 1 405 0 0 452 0 0 54 1 1 305 0 0 560 0 0 285 0 0 355 0 0 329 1 1 [171 rows x 2 columns] Untuk program dan notebook lengkapnya bisa diunduh disini Referensi \u00b6 Siti Mutrofin, Abidatul Izzah, Arrie Kurniawardhani, Mukhamad Masrur. 2014. OPTIMASI TEKNIK KLASIFIKASI MODIFIED K NEAREST NEIGHBOR. JURNAL GAMMA. 10(1): 1-5 Mayu Shinohara. 2017. Hyper Parameters Tuning of DTree,RF,SVM,kNN di https://www.kaggle.com/mayu0116/hyper-parameters-tuning-of-dtree-rf-svm-knn informatika. 2017. Algoritma K-Nearest Neighbor (K-NN) di https://informatikalogi.com/algoritma-k-nn-k-nearest-neighbor/ Asep Maulana Ismail. 2018. Cara Kerja Algoritma k-Nearest Neighbor (k-NN) di https://medium.com/bee-solution-partners/cara-kerja-algoritma-k-nearest-neighbor-k-nn-389297de543e","title":"Pengertian K-Nearest Neighbor"},{"location":"knn1/#pengertian-k-nearest-neighbor","text":"Algoritma k-Nearest Neighbor adalah algoritma supervised learning dimana hasil dari instance yang baru diklasifikasikan berdasarkan mayoritas dari kategori k-tetangga terdekat. Tujuan dari algoritma ini adalah untuk mengklasifikasikan obyek baru berdasarkan atribut dan sample-sample dari training data. Algoritma k-Nearest Neighbor menggunakan Neighborhood Classification sebagai nilai prediksi dari nilai instance yang baru.","title":"Pengertian K-Nearest Neighbor"},{"location":"knn1/#kelebihan-dan-kekurangan-k-nearest-neighbor","text":"","title":"Kelebihan dan Kekurangan K-Nearest Neighbor"},{"location":"knn1/#kelebihan-k-nearest-neighbor","text":"Pelatihan sangat cepat Sederhana dan mudah dipelajari Tahan terhadap data pelatihan yang memiliki derau Efektif jika data pelatihan besar.","title":"Kelebihan K-Nearest Neighbor"},{"location":"knn1/#kekurangan-k-nearest-neighbor","text":"Nilai k bias Komputasi kompleks Keterbatasan memori, Mudah tertipu dengan atribut yang tidak relevan.","title":"Kekurangan K-Nearest Neighbor"},{"location":"knn1/#algoritma-k-nearest-neighbor","text":"","title":"Algoritma K-Nearest Neighbor"},{"location":"knn1/#langkah-langkah-k-nearest-neighbor","text":"Menentukan parameter K sebagai banyaknya jumlah tetangga terdekat dengan objek baru. Menghitung jarak antar objek/data baru terhadap semua objek/data yan gtelah di training. Urutkan hasil perhitungan tersebut. Tentukan tetangga terdekat berdasarkan jarak minimum ke K. Tentukan kategori dari tetangga terdekat dengan objek/data. Gunakan kategori mayoritas sebagai klasifikasi objek/data baru.","title":"Langkah-langkah K-Nearest Neighbor  :"},{"location":"knn1/#contoh-perhitungan-knn-sederhana","text":"Misalnya ada sebuah rumah yang berada tepat di tengah perbatasan antara Kota Bandung dan Kabupaten Bandung, sehingga pemerintah kesulitan untuk menentukan apakah rumah tersebut termasuk kedalam wilayah Kota Bandung atau Kabupaten Bandung . Kita bisa menentukannya dengan menggunakan Algoritma k-NN , yaitu dengan melibatkan jarak antara rumah tersebut dengan rumah-rumah yang ada disekitarnya (tetangganya). Pertama , kita harus menentukan jumlah tetangga yg akan kita perhitungkan (k), misalnya kita tentukan 3 tetangga terdekat ( k = 3 ). Kedua , hitung jarak setiap tetangga terhadap rumah tersebut, lalu urutkan hasilnya berdasarkan jarak, mulai dari yang terkecil ke yang terbesar. Ketiga , ambil 3 (k) tetangga yg paling dekat, lalu kita lihat masing-masing dari tetangga tersebut apakah termasuk kedalam wilayah Kota atau Kabupaten. Ada 2 kemungkinan: Bila dari 3 tetangga tersebut terdapat ada 2 rumah yg termasuk kedalam wilayah Kota Bandung, maka rumah tersebut termasuk kedalam wilayah Kota Bandung. Sebaliknya, bila dari 3 tetangga tersebut terdapat 2 rumah yg termasuk kedalam wilayah Kabupaten Bandung, maka rumah tersebut termasuk kedalam wilayah Kabupaten Bandung. Dalam menentukan nilai k* , bila* jumlah klasifikasi kita genap maka sebaiknya kita gunakan nilai k ganjil* , dan begitu pula sebaliknya bila* jumlah klasifikasi kita ganjil maka sebaiknya gunakan nilai k genap* , karena jika tidak begitu, ada kemungkinan kita* tidak akan mendapatkan jawaban* .*","title":"Contoh Perhitungan KNN Sederhana"},{"location":"knn1/#pembahasan-lebih-detil","text":"Pada kasus diatas, kita menghitung jarak suatu rumah terhadap tetangga-tetangganya, itu berarti kita harus mengetahui posisi dari setiap rumah. Kita bisa menggunakan latitude dan longitude (atau garis lintang dan garis bujur) sebagai posisi. Untuk mempermudah pemahaman, saya akan coba menggunakan data yang nilainya sederhana. Data yang akan digunakan adalah sebagai berikut: Dari data diatas, kita mendapatkan beberapa informasi, diantaranya: Independent Variables , yaitu variable yang nilainya tidak dipengaruhi oleh variable lain. Pada contoh data diatas, yang termasuk independent variable adalah Lat , dan Long . Dependent Variables , yaitu variable yang nilainya dipengaruhi oleh variable lain. Pada contoh data diatas, yang termasuk *dependent variable*adalah Lokasi . Rumah A-E adalah rumah yang masuk ke dalam wilayah Kota . Rumah F-G adalah rumah yang masuk ke dalam wilayah Kabupaten . Rumah X adalah rumah yang akan kita prediksi menggunakan algoritma kNN apakah termasuk ke dalam wilayah Kota atau Kabupaten. Didalam dunia Machine Learning* ,* Independent Variables sering disebut juga sebagai Features Selanjutnya kita hitung jarak antara rumah X terhadap rumah A-G dengan menggunakan rumus Euclidean Distance : $$ \\begin{aligned} d(p, q)=d(q, p) &=\\sqrt{\\left(q_{1}-p_{1}\\right) {2}+\\left(q_{2}-p_{2}\\right) {2}+\\cdots+\\left(q_{n}-p_{n}\\right)^{2}} \\ &=\\sqrt{\\sum_{i=1} {n}\\left(q_{1}-p_{i}\\right) {2}} \\end{aligned} $$ Perhitungan jarak rumah X dengan rumah A: =\u221a(11-19)^2 + (26-25)^2 =\u221a(-8)^2 + (1)^2 =\u221a65 =8,062257748298550 setelah jarak rumah X dengan rumah A ditemukan tinggal menghitung jarak antara rumah X terhadap rumah B-G Setelah dihitung, selanjutnya adalah urutkan jarak tersebut dari yang paling kecil ke yang paling besar , hasilnya adalah sebagai berikut: Dapat dilihat dari hasil perhitungan diatas, bahwa ternyata 3 tetangga terdekat dari rumah X adalah: Rumah H (Kabupaten) yang memiliki jarak 2.24 , Rumah C (Kota) yang memiliki jarak 3 , dan Rumah E (Kota) yang memiliki jarak 3.16 . Dari ke-3 tetangga terdekat, terdapat 2 rumah yang termasuk kedalam wilayah Kota dan 1 rumah yang masuk ke dalam wilayah Kabupaten . Sehingga dapat disimpulkan, bahwa Rumah X adalah rumah yang termasuk kedalam wilayah Kota","title":"Pembahasan Lebih Detil"},{"location":"knn1/#implementasi-k-nearest-neighbor-breast-cancer-wisconsin-diagnostic-dataset","text":"","title":"Implementasi K-Nearest Neighbor Breast Cancer Wisconsin (Diagnostic) Dataset"},{"location":"knn1/#import-library-yang-dibutuhkan","text":"import numpy as np import numpy as np import pandas as pd import matplotlib.pyplot as plt from sklearn.neighbors import KNeighborsClassifier from sklearn.model_selection import train_test_split from sklearn import metrics #importing modul metrik","title":"Import Library yang dibutuhkan"},{"location":"knn1/#memuat-dataset","text":"Mengimport dataset yang digunakan untuk pengimplemtasian K-Nearest Neighbor dataset bisa didownload disini atau langsung dari kaggle disini # Memuat dataset data = pd.read_csv(\"E:\\Semester 4\\data mining/bc.csv\",encoding = \"ISO-8859-1\")","title":"Memuat Dataset"},{"location":"knn1/#data-preprocessing","text":"","title":"Data Preprocessing"},{"location":"knn1/#menampilkan-5-data-teratas","text":"Menampilkan jumlah masing-masing diagnosis dan kolom diagnosis digunakan sebagai class nantinya. data.head(5) Output:","title":"Menampilkan 5 Data teratas"},{"location":"knn1/#menampilkan-ringkasan-dataset-serta-menghilangkan-kolom-yang-tidak-berguna","text":"# ringkasan dataset data.info() Output: <class 'pandas.core.frame.DataFrame'> RangeIndex: 569 entries, 0 to 568 Data columns (total 33 columns): id 569 non-null int64 diagnosis 569 non-null object radius_mean 569 non-null float64 texture_mean 569 non-null float64 perimeter_mean 569 non-null float64 area_mean 569 non-null float64 smoothness_mean 569 non-null float64 compactness_mean 569 non-null float64 concavity_mean 569 non-null float64 concave points_mean 569 non-null float64 symmetry_mean 569 non-null float64 fractal_dimension_mean 569 non-null float64 radius_se 569 non-null float64 texture_se 569 non-null float64 perimeter_se 569 non-null float64 area_se 569 non-null float64 smoothness_se 569 non-null float64 compactness_se 569 non-null float64 concavity_se 569 non-null float64 concave points_se 569 non-null float64 symmetry_se 569 non-null float64 fractal_dimension_se 569 non-null float64 radius_worst 569 non-null float64 texture_worst 569 non-null float64 perimeter_worst 569 non-null float64 area_worst 569 non-null float64 smoothness_worst 569 non-null float64 compactness_worst 569 non-null float64 concavity_worst 569 non-null float64 concave points_worst 569 non-null float64 symmetry_worst 569 non-null float64 fractal_dimension_worst 569 non-null float64 Unnamed: 32 0 non-null float64 dtypes: float64(31), int64(1), object(1) memory usage: 146.8+ KB bisa dilihat dari hasil output diatas ada kolom yang tidak bernama (\"Unnamed\") akan kita hilangkan, serta menghilangkan kolom yang tidak berguna yaitu kolom ()\"id\") : #menghapus kolom yang tidak berguna #menghapus kolom \"id\" data.drop(\"id\",axis=1,inplace=True) #menghapus the \"Unnamed: 32\" column data.drop(\"Unnamed: 32\",axis=1,inplace=True) #hasil data.info() Output: bisa dilihat kolom \"id\" dan kolom yang tidak mempunyai nama \"unnamed\" telah hilang <class 'pandas.core.frame.DataFrame'> RangeIndex: 569 entries, 0 to 568 Data columns (total 31 columns): diagnosis 569 non-null object radius_mean 569 non-null float64 texture_mean 569 non-null float64 perimeter_mean 569 non-null float64 area_mean 569 non-null float64 smoothness_mean 569 non-null float64 compactness_mean 569 non-null float64 concavity_mean 569 non-null float64 concave points_mean 569 non-null float64 symmetry_mean 569 non-null float64 fractal_dimension_mean 569 non-null float64 radius_se 569 non-null float64 texture_se 569 non-null float64 perimeter_se 569 non-null float64 area_se 569 non-null float64 smoothness_se 569 non-null float64 compactness_se 569 non-null float64 concavity_se 569 non-null float64 concave points_se 569 non-null float64 symmetry_se 569 non-null float64 fractal_dimension_se 569 non-null float64 radius_worst 569 non-null float64 texture_worst 569 non-null float64 perimeter_worst 569 non-null float64 area_worst 569 non-null float64 smoothness_worst 569 non-null float64 compactness_worst 569 non-null float64 concavity_worst 569 non-null float64 concave points_worst 569 non-null float64 symmetry_worst 569 non-null float64 fractal_dimension_worst 569 non-null float64 dtypes: float64(30), object(1) memory usage: 137.9+ KB Kemudian kita lihat lagi kolom beserta 5 data pertama: # 5 baris pertama data.head(5) Output:","title":"Menampilkan ringkasan dataset serta menghilangkan kolom yang tidak berguna"},{"location":"knn1/#mengganti-class-m-dan-b-menjadi-0-dan-1","text":"diagnosis adalah variabel yang bertanggung jawab untuk klasifikasi disini kita mengganti M dan B masing-masing dengan 1 dan 0 #diagnosis adalah variabel yang bertanggung jawab untuk klasifikasi #mengganti M dan B masing-masing dengan 1 dan 0 data.diagnosis=data.diagnosis.map({'M':1,'B':0}) Kemudian kita hitung berapa banyak jumlah masing- masing feature #menghitung variabel diagnosis data.diagnosis.value_counts() output: 0 357 1 212 Name: diagnosis, dtype: int64","title":"Mengganti class M dan B menjadi 0 dan 1"},{"location":"knn1/#membagi-data-30-sebagai-data-testing-dan-70-sebagai-data-training","text":"# preprocessing dataset selesai #splitting dataset ke training dan testing train, test = train_test_split(data, test_size = 0.3,random_state=1234) #mencari hasil print(train.shape) print(test.shape) (398, 31) (171, 31)","title":"Membagi data 30% sebagai data testing dan 70% sebagai data training"},{"location":"knn1/#membuat-variabel-independen-dan-responsible","text":"variabel independen dan responsible nantinya akan digunakan dalam proses prediksi variable independen mengambil dari semua kolom dan variable responsible dari diagnosis #membuat variabel independen untuk training train_X = train.iloc[:, 1:31] #membuat variabel responsible untuk training train_y=train.diagnosis #membuat variabel independen untuk testing test_X= test.iloc[:, 1:31] #membuat variabel responsible untuk ttesting test_y =test.diagnosis kita cek dulu berapa jumlahnya #mencari hasil print(train_X.shape) print(train_y.shape) print(test_X.shape) print(test_y.shape) output: (398, 30) (398,) (171, 30) (171,)","title":"Membuat variabel independen dan responsible"},{"location":"knn1/#mencari-k-yang-terbaikideal","text":"K yang terbaik disini akan menghasilkan akurasi yang tinggi, Pada langkah ini saya memberikan range / jarak 1 sampai 30 untuk mencari K terbaik dan akurasi terbaik. Sehingga saya melakukan pengulangan 1 - 30 dan mencari nilai akurasi tertinggi. neighbors=np.arange(1,31) accuracy_train=[] accuracy_test=[] for i,k in enumerate(neighbors): knn=KNeighborsClassifier(n_neighbors=k) knn.fit(train_X,train_y) accuracy_train.append(knn.score(train_X,train_y)) accuracy_test.append(knn.score(test_X,test_y)) plt.figure(figsize=(13,8)) plt.plot(neighbors,accuracy_train,label=\"Akurasi training\") plt.plot(neighbors,accuracy_test,label=\"Akurasi testing\") acideal=np.max(accuracy_test) kideal=1+accuracy_test.index(np.max(accuracy_test)) plt.title('Tingkat Akurasi') plt.xlabel(\"K yang dipilih\") plt.ylabel(\"Tingkat Akurasi\") plt.xticks(neighbors) plt.legend() plt.show() print(\"Akurasi ideal:\",acideal) print(\"K ideal:\",kideal) Output: Akurasi ideal: 0.9415204678362573 K ideal: 7 Akurasi yang ideal 0.9415204678362573 pada k = 7","title":"Mencari K yang terbaik(ideal)"},{"location":"knn1/#menghitung-knn","text":"#membuat instance model = KNeighborsClassifier(n_neighbors=int(input(\"Masukan jumlah k:\"))) #learning model.fit(train_X,train_y) #Prediksi prediction=model.predict(test_X) #evaluation (Akurasi) print(\"Akurasi:\",metrics.accuracy_score(prediction,test_y)) #evaluation(Confusion Metrix) print(\"Confusion Metrix:\\n\",metrics.confusion_matrix(prediction,test_y)) Output: Masukan jumlah k:7 Akurasi: 0.9415204678362573 Confusion Metrix: [[102 7] [ 3 59]]","title":"Menghitung KNN"},{"location":"knn1/#menampilkan-hasil-prediksi-datatesting","text":"Pada langkah ini kita menggabungkan data testing, diagnosis dan prediction . sehingga kita dapat melihat hasil prediksi dari semua data. datatest=pd.DataFrame(test_X) datatest['diagnosis']=test_y datatest['prediksi']=prediction print(datatest) Output: radius_mean texture_mean perimeter_mean area_mean smoothness_mean \\ 531 11.670 20.02 75.21 416.2 0.10160 166 10.800 9.71 68.77 357.6 0.09594 485 12.450 16.41 82.85 476.7 0.09514 66 9.465 21.01 60.11 269.4 0.10440 220 13.650 13.16 87.88 568.9 0.09646 356 13.050 18.59 85.09 512.0 0.10820 414 15.130 29.81 96.71 719.5 0.08320 525 8.571 13.10 54.53 221.3 0.10360 77 18.050 16.15 120.20 1006.0 0.10650 239 17.460 39.28 113.40 920.6 0.09812 254 19.450 19.33 126.50 1169.0 0.10350 447 14.800 17.66 95.88 674.8 0.09179 301 12.460 19.89 80.43 471.3 0.08451 133 15.710 13.93 102.00 761.7 0.09462 187 11.710 17.19 74.68 420.3 0.09774 78 20.180 23.97 143.70 1245.0 0.12860 319 12.430 17.00 78.60 477.3 0.07557 412 9.397 21.68 59.75 268.8 0.07969 349 11.950 14.96 77.23 426.7 0.11580 11 15.780 17.89 103.60 781.0 0.09710 240 13.640 15.60 87.38 575.3 0.09423 29 17.570 15.05 115.00 955.1 0.09847 302 20.090 23.86 134.70 1247.0 0.10800 521 24.630 21.60 165.50 1841.0 0.10300 373 20.640 17.35 134.80 1335.0 0.09446 481 13.900 19.24 88.73 602.9 0.07991 100 13.610 24.98 88.05 582.7 0.09488 304 11.460 18.16 73.59 403.1 0.08853 159 10.900 12.96 68.69 366.8 0.07515 360 12.540 18.07 79.42 491.9 0.07436 .. ... ... ... ... ... 202 23.290 26.67 158.90 1685.0 0.11410 435 13.980 19.62 91.12 599.5 0.10600 375 16.170 16.07 106.30 788.5 0.09880 47 13.170 18.66 85.98 534.6 0.11580 497 12.470 17.31 80.45 480.1 0.08928 13 15.850 23.95 103.70 782.7 0.08401 221 13.560 13.90 88.59 561.3 0.10510 22 15.340 14.26 102.50 704.4 0.10730 255 13.960 17.05 91.43 602.4 0.10960 109 11.340 21.26 72.48 396.5 0.08759 348 11.470 16.03 73.02 402.7 0.09076 129 19.790 25.12 130.40 1192.0 0.10150 152 9.731 15.34 63.78 300.2 0.10720 67 11.310 19.04 71.80 394.1 0.08139 213 17.420 25.56 114.50 948.0 0.10060 495 14.870 20.21 96.12 680.9 0.09587 517 19.890 20.26 130.50 1214.0 0.10370 219 19.530 32.47 128.00 1223.0 0.08420 290 14.410 19.73 96.03 651.0 0.08757 488 11.680 16.17 75.49 420.5 0.11280 309 13.050 13.84 82.71 530.6 0.08352 6 18.250 19.98 119.60 1040.0 0.09463 405 10.940 18.59 70.39 370.0 0.10040 452 12.000 28.23 76.77 442.5 0.08437 54 15.100 22.02 97.26 712.8 0.09056 305 11.600 24.49 74.23 417.2 0.07474 560 14.050 27.15 91.38 600.4 0.09929 285 12.580 18.40 79.83 489.0 0.08393 355 12.560 19.07 81.92 485.8 0.08760 329 16.260 21.88 107.50 826.8 0.11650 compactness_mean concavity_mean concave points_mean symmetry_mean \\ 531 0.09453 0.042000 0.021570 0.1859 166 0.05736 0.025310 0.016980 0.1381 485 0.15110 0.154400 0.048460 0.2082 66 0.07773 0.021720 0.015040 0.1717 220 0.08711 0.038880 0.025630 0.1360 356 0.13040 0.096030 0.056030 0.2035 414 0.04605 0.046860 0.027390 0.1852 525 0.07632 0.025650 0.015100 0.1678 77 0.21460 0.168400 0.108000 0.2152 239 0.12980 0.141700 0.088110 0.1809 254 0.11880 0.137900 0.085910 0.1776 447 0.08890 0.040690 0.022600 0.1893 301 0.10140 0.068300 0.030990 0.1781 133 0.09462 0.071350 0.059330 0.1816 187 0.06141 0.038090 0.032390 0.1516 78 0.34540 0.375400 0.160400 0.2906 319 0.03454 0.013420 0.016990 0.1472 412 0.06053 0.037350 0.005128 0.1274 349 0.12060 0.011710 0.017870 0.2459 11 0.12920 0.099540 0.066060 0.1842 240 0.06630 0.047050 0.037310 0.1717 29 0.11570 0.098750 0.079530 0.1739 302 0.18380 0.228300 0.128000 0.2249 521 0.21060 0.231000 0.147100 0.1991 373 0.10760 0.152700 0.089410 0.1571 481 0.05326 0.029950 0.020700 0.1579 100 0.08511 0.086250 0.044890 0.1609 304 0.07694 0.033440 0.015020 0.1411 159 0.03718 0.003090 0.006588 0.1442 360 0.02650 0.001194 0.005449 0.1528 .. ... ... ... ... 202 0.20840 0.352300 0.162000 0.2200 435 0.11330 0.112600 0.064630 0.1669 375 0.14380 0.066510 0.053970 0.1990 47 0.12310 0.122600 0.073400 0.2128 497 0.07630 0.036090 0.023690 0.1526 13 0.10020 0.099380 0.053640 0.1847 221 0.11920 0.078600 0.044510 0.1962 22 0.21350 0.207700 0.097560 0.2521 255 0.12790 0.097890 0.052460 0.1908 109 0.06575 0.051330 0.018990 0.1487 348 0.05886 0.025870 0.023220 0.1634 129 0.15890 0.254500 0.114900 0.2202 152 0.15990 0.410800 0.078570 0.2548 67 0.04701 0.037090 0.022300 0.1516 213 0.11460 0.168200 0.065970 0.1308 495 0.08345 0.068240 0.049510 0.1487 517 0.13100 0.141100 0.094310 0.1802 219 0.11300 0.114500 0.066370 0.1428 290 0.16760 0.136200 0.066020 0.1714 488 0.09263 0.042790 0.031320 0.1853 309 0.03735 0.004559 0.008829 0.1453 6 0.10900 0.112700 0.074000 0.1794 405 0.07460 0.049440 0.029320 0.1486 452 0.06450 0.040550 0.019450 0.1615 54 0.07081 0.052530 0.033340 0.1616 305 0.05688 0.019740 0.013130 0.1935 560 0.11260 0.044620 0.043040 0.1537 285 0.04216 0.001860 0.002924 0.1697 355 0.10380 0.103000 0.043910 0.1533 329 0.12830 0.179900 0.079810 0.1869 fractal_dimension_mean ... perimeter_worst area_worst \\ 531 0.06461 ... 87.00 550.6 166 0.06400 ... 73.66 414.0 485 0.07325 ... 97.82 580.6 66 0.06899 ... 67.03 330.7 220 0.06344 ... 99.71 706.2 356 0.06501 ... 94.22 591.2 414 0.05294 ... 110.10 931.4 525 0.07126 ... 63.30 275.6 77 0.06673 ... 150.10 1610.0 239 0.05966 ... 141.20 1408.0 254 0.05647 ... 163.10 1972.0 447 0.05886 ... 105.90 829.5 301 0.06249 ... 88.13 551.3 133 0.05723 ... 114.30 922.8 187 0.06095 ... 84.42 521.5 78 0.08142 ... 170.30 1623.0 319 0.05561 ... 81.76 515.9 412 0.06724 ... 66.61 301.0 349 0.06581 ... 83.09 496.2 11 0.06082 ... 136.50 1299.0 240 0.05660 ... 94.11 683.4 29 0.06149 ... 134.90 1227.0 302 0.07469 ... 158.80 1696.0 521 0.06739 ... 205.70 2642.0 373 0.05478 ... 166.80 1946.0 481 0.05594 ... 104.40 830.5 100 0.05871 ... 108.60 906.5 304 0.06243 ... 82.69 489.8 159 0.05743 ... 78.07 470.0 360 0.05185 ... 86.82 585.7 .. ... ... ... ... 202 0.06229 ... 177.00 1986.0 435 0.06544 ... 113.90 869.3 375 0.06572 ... 113.10 861.5 47 0.06777 ... 102.80 759.4 497 0.06046 ... 92.82 607.3 13 0.05338 ... 112.00 876.5 221 0.06303 ... 101.10 686.6 22 0.07032 ... 125.10 980.9 255 0.06130 ... 108.10 826.0 109 0.06529 ... 83.99 518.1 348 0.06372 ... 79.67 475.8 129 0.06113 ... 148.70 1589.0 152 0.09296 ... 71.04 380.5 67 0.05667 ... 78.00 466.7 213 0.05866 ... 120.40 1021.0 495 0.05748 ... 103.90 783.6 517 0.06188 ... 160.50 1646.0 219 0.05313 ... 180.20 2477.0 290 0.07192 ... 101.70 767.3 488 0.06401 ... 86.57 549.8 309 0.05518 ... 93.96 672.4 6 0.05742 ... 153.20 1606.0 405 0.06615 ... 82.76 472.4 452 0.06104 ... 85.07 523.7 54 0.05684 ... 117.70 1030.0 305 0.05878 ... 81.39 476.5 560 0.06171 ... 100.20 706.7 285 0.05855 ... 85.56 564.1 355 0.06184 ... 89.02 547.4 329 0.06532 ... 113.70 975.2 smoothness_worst compactness_worst concavity_worst \\ 531 0.15500 0.29640 0.275800 166 0.14360 0.12570 0.104700 485 0.11750 0.40610 0.489600 66 0.15480 0.16640 0.094120 220 0.13110 0.24740 0.175900 356 0.13430 0.26580 0.257300 414 0.11480 0.09866 0.154700 525 0.16410 0.22350 0.175400 77 0.14780 0.56340 0.378600 239 0.13650 0.37350 0.324100 254 0.14970 0.31610 0.431700 447 0.12260 0.18810 0.206000 301 0.10500 0.21580 0.190400 133 0.12230 0.19490 0.170900 187 0.13230 0.10400 0.152100 78 0.16390 0.61640 0.768100 319 0.08409 0.04712 0.022370 412 0.10860 0.18870 0.186800 349 0.12930 0.18850 0.031220 11 0.13960 0.56090 0.396500 240 0.12780 0.12910 0.153300 29 0.12550 0.28120 0.248900 302 0.13470 0.33910 0.493200 521 0.13420 0.41880 0.465800 373 0.15620 0.30550 0.415900 481 0.10640 0.14150 0.167300 100 0.12650 0.19430 0.316900 304 0.11440 0.17890 0.122600 159 0.11710 0.08294 0.018540 360 0.09293 0.04327 0.003581 .. ... ... ... 202 0.15360 0.41670 0.789200 435 0.16130 0.35680 0.406900 375 0.12350 0.25500 0.211400 47 0.17860 0.41660 0.500600 497 0.12760 0.25060 0.202800 13 0.11310 0.19240 0.232200 221 0.13760 0.26980 0.257700 22 0.13900 0.59540 0.630500 255 0.15120 0.32620 0.320900 109 0.16990 0.21960 0.312000 348 0.15310 0.11200 0.098230 129 0.12750 0.38610 0.567300 152 0.12920 0.27720 0.821600 67 0.12900 0.09148 0.144400 213 0.12430 0.17930 0.280300 495 0.12160 0.13880 0.170000 517 0.14170 0.33090 0.418500 219 0.14080 0.40970 0.399500 290 0.09983 0.24720 0.222000 488 0.15260 0.14770 0.149000 309 0.10160 0.05847 0.018240 6 0.14420 0.25760 0.378400 405 0.13630 0.16440 0.141200 452 0.12080 0.18560 0.181100 54 0.13890 0.20570 0.271200 305 0.09545 0.13610 0.072390 560 0.12410 0.22640 0.132600 285 0.10380 0.06624 0.005579 355 0.10960 0.20020 0.238800 329 0.14260 0.21160 0.334400 concave points_worst symmetry_worst fractal_dimension_worst diagnosis \\ 531 0.081200 0.3206 0.08950 0 166 0.046030 0.2090 0.07699 0 485 0.134200 0.3231 0.10340 0 66 0.065170 0.2878 0.09211 0 220 0.080560 0.2380 0.08718 0 356 0.125800 0.3113 0.08317 0 414 0.065750 0.3233 0.06165 1 525 0.085120 0.2983 0.10490 0 77 0.210200 0.3751 0.11080 1 239 0.206600 0.2853 0.08496 1 254 0.199900 0.3379 0.08950 1 447 0.083080 0.3600 0.07285 0 301 0.076250 0.2685 0.07764 0 133 0.137400 0.2723 0.07071 0 187 0.109900 0.2572 0.07097 0 78 0.250800 0.5440 0.09964 1 319 0.028320 0.1901 0.05932 0 412 0.025640 0.2376 0.09206 0 349 0.047660 0.3124 0.07590 0 11 0.181000 0.3792 0.10480 1 240 0.092220 0.2530 0.06510 0 29 0.145600 0.2756 0.07919 1 302 0.192300 0.3294 0.09469 1 521 0.247500 0.3157 0.09671 1 373 0.211200 0.2689 0.07055 1 481 0.081500 0.2356 0.07603 0 100 0.118400 0.2651 0.07397 1 304 0.055090 0.2208 0.07638 0 159 0.039530 0.2738 0.07685 0 360 0.016350 0.2233 0.05521 0 .. ... ... ... ... 202 0.273300 0.3198 0.08762 1 435 0.182700 0.3179 0.10550 1 375 0.125100 0.3153 0.08960 0 47 0.208800 0.3900 0.11790 1 497 0.105300 0.3035 0.07661 0 13 0.111900 0.2809 0.06287 1 221 0.090900 0.3065 0.08177 0 22 0.239300 0.4667 0.09946 1 255 0.137400 0.3068 0.07957 1 109 0.082780 0.2829 0.08832 0 348 0.065480 0.2851 0.08763 0 129 0.173200 0.3305 0.08465 1 152 0.157100 0.3108 0.12590 0 67 0.069610 0.2400 0.06641 0 213 0.109900 0.1603 0.06818 1 495 0.101700 0.2369 0.06599 0 517 0.161300 0.2549 0.09136 1 219 0.162500 0.2713 0.07568 1 290 0.102100 0.2272 0.08799 0 488 0.098150 0.2804 0.08024 0 309 0.035320 0.2107 0.06580 0 6 0.193200 0.3063 0.08368 1 405 0.078870 0.2251 0.07732 0 452 0.071160 0.2447 0.08194 0 54 0.153000 0.2675 0.07873 1 305 0.048150 0.3244 0.06745 0 560 0.104800 0.2250 0.08321 0 285 0.008772 0.2505 0.06431 0 355 0.092650 0.2121 0.07188 0 329 0.104700 0.2736 0.07953 1 prediksi 531 0 166 0 485 0 66 0 220 0 356 0 414 1 525 0 77 1 239 1 254 1 447 0 301 0 133 1 187 0 78 1 319 0 412 0 349 0 11 1 240 0 29 1 302 1 521 1 373 1 481 1 100 1 304 0 159 0 360 0 .. ... 202 1 435 0 375 0 47 1 497 0 13 0 221 0 22 1 255 1 109 0 348 0 129 1 152 0 67 0 213 1 495 0 517 1 219 1 290 0 488 0 309 0 6 1 405 0 452 0 54 1 305 0 560 0 285 0 355 0 329 1 [171 rows x 32 columns] Agar lebih enak dilihat kita sederhanakan dulu,kita hasila menampilkan kolom \"diagnosis\" dari data testing dan hasil prediksi \"prediction\" data1=pd.DataFrame({\"diagnosis\":test_y,\"hasil prediksi\":prediction}) print(data1) Output: diagnosis hasil prediksi 531 0 0 166 0 0 485 0 0 66 0 0 220 0 0 356 0 0 414 1 1 525 0 0 77 1 1 239 1 1 254 1 1 447 0 0 301 0 0 133 0 1 187 0 0 78 1 1 319 0 0 412 0 0 349 0 0 11 1 1 240 0 0 29 1 1 302 1 1 521 1 1 373 1 1 481 0 1 100 1 1 304 0 0 159 0 0 360 0 0 .. ... ... 202 1 1 435 1 0 375 0 0 47 1 1 497 0 0 13 1 0 221 0 0 22 1 1 255 1 1 109 0 0 348 0 0 129 1 1 152 0 0 67 0 0 213 1 1 495 0 0 517 1 1 219 1 1 290 0 0 488 0 0 309 0 0 6 1 1 405 0 0 452 0 0 54 1 1 305 0 0 560 0 0 285 0 0 355 0 0 329 1 1 [171 rows x 2 columns] Untuk program dan notebook lengkapnya bisa diunduh disini","title":"Menampilkan Hasil Prediksi Datatesting"},{"location":"knn1/#referensi","text":"Siti Mutrofin, Abidatul Izzah, Arrie Kurniawardhani, Mukhamad Masrur. 2014. OPTIMASI TEKNIK KLASIFIKASI MODIFIED K NEAREST NEIGHBOR. JURNAL GAMMA. 10(1): 1-5 Mayu Shinohara. 2017. Hyper Parameters Tuning of DTree,RF,SVM,kNN di https://www.kaggle.com/mayu0116/hyper-parameters-tuning-of-dtree-rf-svm-knn informatika. 2017. Algoritma K-Nearest Neighbor (K-NN) di https://informatikalogi.com/algoritma-k-nn-k-nearest-neighbor/ Asep Maulana Ismail. 2018. Cara Kerja Algoritma k-Nearest Neighbor (k-NN) di https://medium.com/bee-solution-partners/cara-kerja-algoritma-k-nearest-neighbor-k-nn-389297de543e","title":"Referensi"},{"location":"extensions/admonition/","text":"Admonition \u00b6 Admonition is an extension included in the standard Markdown library that makes it possible to add block-styled side content to your documentation, for example summaries, notes, hints or warnings. Installation \u00b6 Add the following lines to your mkdocs.yml : markdown_extensions: - admonition Usage \u00b6 Admonition blocks follow a simple syntax: every block is started with !!! , followed by a single keyword which is used as the type qualifier of the block. The content of the block then follows on the next line, indented by four spaces. Example: !!! note Lorem ipsum dolor sit amet, consectetur adipiscing elit. Nulla et euismod nulla. Curabitur feugiat, tortor non consequat finibus, justo purus auctor massa, nec semper lorem quam in massa. Result: Note Lorem ipsum dolor sit amet, consectetur adipiscing elit. Nulla et euismod nulla. Curabitur feugiat, tortor non consequat finibus, justo purus auctor massa, nec semper lorem quam in massa. Changing the title \u00b6 By default, the block title will equal the type qualifier in titlecase. However, it can easily be changed by adding a quoted string after the type qualifier. Example: !!! note \"Phasellus posuere in sem ut cursus\" Lorem ipsum dolor sit amet, consectetur adipiscing elit. Nulla et euismod nulla. Curabitur feugiat, tortor non consequat finibus, justo purus auctor massa, nec semper lorem quam in massa. Result: Phasellus posuere in sem ut cursus Lorem ipsum dolor sit amet, consectetur adipiscing elit. Nulla et euismod nulla. Curabitur feugiat, tortor non consequat finibus, justo purus auctor massa, nec semper lorem quam in massa. Removing the title \u00b6 Similar to setting a custom title , the icon and title can be omitted by providing an empty string after the type qualifier: Example: !!! note \"\" Lorem ipsum dolor sit amet, consectetur adipiscing elit. Nulla et euismod nulla. Curabitur feugiat, tortor non consequat finibus, justo purus auctor massa, nec semper lorem quam in massa. Result: Lorem ipsum dolor sit amet, consectetur adipiscing elit. Nulla et euismod nulla. Curabitur feugiat, tortor non consequat finibus, justo purus auctor massa, nec semper lorem quam in massa. Embedded code blocks \u00b6 Blocks can contain all kinds of text content, including headlines, lists, paragraphs and other blocks \u2013 except code blocks, because the parser from the standard Markdown library does not account for those. However, the PyMdown Extensions package adds an extension called SuperFences , which makes it possible to nest code blocks within other blocks, respectively Admonition blocks. Example: Note Lorem ipsum dolor sit amet, consectetur adipiscing elit. Nulla et euismod nulla. Curabitur feugiat, tortor non consequat finibus, justo purus auctor massa, nec semper lorem quam in massa. SELECT Employees.EmployeeID, Employees.Name, Employees.Salary, Manager.Name AS Manager FROM Employees LEFT JOIN Employees AS Manager ON Employees.ManagerID = Manager.EmployeeID WHERE Employees.EmployeeID = '087652'; Nunc eu odio eleifend, blandit leo a, volutpat sapien. Phasellus posuere in sem ut cursus. Nullam sit amet tincidunt ipsum, sit amet elementum turpis. Etiam ipsum quam, mattis in purus vitae, lacinia fermentum enim. Collapsible blocks \u00b6 The Details extension which is also part of the PyMdown Extensions package adds support for rendering collapsible Admonition blocks. This is useful for FAQs or content that is of secondary nature. Example: ??? note \"Phasellus posuere in sem ut cursus\" Lorem ipsum dolor sit amet, consectetur adipiscing elit. Nulla et euismod nulla. Curabitur feugiat, tortor non consequat finibus, justo purus auctor massa, nec semper lorem quam in massa. Result: Phasellus posuere in sem ut cursus Lorem ipsum dolor sit amet, consectetur adipiscing elit. Nulla et euismod nulla. Curabitur feugiat, tortor non consequat finibus, justo purus auctor massa, nec semper lorem quam in massa. By adding a + sign directly after the start marker, blocks can be rendered open by default. Types \u00b6 Admonition supports user-defined type qualifiers which may influence the style of the inserted block. Following is a list of type qualifiers provided by the Material theme, whereas the default type, and thus fallback for unknown type qualifiers, is note . Note \u00b6 Example: !!! note Lorem ipsum dolor sit amet, consectetur adipiscing elit. Nulla et euismod nulla. Curabitur feugiat, tortor non consequat finibus, justo purus auctor massa, nec semper lorem quam in massa. Result: Note Lorem ipsum dolor sit amet, consectetur adipiscing elit. Nulla et euismod nulla. Curabitur feugiat, tortor non consequat finibus, justo purus auctor massa, nec semper lorem quam in massa. Qualifiers: note seealso Abstract \u00b6 Example: !!! abstract Lorem ipsum dolor sit amet, consectetur adipiscing elit. Nulla et euismod nulla. Curabitur feugiat, tortor non consequat finibus, justo purus auctor massa, nec semper lorem quam in massa. Result: Abstract Lorem ipsum dolor sit amet, consectetur adipiscing elit. Nulla et euismod nulla. Curabitur feugiat, tortor non consequat finibus, justo purus auctor massa, nec semper lorem quam in massa. Qualifiers: abstract summary tldr Info \u00b6 Example: !!! info Lorem ipsum dolor sit amet, consectetur adipiscing elit. Nulla et euismod nulla. Curabitur feugiat, tortor non consequat finibus, justo purus auctor massa, nec semper lorem quam in massa. Result: Info Lorem ipsum dolor sit amet, consectetur adipiscing elit. Nulla et euismod nulla. Curabitur feugiat, tortor non consequat finibus, justo purus auctor massa, nec semper lorem quam in massa. Qualifiers: info todo Tip \u00b6 Example: !!! tip Lorem ipsum dolor sit amet, consectetur adipiscing elit. Nulla et euismod nulla. Curabitur feugiat, tortor non consequat finibus, justo purus auctor massa, nec semper lorem quam in massa. Result: Tip Lorem ipsum dolor sit amet, consectetur adipiscing elit. Nulla et euismod nulla. Curabitur feugiat, tortor non consequat finibus, justo purus auctor massa, nec semper lorem quam in massa. Qualifiers: tip hint important Success \u00b6 Example: !!! success Lorem ipsum dolor sit amet, consectetur adipiscing elit. Nulla et euismod nulla. Curabitur feugiat, tortor non consequat finibus, justo purus auctor massa, nec semper lorem quam in massa. Result: Success Lorem ipsum dolor sit amet, consectetur adipiscing elit. Nulla et euismod nulla. Curabitur feugiat, tortor non consequat finibus, justo purus auctor massa, nec semper lorem quam in massa. Qualifiers: success check done Question \u00b6 Example: !!! question Lorem ipsum dolor sit amet, consectetur adipiscing elit. Nulla et euismod nulla. Curabitur feugiat, tortor non consequat finibus, justo purus auctor massa, nec semper lorem quam in massa. Result: Question Lorem ipsum dolor sit amet, consectetur adipiscing elit. Nulla et euismod nulla. Curabitur feugiat, tortor non consequat finibus, justo purus auctor massa, nec semper lorem quam in massa. Qualifiers: question help faq Warning \u00b6 Example: !!! warning Lorem ipsum dolor sit amet, consectetur adipiscing elit. Nulla et euismod nulla. Curabitur feugiat, tortor non consequat finibus, justo purus auctor massa, nec semper lorem quam in massa. Result: Warning Lorem ipsum dolor sit amet, consectetur adipiscing elit. Nulla et euismod nulla. Curabitur feugiat, tortor non consequat finibus, justo purus auctor massa, nec semper lorem quam in massa. Qualifiers: warning caution attention Failure \u00b6 Example: !!! failure Lorem ipsum dolor sit amet, consectetur adipiscing elit. Nulla et euismod nulla. Curabitur feugiat, tortor non consequat finibus, justo purus auctor massa, nec semper lorem quam in massa. Result: Failure Lorem ipsum dolor sit amet, consectetur adipiscing elit. Nulla et euismod nulla. Curabitur feugiat, tortor non consequat finibus, justo purus auctor massa, nec semper lorem quam in massa. Qualifiers: failure fail missing Danger \u00b6 Example: !!! danger Lorem ipsum dolor sit amet, consectetur adipiscing elit. Nulla et euismod nulla. Curabitur feugiat, tortor non consequat finibus, justo purus auctor massa, nec semper lorem quam in massa. Result: Danger Lorem ipsum dolor sit amet, consectetur adipiscing elit. Nulla et euismod nulla. Curabitur feugiat, tortor non consequat finibus, justo purus auctor massa, nec semper lorem quam in massa. Qualifiers: danger error Bug \u00b6 Example: !!! bug Lorem ipsum dolor sit amet, consectetur adipiscing elit. Nulla et euismod nulla. Curabitur feugiat, tortor non consequat finibus, justo purus auctor massa, nec semper lorem quam in massa. Result: Bug Lorem ipsum dolor sit amet, consectetur adipiscing elit. Nulla et euismod nulla. Curabitur feugiat, tortor non consequat finibus, justo purus auctor massa, nec semper lorem quam in massa. Qualifiers: bug Example \u00b6 Example: !!! example Lorem ipsum dolor sit amet, consectetur adipiscing elit. Nulla et euismod nulla. Curabitur feugiat, tortor non consequat finibus, justo purus auctor massa, nec semper lorem quam in massa. Result: Example Lorem ipsum dolor sit amet, consectetur adipiscing elit. Nulla et euismod nulla. Curabitur feugiat, tortor non consequat finibus, justo purus auctor massa, nec semper lorem quam in massa. Qualifiers: example snippet Quote \u00b6 Example: !!! quote Lorem ipsum dolor sit amet, consectetur adipiscing elit. Nulla et euismod nulla. Curabitur feugiat, tortor non consequat finibus, justo purus auctor massa, nec semper lorem quam in massa. Result: Quote Lorem ipsum dolor sit amet, consectetur adipiscing elit. Nulla et euismod nulla. Curabitur feugiat, tortor non consequat finibus, justo purus auctor massa, nec semper lorem quam in massa. Qualifiers: quote cite","title":"Admonition"},{"location":"extensions/admonition/#admonition","text":"Admonition is an extension included in the standard Markdown library that makes it possible to add block-styled side content to your documentation, for example summaries, notes, hints or warnings.","title":"Admonition"},{"location":"extensions/admonition/#installation","text":"Add the following lines to your mkdocs.yml : markdown_extensions: - admonition","title":"Installation"},{"location":"extensions/admonition/#usage","text":"Admonition blocks follow a simple syntax: every block is started with !!! , followed by a single keyword which is used as the type qualifier of the block. The content of the block then follows on the next line, indented by four spaces. Example: !!! note Lorem ipsum dolor sit amet, consectetur adipiscing elit. Nulla et euismod nulla. Curabitur feugiat, tortor non consequat finibus, justo purus auctor massa, nec semper lorem quam in massa. Result: Note Lorem ipsum dolor sit amet, consectetur adipiscing elit. Nulla et euismod nulla. Curabitur feugiat, tortor non consequat finibus, justo purus auctor massa, nec semper lorem quam in massa.","title":"Usage"},{"location":"extensions/admonition/#changing-the-title","text":"By default, the block title will equal the type qualifier in titlecase. However, it can easily be changed by adding a quoted string after the type qualifier. Example: !!! note \"Phasellus posuere in sem ut cursus\" Lorem ipsum dolor sit amet, consectetur adipiscing elit. Nulla et euismod nulla. Curabitur feugiat, tortor non consequat finibus, justo purus auctor massa, nec semper lorem quam in massa. Result: Phasellus posuere in sem ut cursus Lorem ipsum dolor sit amet, consectetur adipiscing elit. Nulla et euismod nulla. Curabitur feugiat, tortor non consequat finibus, justo purus auctor massa, nec semper lorem quam in massa.","title":"Changing the title"},{"location":"extensions/admonition/#removing-the-title","text":"Similar to setting a custom title , the icon and title can be omitted by providing an empty string after the type qualifier: Example: !!! note \"\" Lorem ipsum dolor sit amet, consectetur adipiscing elit. Nulla et euismod nulla. Curabitur feugiat, tortor non consequat finibus, justo purus auctor massa, nec semper lorem quam in massa. Result: Lorem ipsum dolor sit amet, consectetur adipiscing elit. Nulla et euismod nulla. Curabitur feugiat, tortor non consequat finibus, justo purus auctor massa, nec semper lorem quam in massa.","title":"Removing the title"},{"location":"extensions/admonition/#embedded-code-blocks","text":"Blocks can contain all kinds of text content, including headlines, lists, paragraphs and other blocks \u2013 except code blocks, because the parser from the standard Markdown library does not account for those. However, the PyMdown Extensions package adds an extension called SuperFences , which makes it possible to nest code blocks within other blocks, respectively Admonition blocks. Example: Note Lorem ipsum dolor sit amet, consectetur adipiscing elit. Nulla et euismod nulla. Curabitur feugiat, tortor non consequat finibus, justo purus auctor massa, nec semper lorem quam in massa. SELECT Employees.EmployeeID, Employees.Name, Employees.Salary, Manager.Name AS Manager FROM Employees LEFT JOIN Employees AS Manager ON Employees.ManagerID = Manager.EmployeeID WHERE Employees.EmployeeID = '087652'; Nunc eu odio eleifend, blandit leo a, volutpat sapien. Phasellus posuere in sem ut cursus. Nullam sit amet tincidunt ipsum, sit amet elementum turpis. Etiam ipsum quam, mattis in purus vitae, lacinia fermentum enim.","title":"Embedded code blocks"},{"location":"extensions/admonition/#collapsible-blocks","text":"The Details extension which is also part of the PyMdown Extensions package adds support for rendering collapsible Admonition blocks. This is useful for FAQs or content that is of secondary nature. Example: ??? note \"Phasellus posuere in sem ut cursus\" Lorem ipsum dolor sit amet, consectetur adipiscing elit. Nulla et euismod nulla. Curabitur feugiat, tortor non consequat finibus, justo purus auctor massa, nec semper lorem quam in massa. Result: Phasellus posuere in sem ut cursus Lorem ipsum dolor sit amet, consectetur adipiscing elit. Nulla et euismod nulla. Curabitur feugiat, tortor non consequat finibus, justo purus auctor massa, nec semper lorem quam in massa. By adding a + sign directly after the start marker, blocks can be rendered open by default.","title":"Collapsible blocks"},{"location":"extensions/admonition/#types","text":"Admonition supports user-defined type qualifiers which may influence the style of the inserted block. Following is a list of type qualifiers provided by the Material theme, whereas the default type, and thus fallback for unknown type qualifiers, is note .","title":"Types"},{"location":"extensions/admonition/#note","text":"Example: !!! note Lorem ipsum dolor sit amet, consectetur adipiscing elit. Nulla et euismod nulla. Curabitur feugiat, tortor non consequat finibus, justo purus auctor massa, nec semper lorem quam in massa. Result: Note Lorem ipsum dolor sit amet, consectetur adipiscing elit. Nulla et euismod nulla. Curabitur feugiat, tortor non consequat finibus, justo purus auctor massa, nec semper lorem quam in massa. Qualifiers: note seealso","title":"Note"},{"location":"extensions/admonition/#abstract","text":"Example: !!! abstract Lorem ipsum dolor sit amet, consectetur adipiscing elit. Nulla et euismod nulla. Curabitur feugiat, tortor non consequat finibus, justo purus auctor massa, nec semper lorem quam in massa. Result: Abstract Lorem ipsum dolor sit amet, consectetur adipiscing elit. Nulla et euismod nulla. Curabitur feugiat, tortor non consequat finibus, justo purus auctor massa, nec semper lorem quam in massa. Qualifiers: abstract summary tldr","title":"Abstract"},{"location":"extensions/admonition/#info","text":"Example: !!! info Lorem ipsum dolor sit amet, consectetur adipiscing elit. Nulla et euismod nulla. Curabitur feugiat, tortor non consequat finibus, justo purus auctor massa, nec semper lorem quam in massa. Result: Info Lorem ipsum dolor sit amet, consectetur adipiscing elit. Nulla et euismod nulla. Curabitur feugiat, tortor non consequat finibus, justo purus auctor massa, nec semper lorem quam in massa. Qualifiers: info todo","title":"Info"},{"location":"extensions/admonition/#tip","text":"Example: !!! tip Lorem ipsum dolor sit amet, consectetur adipiscing elit. Nulla et euismod nulla. Curabitur feugiat, tortor non consequat finibus, justo purus auctor massa, nec semper lorem quam in massa. Result: Tip Lorem ipsum dolor sit amet, consectetur adipiscing elit. Nulla et euismod nulla. Curabitur feugiat, tortor non consequat finibus, justo purus auctor massa, nec semper lorem quam in massa. Qualifiers: tip hint important","title":"Tip"},{"location":"extensions/admonition/#success","text":"Example: !!! success Lorem ipsum dolor sit amet, consectetur adipiscing elit. Nulla et euismod nulla. Curabitur feugiat, tortor non consequat finibus, justo purus auctor massa, nec semper lorem quam in massa. Result: Success Lorem ipsum dolor sit amet, consectetur adipiscing elit. Nulla et euismod nulla. Curabitur feugiat, tortor non consequat finibus, justo purus auctor massa, nec semper lorem quam in massa. Qualifiers: success check done","title":"Success"},{"location":"extensions/admonition/#question","text":"Example: !!! question Lorem ipsum dolor sit amet, consectetur adipiscing elit. Nulla et euismod nulla. Curabitur feugiat, tortor non consequat finibus, justo purus auctor massa, nec semper lorem quam in massa. Result: Question Lorem ipsum dolor sit amet, consectetur adipiscing elit. Nulla et euismod nulla. Curabitur feugiat, tortor non consequat finibus, justo purus auctor massa, nec semper lorem quam in massa. Qualifiers: question help faq","title":"Question"},{"location":"extensions/admonition/#warning","text":"Example: !!! warning Lorem ipsum dolor sit amet, consectetur adipiscing elit. Nulla et euismod nulla. Curabitur feugiat, tortor non consequat finibus, justo purus auctor massa, nec semper lorem quam in massa. Result: Warning Lorem ipsum dolor sit amet, consectetur adipiscing elit. Nulla et euismod nulla. Curabitur feugiat, tortor non consequat finibus, justo purus auctor massa, nec semper lorem quam in massa. Qualifiers: warning caution attention","title":"Warning"},{"location":"extensions/admonition/#failure","text":"Example: !!! failure Lorem ipsum dolor sit amet, consectetur adipiscing elit. Nulla et euismod nulla. Curabitur feugiat, tortor non consequat finibus, justo purus auctor massa, nec semper lorem quam in massa. Result: Failure Lorem ipsum dolor sit amet, consectetur adipiscing elit. Nulla et euismod nulla. Curabitur feugiat, tortor non consequat finibus, justo purus auctor massa, nec semper lorem quam in massa. Qualifiers: failure fail missing","title":"Failure"},{"location":"extensions/admonition/#danger","text":"Example: !!! danger Lorem ipsum dolor sit amet, consectetur adipiscing elit. Nulla et euismod nulla. Curabitur feugiat, tortor non consequat finibus, justo purus auctor massa, nec semper lorem quam in massa. Result: Danger Lorem ipsum dolor sit amet, consectetur adipiscing elit. Nulla et euismod nulla. Curabitur feugiat, tortor non consequat finibus, justo purus auctor massa, nec semper lorem quam in massa. Qualifiers: danger error","title":"Danger"},{"location":"extensions/admonition/#bug","text":"Example: !!! bug Lorem ipsum dolor sit amet, consectetur adipiscing elit. Nulla et euismod nulla. Curabitur feugiat, tortor non consequat finibus, justo purus auctor massa, nec semper lorem quam in massa. Result: Bug Lorem ipsum dolor sit amet, consectetur adipiscing elit. Nulla et euismod nulla. Curabitur feugiat, tortor non consequat finibus, justo purus auctor massa, nec semper lorem quam in massa. Qualifiers: bug","title":"Bug"},{"location":"extensions/admonition/#example","text":"Example: !!! example Lorem ipsum dolor sit amet, consectetur adipiscing elit. Nulla et euismod nulla. Curabitur feugiat, tortor non consequat finibus, justo purus auctor massa, nec semper lorem quam in massa. Result: Example Lorem ipsum dolor sit amet, consectetur adipiscing elit. Nulla et euismod nulla. Curabitur feugiat, tortor non consequat finibus, justo purus auctor massa, nec semper lorem quam in massa. Qualifiers: example snippet","title":"Example"},{"location":"extensions/admonition/#quote","text":"Example: !!! quote Lorem ipsum dolor sit amet, consectetur adipiscing elit. Nulla et euismod nulla. Curabitur feugiat, tortor non consequat finibus, justo purus auctor massa, nec semper lorem quam in massa. Result: Quote Lorem ipsum dolor sit amet, consectetur adipiscing elit. Nulla et euismod nulla. Curabitur feugiat, tortor non consequat finibus, justo purus auctor massa, nec semper lorem quam in massa. Qualifiers: quote cite","title":"Quote"},{"location":"extensions/codehilite/","text":"CodeHilite \u00b6 CodeHilite is an extension that adds syntax highlighting to code blocks and is included in the standard Markdown library. The highlighting process is executed during compilation of the Markdown file. Syntax highlighting not working? Please ensure that Pygments is installed. See the next section for further directions on how to set up Pygments or use the official Docker image with all dependencies pre-installed. Installation \u00b6 CodeHilite parses code blocks and wraps them in pre tags. If Pygments is installed, which is a generic syntax highlighter with support for over 300 languages , CodeHilite will also highlight the code block. Pygments can be installed with the following command: pip install pygments To enable CodeHilite, add the following lines to your mkdocs.yml : markdown_extensions: - codehilite Usage \u00b6 Specifying the language \u00b6 The CodeHilite extension uses the same syntax as regular Markdown code blocks, but needs to know the language of the code block. This can be done in three different ways. via Markdown syntax recommended \u00b6 In Markdown, code blocks can be opened and closed by writing three backticks on separate lines. To add code highlighting to those blocks, the easiest way is to specify the language directly after the opening block. Example: ``` python import tensorflow as tf ``` Result: import tensorflow as tf via Shebang \u00b6 Alternatively, if the first line of a code block contains a shebang, the language is derived from the path referenced in the shebang. This will only work for code blocks that are indented using four spaces, not for those encapsulated in three backticks. Example: #!/usr/bin/python import tensorflow as tf Result: #!/usr/bin/python import tensorflow as tf via three colons \u00b6 If the first line starts with three colons followed by a language identifier, the first line is stripped. This will only work for code blocks that are indented using four spaces, not for those encapsulated in three backticks. Example: :::python import tensorflow as tf Result: import tensorflow as tf Adding line numbers \u00b6 Line numbers can be added by enabling the linenums flag in your mkdocs.yml : markdown_extensions: - codehilite: linenums: true Example: ``` python \"\"\" Bubble sort \"\"\" def bubble_sort(items): for i in range(len(items)): for j in range(len(items) - 1 - i): if items[j] > items[j + 1]: items[j], items[j + 1] = items[j + 1], items[j] ``` Result: \"\"\" Bubble sort \"\"\" def bubble_sort(items): for i in range(len(items)): for j in range(len(items) - 1 - i): if items[j] > items[j + 1]: items[j], items[j + 1] = items[j + 1], items[j] Grouping code blocks \u00b6 The SuperFences extension which is part of the PyMdown Extensions package adds support for grouping code blocks with tabs. This is especially useful for documenting projects with multiple language bindings. Example: ``` bash tab=\"Bash\" #!/bin/bash echo \"Hello world!\" ``` ``` c tab=\"C\" #include <stdio.h> int main(void) { printf(\"Hello world!\\n\"); } ``` ``` c++ tab=\"C++\" #include <iostream> int main() { std::cout << \"Hello world!\" << std::endl; return 0; } ``` ``` c# tab=\"C#\" using System; class Program { static void Main(string[] args) { Console.WriteLine(\"Hello world!\"); } } ``` Result: Bash #!/bin/bash echo \"Hello world!\" C #include <stdio.h> int main(void) { printf(\"Hello world!\\n\"); } C++ #include <iostream> int main() { std::cout << \"Hello world!\" << std::endl; return 0; } C# using System; class Program { static void Main(string[] args) { Console.WriteLine(\"Hello world!\"); } } Highlighting specific lines \u00b6 Specific lines can be highlighted by passing the line numbers to the hl_lines argument placed right after the language identifier. Line counts start at 1. Example: ``` python hl_lines=\"3 4\" \"\"\" Bubble sort \"\"\" def bubble_sort(items): for i in range(len(items)): for j in range(len(items) - 1 - i): if items[j] > items[j + 1]: items[j], items[j + 1] = items[j + 1], items[j] ``` Result: \"\"\" Bubble sort \"\"\" def bubble_sort(items): for i in range(len(items)): for j in range(len(items) - 1 - i): if items[j] > items[j + 1]: items[j], items[j + 1] = items[j + 1], items[j] Supported languages excerpt \u00b6 CodeHilite uses Pygments , a generic syntax highlighter with support for over 300 languages , so the following list of examples is just an excerpt. Bash \u00b6 #!/bin/bash for OPT in \"$@\" do case \"$OPT\" in '-f' ) canonicalize=1 ;; '-n' ) switchlf=\"-n\" ;; esac done # readlink -f function __readlink_f { target=\"$1\" while test -n \"$target\"; do filepath=\"$target\" cd `dirname \"$filepath\"` target=`readlink \"$filepath\"` done /bin/echo $switchlf `pwd -P`/`basename \"$filepath\"` } if [ ! \"$canonicalize\" ]; then readlink $switchlf \"$@\" else for file in \"$@\" do case \"$file\" in -* ) ;; * ) __readlink_f \"$file\" ;; esac done fi exit $? C \u00b6 extern size_t pb_varint_scan(const uint8_t data[], size_t left) { assert(data && left); left = left > 10 ? 10 : left; #ifdef __SSE2__ /* Mapping: remaining bytes ==> bitmask */ static const int mask_map[] = { 0x0000, 0x0001, 0x0003, 0x0007, 0x000F, 0x001F, 0x003F, 0x007F, 0x00FF, 0x01FF, 0x03FF }; /* Load buffer into 128-bit integer and create high-bit mask */ __m128i temp = _mm_loadu_si128((const __m128i *)data); __m128i high = _mm_set1_epi8(0x80); /* Intersect and extract mask with high-bits set */ int mask = _mm_movemask_epi8(_mm_and_si128(temp, high)); mask = (mask & mask_map[left]) ^ mask_map[left]; /* Count trailing zeroes */ return mask ? __builtin_ctz(mask) + 1 : 0; #else /* Linear scan */ size_t size = 0; while (data[size++] & 0x80) if (!--left) return 0; return size; #endif /* __SSE2__ */ } C++ \u00b6 Extension:: Extension(const Descriptor *descriptor, const Descriptor *scope) : descriptor_(descriptor), scope_(scope) { /* Extract full name for signature */ variables_[\"signature\"] = descriptor_->full_name(); /* Prepare message symbol */ variables_[\"message\"] = StringReplace( variables_[\"signature\"], \".\", \"_\", true); LowerString(&(variables_[\"message\"])); /* Suffix scope to identifiers, if given */ string suffix (\"\"); if (scope_) { suffix = scope_->full_name(); /* Check if the base and extension types are in the same package */ if (!scope_->file()->package().compare(descriptor_->file()->package())) suffix = StripPrefixString(suffix, scope_->file()->package() + \".\"); /* Append to signature */ variables_[\"signature\"] += \".[\" + suffix +\"]\"; suffix = \"_\" + suffix; } /* Prepare extension symbol */ variables_[\"extension\"] = StringReplace( suffix, \".\", \"_\", true); LowerString(&(variables_[\"extension\"])); } C& #35 ; \u00b6 public static void Send( Socket socket, byte[] buffer, int offset, int size, int timeout) { int startTickCount = Environment.TickCount; int sent = 0; do { if (Environment.TickCount > startTickCount + timeout) throw new Exception(\"Timeout.\"); try { sent += socket.Send(buffer, offset + sent, size - sent, SocketFlags.None); } catch (SocketException ex) { if (ex.SocketErrorCode == SocketError.WouldBlock || ex.SocketErrorCode == SocketError.IOPending || ex.SocketErrorCode == SocketError.NoBufferSpaceAvailable) { /* Socket buffer is probably full, wait and try again */ Thread.Sleep(30); } else { throw ex; } } } while (sent < size); } Clojure \u00b6 (clojure-version) (defn partition-when [f] (fn [rf] (let [a (java.util.ArrayList.) fval (volatile! false)] (fn ([] (rf)) ([result] (let [result (if (.isEmpty a) result (let [v (vec (.toArray a))] ;; Clear first (.clear a) (unreduced (rf result v))))] (rf result))) ([result input] (if-not (and (f input) @fval) (do (vreset! fval true) (.add a input) result) (let [v (vec (.toArray a))] (.clear a) (let [ret (rf result v)] (when-not (reduced? ret) (.add a input)) ret)))))))) (into [] (partition-when #(.startsWith % \">>\")) [\"1d\" \"33\" \">> 1\" \">> 2\" \"22\" \">> 3\"]) Diff \u00b6 Index: grunt.js =================================================================== --- grunt.js (revision 31200) +++ grunt.js (working copy) @@ -12,6 +12,7 @@ module.exports = function (grunt) { + console.log('hello world'); // Project configuration. grunt.initConfig({ lint: { @@ -19,10 +20,6 @@ 'packages/services.web/{!(test)/**/,}*.js', 'packages/error/**/*.js' ], - scripts: [ - 'grunt.js', - 'db/**/*.js' - ], browser: [ 'packages/web/server.js', 'packages/web/server/**/*.js', Docker \u00b6 FROM ubuntu # Install vnc, xvfb in order to create a 'fake' display and firefox RUN apt-get update && apt-get install -y x11vnc xvfb firefox RUN mkdir ~/.vnc # Setup a password RUN x11vnc -storepasswd 1234 ~/.vnc/passwd # Autostart firefox (might not be the best way, but it does the trick) RUN bash -c 'echo \"firefox\" >> /.bashrc' EXPOSE 5900 CMD [\"x11vnc\", \"-forever\", \"-usepw\", \"-create\"] Elixir \u00b6 require Logger def accept(port) do {:ok, socket} = :gen_tcp.listen(port, [:binary, packet: :line, active: false, reuseaddr: true]) Logger.info \"Accepting connections on port #{port}\" loop_acceptor(socket) end defp loop_acceptor(socket) do {:ok, client} = :gen_tcp.accept(socket) serve(client) loop_acceptor(socket) end defp serve(socket) do socket |> read_line() |> write_line(socket) serve(socket) end defp read_line(socket) do {:ok, data} = :gen_tcp.recv(socket, 0) data end defp write_line(line, socket) do :gen_tcp.send(socket, line) end Erlang \u00b6 circular(Defs) -> [ { { Type, Base }, Fields } || { { Type, Base }, Fields } <- Defs, Type == msg, circular(Base, Defs) ]. circular(Base, Defs) -> Fields = proplists:get_value({ msg, Base }, Defs), circular(Defs, Fields, [Base]). circular(_Defs, [], _Path) -> false; circular(Defs, [Field | Fields], Path) -> case Field#field.type of { msg, Type } -> case lists:member(Type, Path) of false -> Children = proplists:get_value({ msg, Type }, Defs), case circular(Defs, Children, [Type | Path]) of false -> circular(Defs, Fields, Path); true -> true end; true -> Type == lists:last(Path) andalso (length(Path) == 1 orelse not is_tree(Path)) end; _ -> circular(Defs, Fields, Path) end. F& #35 ; \u00b6 /// Asynchronously download retangles from the server /// and decode the JSON format to F# Rectangle record let [<Js>] getRectangles () : Async<Rectangle[]> = async { let req = XMLHttpRequest() req.Open(\"POST\", \"/get\", true) let! resp = req.AsyncSend() return JSON.parse(resp) } /// Repeatedly update rectangles after 0.5 sec let [<Js>] updateLoop () = async { while true do do! Async.Sleep(500) let! rects = getRectangles() cleanRectangles() rects |> Array.iter createRectangle } Go \u00b6 package main import \"fmt\" func counter(id int, channel chan int, closer bool) { for i := 0; i < 10000000; i++ { fmt.Println(\"process\", id,\" send\", i) channel <- 1 } if closer { close(channel ) } } func main() { channel := make(chan int) go counter(1, channel, false) go counter(2, channel, true) x := 0 // receiving data from channel for i := range channel { fmt.Println(\"receiving\") x += i } fmt.Println(x) } HTML \u00b6 <!doctype html> <html class=\"no-js\" lang=\"\"> <head> <meta charset=\"utf-8\"> <meta http-equiv=\"x-ua-compatible\" content=\"ie=edge\"> <title>HTML5 Boilerplate</title> <meta name=\"description\" content=\"\"> <meta name=\"viewport\" content=\"width=device-width, initial-scale=1\"> <link rel=\"apple-touch-icon\" href=\"apple-touch-icon.png\"> <link rel=\"stylesheet\" href=\"css/normalize.css\"> <link rel=\"stylesheet\" href=\"css/main.css\"> <script src=\"js/vendor/modernizr-2.8.3.min.js\"></script> </head> <body> <p>Hello world! This is HTML5 Boilerplate.</p> </body> </html> Java \u00b6 import java.util.LinkedList; import java.lang.reflect.Array; public class UnsortedHashSet<E> { private static final double LOAD_FACTOR_LIMIT = 0.7; private int size; private LinkedList<E>[] con; public UnsortedHashSet() { con = (LinkedList<E>[])(new LinkedList[10]); } public boolean add(E obj) { int oldSize = size; int index = Math.abs(obj.hashCode()) % con.length; if (con[index] == null) con[index] = new LinkedList<E>(); if (!con[index].contains(obj)) { con[index].add(obj); size++; } if (1.0 * size / con.length > LOAD_FACTOR_LIMIT) resize(); return oldSize != size; } private void resize() { UnsortedHashSet<E> temp = new UnsortedHashSet<E>(); temp.con = (LinkedList<E>[])(new LinkedList[con.length * 2 + 1]); for (int i = 0; i < con.length; i++) { if (con[i] != null) for (E e : con[i]) temp.add(e); } con = temp.con; } public int size() { return size; } } JavaScript \u00b6 var Math = require('lib/math'); var _extends = function (target) { for (var i = 1; i < arguments.length; i++) { var source = arguments[i]; for (var key in source) { target[key] = source[key]; } } return target; }; var e = exports.e = 2.71828182846; exports['default'] = function (x) { return Math.exp(x); }; module.exports = _extends(exports['default'], exports); JSON \u00b6 { \"name\": \"mkdocs-material\", \"version\": \"0.2.4\", \"description\": \"A Material Design theme for MkDocs\", \"homepage\": \"http://squidfunk.github.io/mkdocs-material/\", \"authors\": [ \"squidfunk <martin.donath@squidfunk.com>\" ], \"license\": \"MIT\", \"main\": \"Gulpfile.js\", \"scripts\": { \"start\": \"./node_modules/.bin/gulp watch --mkdocs\", \"build\": \"./node_modules/.bin/gulp build --production\" } ... } Julia \u00b6 using MXNet mlp = @mx.chain mx.Variable(:data) => mx.FullyConnected(name=:fc1, num_hidden=128) => mx.Activation(name=:relu1, act_type=:relu) => mx.FullyConnected(name=:fc2, num_hidden=64) => mx.Activation(name=:relu2, act_type=:relu) => mx.FullyConnected(name=:fc3, num_hidden=10) => mx.SoftmaxOutput(name=:softmax) # data provider batch_size = 100 include(Pkg.dir(\"MXNet\", \"examples\", \"mnist\", \"mnist-data.jl\")) train_provider, eval_provider = get_mnist_providers(batch_size) # setup model model = mx.FeedForward(mlp, context=mx.cpu()) # optimization algorithm optimizer = mx.SGD(lr=0.1, momentum=0.9) # fit parameters mx.fit(model, optimizer, train_provider, n_epoch=20, eval_data=eval_provider) Lua \u00b6 local ffi = require(\"ffi\") ffi.cdef[[ void Sleep(int ms); int poll(struct pollfd *fds, unsigned long nfds, int timeout); ]] local sleep if ffi.os == \"Windows\" then function sleep(s) ffi.C.Sleep(s*1000) end else function sleep(s) ffi.C.poll(nil, 0, s * 1000) end end for i = 1,160 do io.write(\".\"); io.flush() sleep(0.01) end io.write(\"\\n\") MySQL \u00b6 SELECT Employees.EmployeeID, Employees.Name, Employees.Salary, Manager.Name AS Manager FROM Employees LEFT JOIN Employees AS Manager ON Employees.ManagerID = Manager.EmployeeID WHERE Employees.EmployeeID = '087652'; PHP \u00b6 <?php // src/AppBundle/Controller/LuckyController.php namespace AppBundle\\Controller; use Sensio\\Bundle\\FrameworkExtraBundle\\Configuration\\Route; use Symfony\\Component\\HttpFoundation\\Response; class LuckyController { /** * @Route(\"/lucky/number\") */ public function numberAction() { $number = mt_rand(0, 100); return new Response( '<html><body>Lucky number: '.$number.'</body></html>' ); } } Protocol Buffers \u00b6 syntax = \"proto2\"; package caffe; // Specifies the shape (dimensions) of a Blob. message BlobShape { repeated int64 dim = 1 [packed = true]; } message BlobProto { optional BlobShape shape = 7; repeated float data = 5 [packed = true]; repeated float diff = 6 [packed = true]; // 4D dimensions -- deprecated. Use \"shape\" instead. optional int32 num = 1 [default = 0]; optional int32 channels = 2 [default = 0]; optional int32 height = 3 [default = 0]; optional int32 width = 4 [default = 0]; } Python \u00b6 \"\"\" A very simple MNIST classifier. See extensive documentation at http://tensorflow.org/tutorials/mnist/beginners/index.md \"\"\" from __future__ import absolute_import from __future__ import division from __future__ import print_function # Import data from tensorflow.examples.tutorials.mnist import input_data import tensorflow as tf flags = tf.app.flags FLAGS = flags.FLAGS flags.DEFINE_string('data_dir', '/tmp/data/', 'Directory for storing data') mnist = input_data.read_data_sets(FLAGS.data_dir, one_hot=True) sess = tf.InteractiveSession() # Create the model x = tf.placeholder(tf.float32, [None, 784]) W = tf.Variable(tf.zeros([784, 10])) b = tf.Variable(tf.zeros([10])) y = tf.nn.softmax(tf.matmul(x, W) + b) Ruby \u00b6 require 'finity/event' require 'finity/machine' require 'finity/state' require 'finity/transition' require 'finity/version' module Finity class InvalidCallback < StandardError; end class MissingCallback < StandardError; end class InvalidState < StandardError; end # Class methods to be injected into the including class upon inclusion. module ClassMethods # Instantiate a new state machine for the including class by accepting a # block with state and event (and subsequent transition) definitions. def finity options = {}, &block @finity ||= Machine.new self, options, &block end # Return the names of all registered states. def states @finity.states.map { |name, _| name } end # Return the names of all registered events. def events @finity.events.map { |name, _| name } end end # Inject methods into the including class upon inclusion. def self.included base base.extend ClassMethods end end XML \u00b6 <?xml version=\"1.0\" encoding=\"UTF-8\"?> <!DOCTYPE mainTag SYSTEM \"some.dtd\" [ENTITY % entity]> <?oxygen RNGSchema=\"some.rng\" type=\"xml\"?> <xs:main-Tag xmlns:xs=\"http://www.w3.org/2001/XMLSchema\"> <!-- This is a sample comment --> <childTag attribute=\"Quoted Value\" another-attribute='Single quoted value' a-third-attribute='123'> <withTextContent>Some text content</withTextContent> <withEntityContent>Some text content with &lt;entities&gt; and mentioning uint8_t and int32_t</withEntityContent> <otherTag attribute='Single quoted Value'/> </childTag> <![CDATA[ some CData ]]> </main-Tag>","title":"CodeHilite"},{"location":"extensions/codehilite/#codehilite","text":"CodeHilite is an extension that adds syntax highlighting to code blocks and is included in the standard Markdown library. The highlighting process is executed during compilation of the Markdown file. Syntax highlighting not working? Please ensure that Pygments is installed. See the next section for further directions on how to set up Pygments or use the official Docker image with all dependencies pre-installed.","title":"CodeHilite"},{"location":"extensions/codehilite/#installation","text":"CodeHilite parses code blocks and wraps them in pre tags. If Pygments is installed, which is a generic syntax highlighter with support for over 300 languages , CodeHilite will also highlight the code block. Pygments can be installed with the following command: pip install pygments To enable CodeHilite, add the following lines to your mkdocs.yml : markdown_extensions: - codehilite","title":"Installation"},{"location":"extensions/codehilite/#usage","text":"","title":"Usage"},{"location":"extensions/codehilite/#specifying-the-language","text":"The CodeHilite extension uses the same syntax as regular Markdown code blocks, but needs to know the language of the code block. This can be done in three different ways.","title":"Specifying the language"},{"location":"extensions/codehilite/#via-markdown-syntax-recommended","text":"In Markdown, code blocks can be opened and closed by writing three backticks on separate lines. To add code highlighting to those blocks, the easiest way is to specify the language directly after the opening block. Example: ``` python import tensorflow as tf ``` Result: import tensorflow as tf","title":"via Markdown syntax recommended"},{"location":"extensions/codehilite/#via-shebang","text":"Alternatively, if the first line of a code block contains a shebang, the language is derived from the path referenced in the shebang. This will only work for code blocks that are indented using four spaces, not for those encapsulated in three backticks. Example: #!/usr/bin/python import tensorflow as tf Result: #!/usr/bin/python import tensorflow as tf","title":"via Shebang"},{"location":"extensions/codehilite/#via-three-colons","text":"If the first line starts with three colons followed by a language identifier, the first line is stripped. This will only work for code blocks that are indented using four spaces, not for those encapsulated in three backticks. Example: :::python import tensorflow as tf Result: import tensorflow as tf","title":"via three colons"},{"location":"extensions/codehilite/#adding-line-numbers","text":"Line numbers can be added by enabling the linenums flag in your mkdocs.yml : markdown_extensions: - codehilite: linenums: true Example: ``` python \"\"\" Bubble sort \"\"\" def bubble_sort(items): for i in range(len(items)): for j in range(len(items) - 1 - i): if items[j] > items[j + 1]: items[j], items[j + 1] = items[j + 1], items[j] ``` Result: \"\"\" Bubble sort \"\"\" def bubble_sort(items): for i in range(len(items)): for j in range(len(items) - 1 - i): if items[j] > items[j + 1]: items[j], items[j + 1] = items[j + 1], items[j]","title":"Adding line numbers"},{"location":"extensions/codehilite/#grouping-code-blocks","text":"The SuperFences extension which is part of the PyMdown Extensions package adds support for grouping code blocks with tabs. This is especially useful for documenting projects with multiple language bindings. Example: ``` bash tab=\"Bash\" #!/bin/bash echo \"Hello world!\" ``` ``` c tab=\"C\" #include <stdio.h> int main(void) { printf(\"Hello world!\\n\"); } ``` ``` c++ tab=\"C++\" #include <iostream> int main() { std::cout << \"Hello world!\" << std::endl; return 0; } ``` ``` c# tab=\"C#\" using System; class Program { static void Main(string[] args) { Console.WriteLine(\"Hello world!\"); } } ``` Result: Bash #!/bin/bash echo \"Hello world!\" C #include <stdio.h> int main(void) { printf(\"Hello world!\\n\"); } C++ #include <iostream> int main() { std::cout << \"Hello world!\" << std::endl; return 0; } C# using System; class Program { static void Main(string[] args) { Console.WriteLine(\"Hello world!\"); } }","title":"Grouping code blocks"},{"location":"extensions/codehilite/#highlighting-specific-lines","text":"Specific lines can be highlighted by passing the line numbers to the hl_lines argument placed right after the language identifier. Line counts start at 1. Example: ``` python hl_lines=\"3 4\" \"\"\" Bubble sort \"\"\" def bubble_sort(items): for i in range(len(items)): for j in range(len(items) - 1 - i): if items[j] > items[j + 1]: items[j], items[j + 1] = items[j + 1], items[j] ``` Result: \"\"\" Bubble sort \"\"\" def bubble_sort(items): for i in range(len(items)): for j in range(len(items) - 1 - i): if items[j] > items[j + 1]: items[j], items[j + 1] = items[j + 1], items[j]","title":"Highlighting specific lines"},{"location":"extensions/codehilite/#supported-languages-excerpt","text":"CodeHilite uses Pygments , a generic syntax highlighter with support for over 300 languages , so the following list of examples is just an excerpt.","title":"Supported languages excerpt"},{"location":"extensions/codehilite/#bash","text":"#!/bin/bash for OPT in \"$@\" do case \"$OPT\" in '-f' ) canonicalize=1 ;; '-n' ) switchlf=\"-n\" ;; esac done # readlink -f function __readlink_f { target=\"$1\" while test -n \"$target\"; do filepath=\"$target\" cd `dirname \"$filepath\"` target=`readlink \"$filepath\"` done /bin/echo $switchlf `pwd -P`/`basename \"$filepath\"` } if [ ! \"$canonicalize\" ]; then readlink $switchlf \"$@\" else for file in \"$@\" do case \"$file\" in -* ) ;; * ) __readlink_f \"$file\" ;; esac done fi exit $?","title":"Bash"},{"location":"extensions/codehilite/#c","text":"extern size_t pb_varint_scan(const uint8_t data[], size_t left) { assert(data && left); left = left > 10 ? 10 : left; #ifdef __SSE2__ /* Mapping: remaining bytes ==> bitmask */ static const int mask_map[] = { 0x0000, 0x0001, 0x0003, 0x0007, 0x000F, 0x001F, 0x003F, 0x007F, 0x00FF, 0x01FF, 0x03FF }; /* Load buffer into 128-bit integer and create high-bit mask */ __m128i temp = _mm_loadu_si128((const __m128i *)data); __m128i high = _mm_set1_epi8(0x80); /* Intersect and extract mask with high-bits set */ int mask = _mm_movemask_epi8(_mm_and_si128(temp, high)); mask = (mask & mask_map[left]) ^ mask_map[left]; /* Count trailing zeroes */ return mask ? __builtin_ctz(mask) + 1 : 0; #else /* Linear scan */ size_t size = 0; while (data[size++] & 0x80) if (!--left) return 0; return size; #endif /* __SSE2__ */ }","title":"C"},{"location":"extensions/codehilite/#c_1","text":"Extension:: Extension(const Descriptor *descriptor, const Descriptor *scope) : descriptor_(descriptor), scope_(scope) { /* Extract full name for signature */ variables_[\"signature\"] = descriptor_->full_name(); /* Prepare message symbol */ variables_[\"message\"] = StringReplace( variables_[\"signature\"], \".\", \"_\", true); LowerString(&(variables_[\"message\"])); /* Suffix scope to identifiers, if given */ string suffix (\"\"); if (scope_) { suffix = scope_->full_name(); /* Check if the base and extension types are in the same package */ if (!scope_->file()->package().compare(descriptor_->file()->package())) suffix = StripPrefixString(suffix, scope_->file()->package() + \".\"); /* Append to signature */ variables_[\"signature\"] += \".[\" + suffix +\"]\"; suffix = \"_\" + suffix; } /* Prepare extension symbol */ variables_[\"extension\"] = StringReplace( suffix, \".\", \"_\", true); LowerString(&(variables_[\"extension\"])); }","title":"C++"},{"location":"extensions/codehilite/#c35","text":"public static void Send( Socket socket, byte[] buffer, int offset, int size, int timeout) { int startTickCount = Environment.TickCount; int sent = 0; do { if (Environment.TickCount > startTickCount + timeout) throw new Exception(\"Timeout.\"); try { sent += socket.Send(buffer, offset + sent, size - sent, SocketFlags.None); } catch (SocketException ex) { if (ex.SocketErrorCode == SocketError.WouldBlock || ex.SocketErrorCode == SocketError.IOPending || ex.SocketErrorCode == SocketError.NoBufferSpaceAvailable) { /* Socket buffer is probably full, wait and try again */ Thread.Sleep(30); } else { throw ex; } } } while (sent < size); }","title":"C&#35;"},{"location":"extensions/codehilite/#clojure","text":"(clojure-version) (defn partition-when [f] (fn [rf] (let [a (java.util.ArrayList.) fval (volatile! false)] (fn ([] (rf)) ([result] (let [result (if (.isEmpty a) result (let [v (vec (.toArray a))] ;; Clear first (.clear a) (unreduced (rf result v))))] (rf result))) ([result input] (if-not (and (f input) @fval) (do (vreset! fval true) (.add a input) result) (let [v (vec (.toArray a))] (.clear a) (let [ret (rf result v)] (when-not (reduced? ret) (.add a input)) ret)))))))) (into [] (partition-when #(.startsWith % \">>\")) [\"1d\" \"33\" \">> 1\" \">> 2\" \"22\" \">> 3\"])","title":"Clojure"},{"location":"extensions/codehilite/#diff","text":"Index: grunt.js =================================================================== --- grunt.js (revision 31200) +++ grunt.js (working copy) @@ -12,6 +12,7 @@ module.exports = function (grunt) { + console.log('hello world'); // Project configuration. grunt.initConfig({ lint: { @@ -19,10 +20,6 @@ 'packages/services.web/{!(test)/**/,}*.js', 'packages/error/**/*.js' ], - scripts: [ - 'grunt.js', - 'db/**/*.js' - ], browser: [ 'packages/web/server.js', 'packages/web/server/**/*.js',","title":"Diff"},{"location":"extensions/codehilite/#docker","text":"FROM ubuntu # Install vnc, xvfb in order to create a 'fake' display and firefox RUN apt-get update && apt-get install -y x11vnc xvfb firefox RUN mkdir ~/.vnc # Setup a password RUN x11vnc -storepasswd 1234 ~/.vnc/passwd # Autostart firefox (might not be the best way, but it does the trick) RUN bash -c 'echo \"firefox\" >> /.bashrc' EXPOSE 5900 CMD [\"x11vnc\", \"-forever\", \"-usepw\", \"-create\"]","title":"Docker"},{"location":"extensions/codehilite/#elixir","text":"require Logger def accept(port) do {:ok, socket} = :gen_tcp.listen(port, [:binary, packet: :line, active: false, reuseaddr: true]) Logger.info \"Accepting connections on port #{port}\" loop_acceptor(socket) end defp loop_acceptor(socket) do {:ok, client} = :gen_tcp.accept(socket) serve(client) loop_acceptor(socket) end defp serve(socket) do socket |> read_line() |> write_line(socket) serve(socket) end defp read_line(socket) do {:ok, data} = :gen_tcp.recv(socket, 0) data end defp write_line(line, socket) do :gen_tcp.send(socket, line) end","title":"Elixir"},{"location":"extensions/codehilite/#erlang","text":"circular(Defs) -> [ { { Type, Base }, Fields } || { { Type, Base }, Fields } <- Defs, Type == msg, circular(Base, Defs) ]. circular(Base, Defs) -> Fields = proplists:get_value({ msg, Base }, Defs), circular(Defs, Fields, [Base]). circular(_Defs, [], _Path) -> false; circular(Defs, [Field | Fields], Path) -> case Field#field.type of { msg, Type } -> case lists:member(Type, Path) of false -> Children = proplists:get_value({ msg, Type }, Defs), case circular(Defs, Children, [Type | Path]) of false -> circular(Defs, Fields, Path); true -> true end; true -> Type == lists:last(Path) andalso (length(Path) == 1 orelse not is_tree(Path)) end; _ -> circular(Defs, Fields, Path) end.","title":"Erlang"},{"location":"extensions/codehilite/#f35","text":"/// Asynchronously download retangles from the server /// and decode the JSON format to F# Rectangle record let [<Js>] getRectangles () : Async<Rectangle[]> = async { let req = XMLHttpRequest() req.Open(\"POST\", \"/get\", true) let! resp = req.AsyncSend() return JSON.parse(resp) } /// Repeatedly update rectangles after 0.5 sec let [<Js>] updateLoop () = async { while true do do! Async.Sleep(500) let! rects = getRectangles() cleanRectangles() rects |> Array.iter createRectangle }","title":"F&#35;"},{"location":"extensions/codehilite/#go","text":"package main import \"fmt\" func counter(id int, channel chan int, closer bool) { for i := 0; i < 10000000; i++ { fmt.Println(\"process\", id,\" send\", i) channel <- 1 } if closer { close(channel ) } } func main() { channel := make(chan int) go counter(1, channel, false) go counter(2, channel, true) x := 0 // receiving data from channel for i := range channel { fmt.Println(\"receiving\") x += i } fmt.Println(x) }","title":"Go"},{"location":"extensions/codehilite/#html","text":"<!doctype html> <html class=\"no-js\" lang=\"\"> <head> <meta charset=\"utf-8\"> <meta http-equiv=\"x-ua-compatible\" content=\"ie=edge\"> <title>HTML5 Boilerplate</title> <meta name=\"description\" content=\"\"> <meta name=\"viewport\" content=\"width=device-width, initial-scale=1\"> <link rel=\"apple-touch-icon\" href=\"apple-touch-icon.png\"> <link rel=\"stylesheet\" href=\"css/normalize.css\"> <link rel=\"stylesheet\" href=\"css/main.css\"> <script src=\"js/vendor/modernizr-2.8.3.min.js\"></script> </head> <body> <p>Hello world! This is HTML5 Boilerplate.</p> </body> </html>","title":"HTML"},{"location":"extensions/codehilite/#java","text":"import java.util.LinkedList; import java.lang.reflect.Array; public class UnsortedHashSet<E> { private static final double LOAD_FACTOR_LIMIT = 0.7; private int size; private LinkedList<E>[] con; public UnsortedHashSet() { con = (LinkedList<E>[])(new LinkedList[10]); } public boolean add(E obj) { int oldSize = size; int index = Math.abs(obj.hashCode()) % con.length; if (con[index] == null) con[index] = new LinkedList<E>(); if (!con[index].contains(obj)) { con[index].add(obj); size++; } if (1.0 * size / con.length > LOAD_FACTOR_LIMIT) resize(); return oldSize != size; } private void resize() { UnsortedHashSet<E> temp = new UnsortedHashSet<E>(); temp.con = (LinkedList<E>[])(new LinkedList[con.length * 2 + 1]); for (int i = 0; i < con.length; i++) { if (con[i] != null) for (E e : con[i]) temp.add(e); } con = temp.con; } public int size() { return size; } }","title":"Java"},{"location":"extensions/codehilite/#javascript","text":"var Math = require('lib/math'); var _extends = function (target) { for (var i = 1; i < arguments.length; i++) { var source = arguments[i]; for (var key in source) { target[key] = source[key]; } } return target; }; var e = exports.e = 2.71828182846; exports['default'] = function (x) { return Math.exp(x); }; module.exports = _extends(exports['default'], exports);","title":"JavaScript"},{"location":"extensions/codehilite/#json","text":"{ \"name\": \"mkdocs-material\", \"version\": \"0.2.4\", \"description\": \"A Material Design theme for MkDocs\", \"homepage\": \"http://squidfunk.github.io/mkdocs-material/\", \"authors\": [ \"squidfunk <martin.donath@squidfunk.com>\" ], \"license\": \"MIT\", \"main\": \"Gulpfile.js\", \"scripts\": { \"start\": \"./node_modules/.bin/gulp watch --mkdocs\", \"build\": \"./node_modules/.bin/gulp build --production\" } ... }","title":"JSON"},{"location":"extensions/codehilite/#julia","text":"using MXNet mlp = @mx.chain mx.Variable(:data) => mx.FullyConnected(name=:fc1, num_hidden=128) => mx.Activation(name=:relu1, act_type=:relu) => mx.FullyConnected(name=:fc2, num_hidden=64) => mx.Activation(name=:relu2, act_type=:relu) => mx.FullyConnected(name=:fc3, num_hidden=10) => mx.SoftmaxOutput(name=:softmax) # data provider batch_size = 100 include(Pkg.dir(\"MXNet\", \"examples\", \"mnist\", \"mnist-data.jl\")) train_provider, eval_provider = get_mnist_providers(batch_size) # setup model model = mx.FeedForward(mlp, context=mx.cpu()) # optimization algorithm optimizer = mx.SGD(lr=0.1, momentum=0.9) # fit parameters mx.fit(model, optimizer, train_provider, n_epoch=20, eval_data=eval_provider)","title":"Julia"},{"location":"extensions/codehilite/#lua","text":"local ffi = require(\"ffi\") ffi.cdef[[ void Sleep(int ms); int poll(struct pollfd *fds, unsigned long nfds, int timeout); ]] local sleep if ffi.os == \"Windows\" then function sleep(s) ffi.C.Sleep(s*1000) end else function sleep(s) ffi.C.poll(nil, 0, s * 1000) end end for i = 1,160 do io.write(\".\"); io.flush() sleep(0.01) end io.write(\"\\n\")","title":"Lua"},{"location":"extensions/codehilite/#mysql","text":"SELECT Employees.EmployeeID, Employees.Name, Employees.Salary, Manager.Name AS Manager FROM Employees LEFT JOIN Employees AS Manager ON Employees.ManagerID = Manager.EmployeeID WHERE Employees.EmployeeID = '087652';","title":"MySQL"},{"location":"extensions/codehilite/#php","text":"<?php // src/AppBundle/Controller/LuckyController.php namespace AppBundle\\Controller; use Sensio\\Bundle\\FrameworkExtraBundle\\Configuration\\Route; use Symfony\\Component\\HttpFoundation\\Response; class LuckyController { /** * @Route(\"/lucky/number\") */ public function numberAction() { $number = mt_rand(0, 100); return new Response( '<html><body>Lucky number: '.$number.'</body></html>' ); } }","title":"PHP"},{"location":"extensions/codehilite/#protocol-buffers","text":"syntax = \"proto2\"; package caffe; // Specifies the shape (dimensions) of a Blob. message BlobShape { repeated int64 dim = 1 [packed = true]; } message BlobProto { optional BlobShape shape = 7; repeated float data = 5 [packed = true]; repeated float diff = 6 [packed = true]; // 4D dimensions -- deprecated. Use \"shape\" instead. optional int32 num = 1 [default = 0]; optional int32 channels = 2 [default = 0]; optional int32 height = 3 [default = 0]; optional int32 width = 4 [default = 0]; }","title":"Protocol Buffers"},{"location":"extensions/codehilite/#python","text":"\"\"\" A very simple MNIST classifier. See extensive documentation at http://tensorflow.org/tutorials/mnist/beginners/index.md \"\"\" from __future__ import absolute_import from __future__ import division from __future__ import print_function # Import data from tensorflow.examples.tutorials.mnist import input_data import tensorflow as tf flags = tf.app.flags FLAGS = flags.FLAGS flags.DEFINE_string('data_dir', '/tmp/data/', 'Directory for storing data') mnist = input_data.read_data_sets(FLAGS.data_dir, one_hot=True) sess = tf.InteractiveSession() # Create the model x = tf.placeholder(tf.float32, [None, 784]) W = tf.Variable(tf.zeros([784, 10])) b = tf.Variable(tf.zeros([10])) y = tf.nn.softmax(tf.matmul(x, W) + b)","title":"Python"},{"location":"extensions/codehilite/#ruby","text":"require 'finity/event' require 'finity/machine' require 'finity/state' require 'finity/transition' require 'finity/version' module Finity class InvalidCallback < StandardError; end class MissingCallback < StandardError; end class InvalidState < StandardError; end # Class methods to be injected into the including class upon inclusion. module ClassMethods # Instantiate a new state machine for the including class by accepting a # block with state and event (and subsequent transition) definitions. def finity options = {}, &block @finity ||= Machine.new self, options, &block end # Return the names of all registered states. def states @finity.states.map { |name, _| name } end # Return the names of all registered events. def events @finity.events.map { |name, _| name } end end # Inject methods into the including class upon inclusion. def self.included base base.extend ClassMethods end end","title":"Ruby"},{"location":"extensions/codehilite/#xml","text":"<?xml version=\"1.0\" encoding=\"UTF-8\"?> <!DOCTYPE mainTag SYSTEM \"some.dtd\" [ENTITY % entity]> <?oxygen RNGSchema=\"some.rng\" type=\"xml\"?> <xs:main-Tag xmlns:xs=\"http://www.w3.org/2001/XMLSchema\"> <!-- This is a sample comment --> <childTag attribute=\"Quoted Value\" another-attribute='Single quoted value' a-third-attribute='123'> <withTextContent>Some text content</withTextContent> <withEntityContent>Some text content with &lt;entities&gt; and mentioning uint8_t and int32_t</withEntityContent> <otherTag attribute='Single quoted Value'/> </childTag> <![CDATA[ some CData ]]> </main-Tag>","title":"XML"},{"location":"extensions/footnotes/","text":"Footnotes \u00b6 Footnotes is another extension included in the standard Markdown library. As the name says, it adds the ability to add footnotes to your documentation. Installation \u00b6 Add the following lines to your mkdocs.yml : markdown_extensions: - footnotes Usage \u00b6 The markup for footnotes is similar to the standard Markdown markup for links. A reference is inserted in the text, which can then be defined at any point in the document. Inserting the reference \u00b6 The footnote reference is enclosed in square brackets and starts with a caret, followed by an arbitrary label which may contain numeric identifiers [1, 2, 3, ...] or names [Granovetter et al. 1998]. The rendered references are always consecutive superscripted numbers. Example: Lorem ipsum[^1] dolor sit amet, consectetur adipiscing elit.[^2] Result: Lorem ipsum 1 dolor sit amet, consectetur adipiscing elit. 2 Inserting the content \u00b6 The footnote content is also declared with a label, which must match the label used for the footnote reference. It can be inserted at an arbitrary position in the document and is always rendered at the bottom of the page. Furthermore, a backlink is automatically added to the footnote reference. on a single line \u00b6 Short statements can be written on the same line. Example: [^1]: Lorem ipsum dolor sit amet, consectetur adipiscing elit. Result: Jump to footnote at the bottom of the page on multiple lines \u00b6 Paragraphs should be written on the next line. As with all Markdown blocks, the content must be indented by four spaces. Example: [^2]: Lorem ipsum dolor sit amet, consectetur adipiscing elit. Nulla et euismod nulla. Curabitur feugiat, tortor non consequat finibus, justo purus auctor massa, nec semper lorem quam in massa. Result: Jump to footnote at the bottom of the page Lorem ipsum dolor sit amet, consectetur adipiscing elit. \u21a9 Lorem ipsum dolor sit amet, consectetur adipiscing elit. Nulla et euismod nulla. Curabitur feugiat, tortor non consequat finibus, justo purus auctor massa, nec semper lorem quam in massa. \u21a9","title":"Footnotes"},{"location":"extensions/footnotes/#footnotes","text":"Footnotes is another extension included in the standard Markdown library. As the name says, it adds the ability to add footnotes to your documentation.","title":"Footnotes"},{"location":"extensions/footnotes/#installation","text":"Add the following lines to your mkdocs.yml : markdown_extensions: - footnotes","title":"Installation"},{"location":"extensions/footnotes/#usage","text":"The markup for footnotes is similar to the standard Markdown markup for links. A reference is inserted in the text, which can then be defined at any point in the document.","title":"Usage"},{"location":"extensions/footnotes/#inserting-the-reference","text":"The footnote reference is enclosed in square brackets and starts with a caret, followed by an arbitrary label which may contain numeric identifiers [1, 2, 3, ...] or names [Granovetter et al. 1998]. The rendered references are always consecutive superscripted numbers. Example: Lorem ipsum[^1] dolor sit amet, consectetur adipiscing elit.[^2] Result: Lorem ipsum 1 dolor sit amet, consectetur adipiscing elit. 2","title":"Inserting the reference"},{"location":"extensions/footnotes/#inserting-the-content","text":"The footnote content is also declared with a label, which must match the label used for the footnote reference. It can be inserted at an arbitrary position in the document and is always rendered at the bottom of the page. Furthermore, a backlink is automatically added to the footnote reference.","title":"Inserting the content"},{"location":"extensions/footnotes/#on-a-single-line","text":"Short statements can be written on the same line. Example: [^1]: Lorem ipsum dolor sit amet, consectetur adipiscing elit. Result: Jump to footnote at the bottom of the page","title":"on a single line"},{"location":"extensions/footnotes/#on-multiple-lines","text":"Paragraphs should be written on the next line. As with all Markdown blocks, the content must be indented by four spaces. Example: [^2]: Lorem ipsum dolor sit amet, consectetur adipiscing elit. Nulla et euismod nulla. Curabitur feugiat, tortor non consequat finibus, justo purus auctor massa, nec semper lorem quam in massa. Result: Jump to footnote at the bottom of the page Lorem ipsum dolor sit amet, consectetur adipiscing elit. \u21a9 Lorem ipsum dolor sit amet, consectetur adipiscing elit. Nulla et euismod nulla. Curabitur feugiat, tortor non consequat finibus, justo purus auctor massa, nec semper lorem quam in massa. \u21a9","title":"on multiple lines"},{"location":"extensions/metadata/","text":"Metadata \u00b6 The Metadata extension makes it possible to add metadata to a document which gives more control over the theme in a page-specific context. Installation \u00b6 Add the following lines to your mkdocs.yml : markdown_extensions: - meta Usage \u00b6 Metadata is written as a series of key-value pairs at the beginning of the Markdown document, delimited by a blank line which ends the metadata context. Naturally, the metadata is stripped from the document before rendering the actual page content and made available to the theme. Example: title: Lorem ipsum dolor sit amet description: Nullam urna elit, malesuada eget finibus ut, ac tortor. path: path/to/file source: file.js # Headline ... See the next section which covers the metadata that is supported by Material. Setting a hero text \u00b6 Material exposes a simple text-only page-local hero via Metadata, as you can see on the current page when you scroll to the top. It's as simple as: hero: Metadata enables hero teaser texts Linking sources \u00b6 When a document is related to a specific set of source files and the repo_url is defined inside the project's mkdocs.yml , the files can be linked using the source key: source: file.js The filename is appended to the repo_url set in your mkdocs.yml , but can be prefixed with a path to ensure correct path resolving: Example: path: tree/master/docs/extensions source: metadata.md Result: See the source section for the resulting output. Redirecting to another page \u00b6 It's sometimes necessary to move documents around in the navigation tree and redirect user from the old URL to the new one. The redirect meta-tag allows to create a redirection from the current document to the address specified in the tag. For instance, if your document contains: redirect: /new/url accessing that document's URL will automatically redirect to /new/url . Overrides \u00b6 Page title \u00b6 The page title can be overridden on a per-document level: title: Lorem ipsum dolor sit amet This will set the title tag inside the document head for the current page to the provided value. It will also override the default behavior of Material for MkDocs which appends the site title using a dash as a separator to the page title. Page description \u00b6 The page description can also be overridden on a per-document level: description: Nullam urna elit, malesuada eget finibus ut, ac tortor. This will set the meta tag containing the site description inside the document head for the current page to the provided value. Disqus \u00b6 As describe in the getting started guide , the Disqus comments section can be enabled on a per-document level: disqus: your-shortname Disqus can be disabled for a specific page by setting it to an empty value: disqus:","title":"Metadata"},{"location":"extensions/metadata/#metadata","text":"The Metadata extension makes it possible to add metadata to a document which gives more control over the theme in a page-specific context.","title":"Metadata"},{"location":"extensions/metadata/#installation","text":"Add the following lines to your mkdocs.yml : markdown_extensions: - meta","title":"Installation"},{"location":"extensions/metadata/#usage","text":"Metadata is written as a series of key-value pairs at the beginning of the Markdown document, delimited by a blank line which ends the metadata context. Naturally, the metadata is stripped from the document before rendering the actual page content and made available to the theme. Example: title: Lorem ipsum dolor sit amet description: Nullam urna elit, malesuada eget finibus ut, ac tortor. path: path/to/file source: file.js # Headline ... See the next section which covers the metadata that is supported by Material.","title":"Usage"},{"location":"extensions/metadata/#setting-a-hero-text","text":"Material exposes a simple text-only page-local hero via Metadata, as you can see on the current page when you scroll to the top. It's as simple as: hero: Metadata enables hero teaser texts","title":"Setting a hero text"},{"location":"extensions/metadata/#linking-sources","text":"When a document is related to a specific set of source files and the repo_url is defined inside the project's mkdocs.yml , the files can be linked using the source key: source: file.js The filename is appended to the repo_url set in your mkdocs.yml , but can be prefixed with a path to ensure correct path resolving: Example: path: tree/master/docs/extensions source: metadata.md Result: See the source section for the resulting output.","title":"Linking sources"},{"location":"extensions/metadata/#redirecting-to-another-page","text":"It's sometimes necessary to move documents around in the navigation tree and redirect user from the old URL to the new one. The redirect meta-tag allows to create a redirection from the current document to the address specified in the tag. For instance, if your document contains: redirect: /new/url accessing that document's URL will automatically redirect to /new/url .","title":"Redirecting to another page"},{"location":"extensions/metadata/#overrides","text":"","title":"Overrides"},{"location":"extensions/metadata/#page-title","text":"The page title can be overridden on a per-document level: title: Lorem ipsum dolor sit amet This will set the title tag inside the document head for the current page to the provided value. It will also override the default behavior of Material for MkDocs which appends the site title using a dash as a separator to the page title.","title":"Page title"},{"location":"extensions/metadata/#page-description","text":"The page description can also be overridden on a per-document level: description: Nullam urna elit, malesuada eget finibus ut, ac tortor. This will set the meta tag containing the site description inside the document head for the current page to the provided value.","title":"Page description"},{"location":"extensions/metadata/#disqus","text":"As describe in the getting started guide , the Disqus comments section can be enabled on a per-document level: disqus: your-shortname Disqus can be disabled for a specific page by setting it to an empty value: disqus:","title":"Disqus"},{"location":"extensions/permalinks/","text":"Permalinks \u00b6 Permalinks are a feature of the Table of Contents extension, which is part of the standard Markdown library. The extension inserts an anchor at the end of each headline, which makes it possible to directly link to a subpart of the document. Installation \u00b6 To enable permalinks, add the following to your mkdocs.yml : markdown_extensions: - toc: permalink: true This will add a link containing the paragraph symbol \u00b6 at the end of each headline (exactly like on the page you're currently viewing), which the Material theme will make appear on hover. In order to change the text of the permalink, a string can be passed, e.g.: markdown_extensions: - toc: permalink: Link Usage \u00b6 When enabled, permalinks are inserted automatically.","title":"Permalinks"},{"location":"extensions/permalinks/#permalinks","text":"Permalinks are a feature of the Table of Contents extension, which is part of the standard Markdown library. The extension inserts an anchor at the end of each headline, which makes it possible to directly link to a subpart of the document.","title":"Permalinks"},{"location":"extensions/permalinks/#installation","text":"To enable permalinks, add the following to your mkdocs.yml : markdown_extensions: - toc: permalink: true This will add a link containing the paragraph symbol \u00b6 at the end of each headline (exactly like on the page you're currently viewing), which the Material theme will make appear on hover. In order to change the text of the permalink, a string can be passed, e.g.: markdown_extensions: - toc: permalink: Link","title":"Installation"},{"location":"extensions/permalinks/#usage","text":"When enabled, permalinks are inserted automatically.","title":"Usage"},{"location":"extensions/pymdown/","text":"PyMdown Extensions \u00b6 PyMdown Extensions is a collection of Markdown extensions that add some great features to the standard Markdown library. For this reason, the installation of this package is highly recommended as it's well-integrated with the Material theme. Installation \u00b6 The PyMdown Extensions package can be installed with the following command: pip install pymdown-extensions The following list of extensions that are part of the PyMdown Extensions package are recommended to be used together with the Material theme: markdown_extensions: - pymdownx.arithmatex - pymdownx.betterem: smart_enable: all - pymdownx.caret - pymdownx.critic - pymdownx.details - pymdownx.emoji: emoji_generator: !!python/name:pymdownx.emoji.to_svg - pymdownx.inlinehilite - pymdownx.magiclink - pymdownx.mark - pymdownx.smartsymbols - pymdownx.superfences - pymdownx.tasklist: custom_checkbox: true - pymdownx.tilde Usage \u00b6 Arithmatex MathJax \u00b6 Arithmatex integrates Material with MathJax which parses block-style and inline equations written in TeX markup and outputs them in mathematical notation. See this thread for a short introduction and quick reference on how to write equations in TeX syntax. Besides activating the extension in the mkdocs.yml , the MathJax JavaScript runtime needs to be included. This must be done with additional JavaScript : extra_javascript: - 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js?config=TeX-MML-AM_CHTML' If you want to override the default MathJax configuration, you can do this by adding another JavaScript file before the MathJax runtime in extra_javascript which contains your MathJax configuration, e.g.: window.MathJax = { tex2jax: { inlineMath: [ [\"\\\\(\",\"\\\\)\"] ], displayMath: [ [\"\\\\[\",\"\\\\]\"] ] }, TeX: { TagSide: \"right\", TagIndent: \".8em\", MultLineWidth: \"85%\", equationNumbers: { autoNumber: \"AMS\", }, unicode: { fonts: \"STIXGeneral,'Arial Unicode MS'\" } }, displayAlign: \"left\", showProcessingMessages: false, messageStyle: \"none\" }; In your mkdocs.yml , include it with: extra_javascript: - 'javascripts/extra.js' - 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js?config=TeX-MML-AM_CHTML' Blocks \u00b6 Blocks are enclosed in $$...$$ which are placed on separate lines. Example: $$ \\frac{n!}{k!(n-k)!} = \\binom{n}{k} $$ Result: \\frac{n!}{k!(n-k)!} = \\binom{n}{k} \\frac{n!}{k!(n-k)!} = \\binom{n}{k} Inline \u00b6 Inline equations need to be enclosed in $...$ : Example: Lorem ipsum dolor sit amet: $p(x|y) = \\frac{p(y|x)p(x)}{p(y)}$ Result: Lorem ipsum dolor sit amet: p(x|y) = \\frac{p(y|x)p(x)}{p(y)} p(x|y) = \\frac{p(y|x)p(x)}{p(y)} BetterEm \u00b6 BetterEm improves the handling of emphasis markup ( bold and italic ) within Markdown by providing a more sophisticated parser for better detecting start and end tokens. Read the documentation for usage notes . Caret \u00b6 Caret makes it possible to highlight inserted text . The portion of text that should be marked as added must be enclosed in two carets ^^...^^ . Critic \u00b6 Critic implements Critic Markup , a Markdown extension that enables the tracking of changes (additions, deletions and comments) on documents. During compilation of the Markdown document, changes can be rendered (default), accepted or rejected. Text can be deleted and replacement text added . This can also be combined into one a single operation. Highlighting is also possible and comments can be added inline . Formatting can also be applied to blocks, by putting the opening and closing tags on separate lines and adding new lines between the tags and the content. Details \u00b6 Details adds collapsible Admonition-style blocks which can contain arbitrary content using the HTML5 details and summary tags. Additionally, all Admonition qualifiers can be used, e.g. note , question , warning etc.: How many Prolog programmers does it take to change a lightbulb? Yes. Emoji \u00b6 Emoji adds the ability to insert a -load of emojis that we use in our daily lives. See the EmojiOne demo for a list of all available emojis. Happy scrolling Legal disclaimer Material has no affiliation with EmojiOne which is released under CC BY 4.0 . When including EmojiOne images or CSS, please read the EmojiOne license to ensure proper usage and attribution. InlineHilite \u00b6 InlineHilite adds support for inline code highlighting. It's useful for short snippets included within body copy, e.g. var test = 0; and can be achived by prefixing inline code with a shebang and language identifier, e.g. #!js . MagicLink \u00b6 MagicLink detects links in Markdown and auto-generates the necessary markup, so no special syntax is required. It auto-links http[s]:// and ftp:// links, as well as references to email addresses. Mark \u00b6 Mark adds the ability to highlight text like it was marked with a text marker . The portion of text that should be highlighted must be enclosed in two equal signs ==...== . SmartSymbols \u00b6 SmartSymbols converts markup for special characters into their corresponding symbols, e.g. arrows (\u2190, \u2192, \u2194), trademark and copyright symbols (\u00a9, \u2122, \u00ae) and fractions (\u00bd, \u00bc, ...). SuperFences \u00b6 SuperFences provides the ability to nest code blocks under blockquotes, lists and other block elements, which the Fenced Code Blocks extension from the standard Markdown library doesn't parse correctly. SuperFences does also allow grouping code blocks with tabs . Tasklist \u00b6 Tasklist adds support for styled checkbox lists. This is useful for keeping track of tasks and showing what has been done and has yet to be done. Checkbox lists are like regular lists, but prefixed with [ ] for empty or [x] for filled checkboxes. Example: * [x] Lorem ipsum dolor sit amet, consectetur adipiscing elit * [x] Nulla lobortis egestas semper * [x] Curabitur elit nibh, euismod et ullamcorper at, iaculis feugiat est * [ ] Vestibulum convallis sit amet nisi a tincidunt * [x] In hac habitasse platea dictumst * [x] In scelerisque nibh non dolor mollis congue sed et metus * [x] Sed egestas felis quis elit dapibus, ac aliquet turpis mattis * [ ] Praesent sed risus massa * [ ] Aenean pretium efficitur erat, donec pharetra, ligula non scelerisque * [ ] Nulla vel eros venenatis, imperdiet enim id, faucibus nisi Result: Lorem ipsum dolor sit amet, consectetur adipiscing elit Nulla lobortis egestas semper Curabitur elit nibh, euismod et ullamcorper at, iaculis feugiat est Vestibulum convallis sit amet nisi a tincidunt In hac habitasse platea dictumst In scelerisque nibh non dolor mollis congue sed et metus Sed egestas felis quis elit dapibus, ac aliquet turpis mattis Praesent sed risus massa Aenean pretium efficitur erat, donec pharetra, ligula non scelerisque Nulla vel eros venenatis, imperdiet enim id, faucibus nisi Tilde \u00b6 Tilde provides an easy way to strike through cross out text. The portion of text that should be erased must be enclosed in two tildes ~~...~~ and the extension will take care of the rest.","title":"PyMdown Extensions"},{"location":"extensions/pymdown/#pymdown-extensions","text":"PyMdown Extensions is a collection of Markdown extensions that add some great features to the standard Markdown library. For this reason, the installation of this package is highly recommended as it's well-integrated with the Material theme.","title":"PyMdown Extensions"},{"location":"extensions/pymdown/#installation","text":"The PyMdown Extensions package can be installed with the following command: pip install pymdown-extensions The following list of extensions that are part of the PyMdown Extensions package are recommended to be used together with the Material theme: markdown_extensions: - pymdownx.arithmatex - pymdownx.betterem: smart_enable: all - pymdownx.caret - pymdownx.critic - pymdownx.details - pymdownx.emoji: emoji_generator: !!python/name:pymdownx.emoji.to_svg - pymdownx.inlinehilite - pymdownx.magiclink - pymdownx.mark - pymdownx.smartsymbols - pymdownx.superfences - pymdownx.tasklist: custom_checkbox: true - pymdownx.tilde","title":"Installation"},{"location":"extensions/pymdown/#usage","text":"","title":"Usage"},{"location":"extensions/pymdown/#arithmatex-mathjax","text":"Arithmatex integrates Material with MathJax which parses block-style and inline equations written in TeX markup and outputs them in mathematical notation. See this thread for a short introduction and quick reference on how to write equations in TeX syntax. Besides activating the extension in the mkdocs.yml , the MathJax JavaScript runtime needs to be included. This must be done with additional JavaScript : extra_javascript: - 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js?config=TeX-MML-AM_CHTML' If you want to override the default MathJax configuration, you can do this by adding another JavaScript file before the MathJax runtime in extra_javascript which contains your MathJax configuration, e.g.: window.MathJax = { tex2jax: { inlineMath: [ [\"\\\\(\",\"\\\\)\"] ], displayMath: [ [\"\\\\[\",\"\\\\]\"] ] }, TeX: { TagSide: \"right\", TagIndent: \".8em\", MultLineWidth: \"85%\", equationNumbers: { autoNumber: \"AMS\", }, unicode: { fonts: \"STIXGeneral,'Arial Unicode MS'\" } }, displayAlign: \"left\", showProcessingMessages: false, messageStyle: \"none\" }; In your mkdocs.yml , include it with: extra_javascript: - 'javascripts/extra.js' - 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js?config=TeX-MML-AM_CHTML'","title":"Arithmatex MathJax"},{"location":"extensions/pymdown/#blocks","text":"Blocks are enclosed in $$...$$ which are placed on separate lines. Example: $$ \\frac{n!}{k!(n-k)!} = \\binom{n}{k} $$ Result: \\frac{n!}{k!(n-k)!} = \\binom{n}{k} \\frac{n!}{k!(n-k)!} = \\binom{n}{k}","title":"Blocks"},{"location":"extensions/pymdown/#inline","text":"Inline equations need to be enclosed in $...$ : Example: Lorem ipsum dolor sit amet: $p(x|y) = \\frac{p(y|x)p(x)}{p(y)}$ Result: Lorem ipsum dolor sit amet: p(x|y) = \\frac{p(y|x)p(x)}{p(y)} p(x|y) = \\frac{p(y|x)p(x)}{p(y)}","title":"Inline"},{"location":"extensions/pymdown/#betterem","text":"BetterEm improves the handling of emphasis markup ( bold and italic ) within Markdown by providing a more sophisticated parser for better detecting start and end tokens. Read the documentation for usage notes .","title":"BetterEm"},{"location":"extensions/pymdown/#caret","text":"Caret makes it possible to highlight inserted text . The portion of text that should be marked as added must be enclosed in two carets ^^...^^ .","title":"Caret"},{"location":"extensions/pymdown/#critic","text":"Critic implements Critic Markup , a Markdown extension that enables the tracking of changes (additions, deletions and comments) on documents. During compilation of the Markdown document, changes can be rendered (default), accepted or rejected. Text can be deleted and replacement text added . This can also be combined into one a single operation. Highlighting is also possible and comments can be added inline . Formatting can also be applied to blocks, by putting the opening and closing tags on separate lines and adding new lines between the tags and the content.","title":"Critic"},{"location":"extensions/pymdown/#details","text":"Details adds collapsible Admonition-style blocks which can contain arbitrary content using the HTML5 details and summary tags. Additionally, all Admonition qualifiers can be used, e.g. note , question , warning etc.: How many Prolog programmers does it take to change a lightbulb? Yes.","title":"Details"},{"location":"extensions/pymdown/#emoji","text":"Emoji adds the ability to insert a -load of emojis that we use in our daily lives. See the EmojiOne demo for a list of all available emojis. Happy scrolling Legal disclaimer Material has no affiliation with EmojiOne which is released under CC BY 4.0 . When including EmojiOne images or CSS, please read the EmojiOne license to ensure proper usage and attribution.","title":"Emoji"},{"location":"extensions/pymdown/#inlinehilite","text":"InlineHilite adds support for inline code highlighting. It's useful for short snippets included within body copy, e.g. var test = 0; and can be achived by prefixing inline code with a shebang and language identifier, e.g. #!js .","title":"InlineHilite"},{"location":"extensions/pymdown/#magiclink","text":"MagicLink detects links in Markdown and auto-generates the necessary markup, so no special syntax is required. It auto-links http[s]:// and ftp:// links, as well as references to email addresses.","title":"MagicLink"},{"location":"extensions/pymdown/#mark","text":"Mark adds the ability to highlight text like it was marked with a text marker . The portion of text that should be highlighted must be enclosed in two equal signs ==...== .","title":"Mark"},{"location":"extensions/pymdown/#smartsymbols","text":"SmartSymbols converts markup for special characters into their corresponding symbols, e.g. arrows (\u2190, \u2192, \u2194), trademark and copyright symbols (\u00a9, \u2122, \u00ae) and fractions (\u00bd, \u00bc, ...).","title":"SmartSymbols"},{"location":"extensions/pymdown/#superfences","text":"SuperFences provides the ability to nest code blocks under blockquotes, lists and other block elements, which the Fenced Code Blocks extension from the standard Markdown library doesn't parse correctly. SuperFences does also allow grouping code blocks with tabs .","title":"SuperFences"},{"location":"extensions/pymdown/#tasklist","text":"Tasklist adds support for styled checkbox lists. This is useful for keeping track of tasks and showing what has been done and has yet to be done. Checkbox lists are like regular lists, but prefixed with [ ] for empty or [x] for filled checkboxes. Example: * [x] Lorem ipsum dolor sit amet, consectetur adipiscing elit * [x] Nulla lobortis egestas semper * [x] Curabitur elit nibh, euismod et ullamcorper at, iaculis feugiat est * [ ] Vestibulum convallis sit amet nisi a tincidunt * [x] In hac habitasse platea dictumst * [x] In scelerisque nibh non dolor mollis congue sed et metus * [x] Sed egestas felis quis elit dapibus, ac aliquet turpis mattis * [ ] Praesent sed risus massa * [ ] Aenean pretium efficitur erat, donec pharetra, ligula non scelerisque * [ ] Nulla vel eros venenatis, imperdiet enim id, faucibus nisi Result: Lorem ipsum dolor sit amet, consectetur adipiscing elit Nulla lobortis egestas semper Curabitur elit nibh, euismod et ullamcorper at, iaculis feugiat est Vestibulum convallis sit amet nisi a tincidunt In hac habitasse platea dictumst In scelerisque nibh non dolor mollis congue sed et metus Sed egestas felis quis elit dapibus, ac aliquet turpis mattis Praesent sed risus massa Aenean pretium efficitur erat, donec pharetra, ligula non scelerisque Nulla vel eros venenatis, imperdiet enim id, faucibus nisi","title":"Tasklist"},{"location":"extensions/pymdown/#tilde","text":"Tilde provides an easy way to strike through cross out text. The portion of text that should be erased must be enclosed in two tildes ~~...~~ and the extension will take care of the rest.","title":"Tilde"}]}